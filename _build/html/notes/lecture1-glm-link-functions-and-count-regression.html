

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Lecture 1 - Generalized Linear Models: Link Functions and Count Regression &#8212; DSCI 562 - Regression II</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notes/lecture1-glm-link-functions-and-count-regression';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Lecture 2 - Generalized Linear Models: Model Selection and Multinomial Logistic Regression" href="lecture2_glm_model_selection_multinomial.html" />
    <link rel="prev" title="Lecture Learning Objectives" href="../lecture-learning-objectives.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../README.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/UBC_MDS_logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/UBC_MDS_logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../README.html">
                    Welcome to DSCI 562: Regression II
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lectures</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lecture-learning-objectives.html">Lecture Learning Objectives</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Lecture 1 - Generalized Linear Models: Link Functions and Count Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture2_glm_model_selection_multinomial.html">Lecture 2 - Generalized Linear Models: Model Selection and Multinomial Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture3_glm_ordinal_regression.html">Lecture 3 - Generalized Linear Models: Ordinal Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture4_linear_mixed_effects_models.html">Lecture 4 - Linear Mixed-Effects Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture5_survival_analysis.html">Lecture 5 - Survival Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture6_local_regression.html">Lecture 6 - Local Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture7_quantile_regression.html">Lecture 7 - Quantile Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture8_missing_data.html">Lecture 8 - Missing Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendices</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="appendix-binary-log-regression.html">Binary Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix-reg-cheatsheet.html">Regression Cheatsheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix-reg-mindmap.html">Regression Mind Map</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix-dist-cheatsheet.html">Distribution Cheatsheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix-greek-alphabet.html">Greek Alphabet</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">License and Code of Conduct</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../LICENSE.html">License</a></li>
<li class="toctree-l1"><a class="reference internal" href="../CODE-OF-CONDUCT.html">Code of Conduct</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/UBC-MDS/DSCI_562_regr-2" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/UBC-MDS/DSCI_562_regr-2/issues/new?title=Issue%20on%20page%20%2Fnotes/lecture1-glm-link-functions-and-count-regression.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/notes/lecture1-glm-link-functions-and-count-regression.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Lecture 1 - Generalized Linear Models: Link Functions and Count Regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-brief-digression-on-textboxes">A Brief Digression on Textboxes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#today-s-learning-goals">Today’s Learning Goals</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-libraries">Loading Libraries</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-preamble-on-our-data-science-workflow">1. A Preamble on our Data Science Workflow</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#study-design">1.1. Study Design</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-collection-and-wrangling">1.2. Data Collection and Wrangling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exploratory-data-analysis">1.3. Exploratory Data Analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-modelling">1.4. Data Modelling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation">1.5. Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#results">1.6. Results</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#storytelling">1.7. Storytelling</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#review-of-ordinary-least-squares-regression">2. Review of Ordinary Least-squares Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-modelling-framework">2.1. Data Modelling Framework</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#modelling-assumptions">2.1.1. Modelling Assumptions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-regressors">2.1.2. Categorical Regressors</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#can-we-apply-linear-regression-here">2.2. Can We Apply Linear Regression Here?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-makes-a-regression-model-linear">2.3. What makes a regression model “<em>linear</em>”?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">2.4. Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference">2.5. Inference</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#violations-of-assumptions">2.6. Violations of Assumptions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#distributional-misspecification">2.6.1. Distributional Misspecification</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#random-components-with-non-zero-mean">2.6.2. Random Components with Non-Zero Mean</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#heterocedasticity">2.6.3. Heterocedasticity</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#correlated-random-components">2.6.4. Correlated Random Components</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-ordinary-least-squares-regression-does-not-suffice">3. When Ordinary Least-squares Regression Does Not Suffice</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#paving-the-way-to-generalized-linear-models">4. Paving the Way to Generalized Linear Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nature-of-the-model-function">4.1. Nature of the Model Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-regression-problem">4.2. The Regression Problem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#black-box-models">4.3. Black-box Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretability-in-linear-models">4.4. Interpretability in Linear Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-types-of-parametric-assumptions">4.5. The Types of Parametric Assumptions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#an-example-where-ols-regression-totally-goes-wrong">4.6. An example where OLS regression totally goes wrong</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#let-us-make-it-right">4.7. Let us make it right!</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#restricted-response-ranges-in-linear-regression">4.8. Restricted Response Ranges in Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#link-function">4.9. Link Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#poisson-regression">5. Poisson Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-crabs-dataset">5.1. The Crabs Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">5.2. Exploratory Data Analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">5.3. Data Modelling Framework</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">5.4. Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">5.5. Inference</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#coefficient-interpretation">5.6. Coefficient Interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictions">5.7. Predictions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overdispersion">6. Overdispersion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#negative-binomial-regression">7. Negative Binomial Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reparametrization">7.1. Reparametrization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">7.2. Data Modelling Framework</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">7.3. Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-coefficient-interpretation-prediction-goodness-of-fit-and-model-selection">7.4. Inference, Coefficient Interpretation, Prediction, Goodness of Fit, and Model Selection</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wrapping-up-on-count-regression-models">8. Wrapping Up on Count Regression Models</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="lecture-1-generalized-linear-models-link-functions-and-count-regression">
<h1>Lecture 1 - Generalized Linear Models: Link Functions and Count Regression<a class="headerlink" href="#lecture-1-generalized-linear-models-link-functions-and-count-regression" title="Permalink to this heading">#</a></h1>
<section id="a-brief-digression-on-textboxes">
<h2>A Brief Digression on Textboxes<a class="headerlink" href="#a-brief-digression-on-textboxes" title="Permalink to this heading">#</a></h2>
<p>Throughout these lecture notes, you could find textboxes that highlight the following:</p>
<div class="tip admonition">
<p class="admonition-title">Definition</p>
<p>A formal statistical definition of any given concept with a key role in the lecture’s main topic.</p>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>An idea of crucial relevance for the lecture’s main topic. Mapping this idea in your learning process will help in the course’s lab assignments and quizzes.</p>
</div>
<div class="warning admonition">
<p class="admonition-title">Note</p>
<p>An idea that might be slightly out of the main scope of the lecture but with particular relevance for proper statistical practice in a Data Science context.</p>
</div>
<p>Moreover, you might find some sections marked as “<strong>Optional</strong>.” This labelling indicates we will not review that specific material in this course due to time constraints. Still, you might encounter these regression topics in practice. Thus, we provide their corresponding foundations in these notes.</p>
</section>
<section id="today-s-learning-goals">
<h2>Today’s Learning Goals<a class="headerlink" href="#today-s-learning-goals" title="Permalink to this heading">#</a></h2>
<p>By the end of this lecture, you should be able to:</p>
<ul class="simple">
<li><p>Explain the Data Science workflow in Regression Analysis.</p></li>
<li><p>Recall the basics of Ordinary Least-squares (OLS) regression.</p></li>
<li><p>Identify cases where OLS regression is not suitable.</p></li>
<li><p>Distinguish what makes a regression model “<em>linear</em>.”</p></li>
<li><p>Explain the concept of generalized linear models (GLMs).</p></li>
<li><p>Explore the concept of the link function.</p></li>
<li><p>Outline the modelling framework of count regression.</p></li>
<li><p>Fit and interpret count regression.</p></li>
<li><p>Use count regression for prediction.</p></li>
<li><p>Explain and test overdispersion on count-type data.</p></li>
</ul>
</section>
<section id="loading-libraries">
<h2>Loading Libraries<a class="headerlink" href="#loading-libraries" title="Permalink to this heading">#</a></h2>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">options</span><span class="p">(</span><span class="n">repr.matrix.max.rows</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">6</span><span class="p">)</span>
<span class="nf">source</span><span class="p">(</span><span class="s">&quot;../scripts/support_functions.R&quot;</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">tidyverse</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">mlbench</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">AER</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">cowplot</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">broom</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">performance</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">qqplotr</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">glmbb</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="a-preamble-on-our-data-science-workflow">
<span id="ds-workflow-preamble"></span><h2>1. A Preamble on our Data Science Workflow<a class="headerlink" href="#a-preamble-on-our-data-science-workflow" title="Permalink to this heading">#</a></h2>
<p>This course continues on the regression foundations you saw in <a class="reference external" href="https://github.com/UBC-MDS/DSCI_561_regr-1?tab=readme-ov-file"><strong>DSCI 561</strong></a> (Regression I). That said, we will go beyond OLS regression and explore further regression techniques. In practice, these techniques have been developed in the statistical literature to address practical cases where the OLS modelling framework and assumptions are not suitable anymore. Thus, throughout this block, we will cover (at least) one new regression model per lecture.</p>
<p>All in all, you might notice that the lecture pace in this course will be faster in terms of the amount of regression models we aim to cover. Then, to have a homogeneous way to approach each one of these models, I have structured this regression course in three big pillars:</p>
<ol class="arabic simple">
<li><p>The use of an ordered <strong>Data Science workflow</strong>,</p></li>
<li><p>choosing the proper workflow flavour according to either an <strong>inferential</strong> or <strong>predictive</strong> paradigm, and</p></li>
<li><p>the correct use of an <strong>appropriate regression model</strong> based on the response of interest.</p></li>
</ol>
<p>Each one of these three pillars is heavily connected since a general Data Science workflow is applied in each one of these regression models, which aims to help in our learning (i.e., we would be able to know what exact stage to expect in our data analysis regardless of the regression model we are being exposed to).</p>
<p>Therefore, a crucial aspect of the practice of Regression Analysis is the need for this systematic Data Science workflow that will allow us to solve our respective inquiries in a transparent and reproducible way. <a class="reference internal" href="#data-science-workflow"><span class="std std-numref">Fig. 1</span></a> shows this workflow which has the following general stages:</p>
<ol class="arabic simple">
<li><p><strong>Study design</strong>.</p></li>
<li><p><strong>Data collection and wrangling</strong>.</p></li>
<li><p><strong>Exploratory data analysis</strong>.</p></li>
<li><p><strong>Data modelling</strong>.</p></li>
<li><p><strong>Estimation</strong>.</p></li>
<li><p><strong>Results</strong>.</p></li>
<li><p><strong>Storytelling</strong>.</p></li>
</ol>
<div class="full-width docutils">
<figure class="align-default" id="data-science-workflow">
<a class="reference internal image-reference" href="../_images/data-science-workflow.png"><img alt="../_images/data-science-workflow.png" src="../_images/data-science-workflow.png" style="height: 1400px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text">Data Science workflow for <strong>inferential</strong> and <strong>predictive</strong> inquiries in Regression Analysis.</span><a class="headerlink" href="#data-science-workflow" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Note the workflow in <a class="reference internal" href="#data-science-workflow"><span class="std std-numref">Fig. 1</span></a> has certain checkpoints in which we either choose <strong>inference</strong> and <strong>prediction</strong> according to the data inquiries defined in the <strong>Study Design</strong>.</p>
<p>Now, suppose we do not follow a predefined workflow in practice. In that case, we might be at stake in incorrectly addressing our inquiries, translating into meaningless results outside the context of the problem we aim to solve. This is why the formation of a Data Scientist must stress this workflow from the very introductory learning stages.</p>
</div>
<p>Subsequent subsections will define the seven stages and their relationship with previous (or future!) MDS courses.</p>
<section id="study-design">
<h3>1.1. Study Design<a class="headerlink" href="#study-design" title="Permalink to this heading">#</a></h3>
<p>The first stage of this workflow is heavily related to the <strong>main statistical inquiries</strong> we aim to address throughout the whole process. As a Data Scientist, it is your task to primarily translate these inquiries from the stakeholders of the problem as <strong>inferential</strong> or <strong>predictive</strong>. Roughly speaking, this primary classification can be explained as follows:</p>
<ul class="simple">
<li><p><strong>Inferential.</strong> The main objective is to untangle relationships of <strong>association</strong> or <strong>causation</strong> between the <strong>regressors</strong> (i.e., explanatory variables) and the corresponding <strong>response</strong> in the context of the problem of interest. Firstly, we would assess whether there is a statistical relationship between them. Then, if significant, we would quantify by how much.</p></li>
</ul>
<ul class="simple">
<li><p><strong>Predictive.</strong> The main objective is to deliver response predictions on further observations of regressors, having estimated a given model via a current training dataset. Unlike inferential inquiries, assessing a statistically significant association or causation between our variables of interest is not a primary objective but <strong>accurate predictions</strong>. This is one of the fundamental paradigms of <strong>Machine Learning</strong>.</p></li>
</ul>
<figure class="align-default" id="panda">
<a class="reference internal image-reference" href="../_images/panda.png"><img alt="../_images/panda.png" src="../_images/panda.png" style="height: 350px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text">The Questioning Panda comes back.</span><a class="headerlink" href="#panda" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Throughout the lecture notes and lab assignments, this stage will be called “<strong>Main Statistical Inquiries</strong>.”</p>
</div>
</section>
<section id="data-collection-and-wrangling">
<h3>1.2. Data Collection and Wrangling<a class="headerlink" href="#data-collection-and-wrangling" title="Permalink to this heading">#</a></h3>
<p>Once we have defined our main statistical inquiries, it is time to collect our data. It is essential to be careful about the way we collect this data since it might have a particular impact on the quality of our statistical practice:</p>
<ul class="simple">
<li><p>Regarding inferential inquiries, recall the topics from <code class="docutils literal notranslate"><span class="pre">lecture1</span></code> in <a class="reference external" href="https://github.com/UBC-MDS/DSCI_552_stat-inf-1?tab=readme-ov-file"><strong>DSCI 552</strong></a> (Statistical Inference and Computation I). We are approaching <strong>populations</strong> or <strong>systems</strong> of interest governed by <strong>unknown and fixed parameters</strong>. Thus, via sampled data, we aim to <strong>estimate</strong> these parameters. This is why <strong>a proper sampling method</strong> on this population or system of interest is critical to obtaining representative data for <strong>appropriate hypothesis testing</strong>.</p></li>
</ul>
<div class="warning admonition">
<p class="admonition-title">Note</p>
<p>This stage is coloured in gray in <a class="reference internal" href="#data-science-workflow"><span class="std std-numref">Fig. 1</span></a>, unlike the other ones coloured in yellow. This is because sampling topics are out of the scope of this course and MDS in general. Nevertheless, we still need to stress that a  proper sampling method is also key in inferential inquiries to assess association and/or causation between the regressors and your response of interest. That said, depending on the context of the problem, we could apply either one of the following methods of sampling:</p>
<ul class="simple">
<li><p><strong>Simple random sampling</strong>.</p></li>
<li><p><strong>Systematic sampling</strong>.</p></li>
<li><p><strong>Stratified sampling</strong>.</p></li>
<li><p><strong>Clustered sampling</strong>.</p></li>
<li><p>Etc.</p></li>
</ul>
<p>As in the case of Regression Analysis, statistical sampling is a vast field, and we could spend a whole course on it. If you are more interested in these topics, <a class="reference external" href="https://webcat.library.ubc.ca/vwebv/holdingsInfo?bibId=7893726"><em>Sampling: design and analysis</em></a> by Lohr offers great foundations.</p>
</div>
<ul class="simple">
<li><p>In practice, regarding predictive inquiries, we would likely have to deal with databases given that our trained models will not be used to make inference and parameter interpretations.</p></li>
</ul>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>You have learned key technical skills to manage databases such as in <a class="reference external" href="https://github.com/UBC-MDS/DSCI_513_database-data-retr"><strong>DSCI 513</strong></a> (Databases and Data Retrieval). That said, our main focus in this course will be more centred on data modelling rather than database management. Still, throughout the lecture notes and lab assignments, we will sometimes execute some straightforward “<strong>Data Wrangling</strong>.”</p>
</div>
</section>
<section id="exploratory-data-analysis">
<h3>1.3. Exploratory Data Analysis<a class="headerlink" href="#exploratory-data-analysis" title="Permalink to this heading">#</a></h3>
<p>Before delivering proper data modelling and after an appropriate collection, it is essential to depict the relationships between your variables of interest in the corresponding collected <strong>training data</strong>. This is why the third stage corresponds to the exploratory data analysis (EDA). Furthermore, even before we start coding our corresponding plots using our wrangled data, we need to execute a <strong>variable classification</strong> carefully:</p>
<ul class="simple">
<li><p>Under the context of Regression Analysis, what is our response of interest <span class="math notranslate nohighlight">\(Y\)</span>? Is it discrete or continuous?</p>
<ol class="arabic simple">
<li><p>If it is discrete; is it binary, count or categorical?</p></li>
<li><p>If it is continuous, is it bounded or unbounded?</p></li>
</ol>
</li>
<li><p>Again, under the context of Regression Analysis, what are our <span class="math notranslate nohighlight">\(k\)</span> regressors <span class="math notranslate nohighlight">\(X_1, \dots, X_k\)</span>?</p>
<ol class="arabic simple">
<li><p>If a given regressor is discrete; is it binary, count or categorical?</p></li>
<li><p>If a given regressor is continuous, is it bounded or unbounded?</p></li>
</ol>
</li>
</ul>
<p>The above classification will give us the necessary insights to choose the most appropriate modelling approach for our response of interest. Moreover, if our inquiries are inferential, this classification will provide a solid idea of the interpretation for each estimated parameter corresponding to these <span class="math notranslate nohighlight">\(k\)</span> regressors.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>You have learned vital technical skills for plotting such as in <a class="reference external" href="https://ubc-mds.github.io/DSCI_531_viz-1/"><strong>DSCI 531</strong></a> (Data Visualization I). Hence, we can proceed with our plot coding once we have executed a correct variable classification. Throughout the lecture notes and lab assignments, you must select the suitable set of EDA plots based on this classification along with key <strong>descriptive summary statistics</strong>.</p>
</div>
</section>
<section id="data-modelling">
<h3>1.4. Data Modelling<a class="headerlink" href="#data-modelling" title="Permalink to this heading">#</a></h3>
<p><a class="reference external" href="https://ubc-mds.github.io/DSCI_551_stat-prob-dsci/lectures/"><strong>DSCI 551</strong></a> (Descriptive Statistics and Probability for Data Science) touched upon essential foundations in probability related to random variables and distributions. That said, we will follow three important steps:</p>
<ol class="arabic simple">
<li><p>Recall that we stressed the fact of proper choice of a random variable, depending on the context of our data modelling problem (i.e., the <a class="reference internal" href="appendix-dist-cheatsheet.html"><span class="doc">Distribution Cheatsheet</span></a>). Hence, we will use this practice to model our response of interest in Regression Analysis. Moreover, we will define whether we aim to model its conditional expected value given our <span class="math notranslate nohighlight">\(k\)</span> regressors or some other statistic (e.g., a conditioned median or some other quantile!). All these facts will allow us to choose <strong>the right regression model</strong>.</p></li>
</ol>
<ol class="arabic simple" start="2">
<li><p>Given our chosen regression model, we must define what <strong>modelling parameters</strong> we aim to estimate via our training data (e.g., intercept and coefficients along with some response variance components such as <span class="math notranslate nohighlight">\(\sigma^2\)</span> in OLS).</p></li>
</ol>
<ol class="arabic simple" start="3">
<li><p>Finally, we have to set up our <strong>mathematical modelling equation</strong>, which will allow us to explain in plain words how the regressors are related to the response once we estimate the modelling parameters. This fact will be significantly emphasized throughout the lecture and lab assignments as “<strong>Data Modelling Framework</strong>.”</p></li>
</ol>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The above steps (1) and (2) can be viewed as having a toolbox of possible models we could use. Take the mind map depicted in <a class="reference internal" href="#reg-mindmap-1"><span class="std std-numref">Fig. 3</span></a> as a starting point with the OLS model you saw in <strong>DSCI 561</strong>. Here is how we can go over the mind map:</p>
<blockquote>
<div><p><em>In Regression Analysis, we can model an <strong>unbounded continuous response</strong> in terms of their <strong>global conditioned mean</strong> using OLS for inference or prediction.</em></p>
</div></blockquote>
<p>Note we added the <strong>global</strong> characteristic to the below mind map since we will also cover local regression models during this block. Furthermore, this mind map will expand as we learn new regression models throughout subsequent lectures.</p>
</div>
<figure class="align-default" id="reg-mindmap-1">
<a class="reference internal image-reference" href="../_images/reg-mindmap-1.png"><img alt="../_images/reg-mindmap-1.png" src="../_images/reg-mindmap-1.png" style="height: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 3 </span><span class="caption-text">Initial regression modelling mind map.</span><a class="headerlink" href="#reg-mindmap-1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="estimation">
<h3>1.5. Estimation<a class="headerlink" href="#estimation" title="Permalink to this heading">#</a></h3>
<p>In most of the models we will learn in this block, in conjunction with the probabilistic topics from <strong>DSCI 551</strong>, we will rely on <strong>maximum likelihood estimation (MLE)</strong> to estimate the modelling parameters we defined in step (2) from the <strong>Data Modelling</strong> stage.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>In plain words, MLE uses the training data’s joint probability distribution, which contains the modelling parameters, to find the numerical values of these parameters that maximize the likelihood function (<strong>which is mathematically equivalent to the joint probability distribution of the training data</strong>).</p>
<p>Even though MLE will be the estimation method for most of the regression models in this block, some other models will rely on loss functions analogous to the estimation method of OLS. Finally, it is important to clarify that our estimation and inferential paradigms will be <strong>frequentist</strong> in this course.</p>
</div>
<div class="warning admonition">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">R</span></code> will be the programming language for this course. Regarding modelling estimation, we will use standalone fitting functions (with their specific syntax) across different packages. Nevertheless, these functions can also be found in <code class="docutils literal notranslate"><span class="pre">Python,</span></code> which is outside the scope of our lectures. The <code class="docutils literal notranslate"><span class="pre">Python</span></code> package <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> offers <a class="reference external" href="https://www.statsmodels.org/stable/user-guide.html"><strong>estimation tools</strong></a> for most models we will review in this block.</p>
</div>
</section>
<section id="results">
<h3>1.6. Results<a class="headerlink" href="#results" title="Permalink to this heading">#</a></h3>
<p>It is time to use our modelling estimates to solve our main statistical inquiries. As we previously discussed, the way we proceed in this stage will depend on whether our inquiries are inferential or predictive:</p>
<ol class="arabic simple">
<li><p><strong>Inferential.</strong> Since our training set is assumed to be a random sample from our population or system of interest, we will conduct the corresponding hypothesis testing (e.g., the <strong><span class="math notranslate nohighlight">\(t\)</span>-test</strong> in OLS) on our modelling estimates for the regression coefficients (along with the yielded standard errors from the corresponding estimation method such as MLE for many models to be reviewed in this block). These tests will allow us to deliver a significance conclusion on whether there is an association/causation between the regressors and the corresponding response of interest. Once we have determined a statistical significance, we can deliver the following:</p>
<ul class="simple">
<li><p><strong>Point estimate.</strong> It is the estimate of the regression coefficient for the <span class="math notranslate nohighlight">\(j\)</span>th regressor (i.e., <span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span>) along with its corresponding interpretation. This is why it is important to fully explain, in plain words, our <strong>mathematical modelling equation</strong> from the <strong>Data Modelling</strong> stage.</p></li>
<li><p><strong>Interval estimate.</strong> This corresponds to the <strong>confidence interval</strong> we will compute using <em><span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span></em> and its standard error. Recall that we always have to provide an interval estimate in our inferential conclusions since it measures the uncertainty of our point estimates.</p></li>
</ul>
</li>
</ol>
<ol class="arabic simple" start="2">
<li><p><strong>Predictive.</strong> Once we have estimated our regression model, we can obtain predictions on a further test set. Moreover, across courses such as <a class="reference external" href="https://ubc-mds.github.io/DSCI_571_sup-learn-1/README.html"><strong>DSCI 571</strong></a> (Supervised Learning I) and <a class="reference external" href="https://github.com/UBC-MDS/DSCI_573_feat-model-select"><strong>DSCI 573</strong></a> (Feature and Model Selection), you have seen metrics and techniques to assess prediction accuracy on a given estimated model.</p></li>
</ol>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Mostly for predictive inquiries, and sometimes for inference, this stage could include applying <strong>model selection techniques</strong> (as seen in <strong>DSCI 561</strong>). These techniques rely on <strong>goodness of fit</strong> and involve using a specific type of regression framework and then, estimating a set of candidate models using the same response but different subsets of regressors coming from our training data. <strong>The final goal of model selection will be choosing the best model fitting.</strong></p>
<p>That said, we will cover <strong>maximum likelihood-based model selection techniques</strong> in this block. Moreover, note that <a class="reference internal" href="#data-science-workflow"><span class="std std-numref">Fig. 1</span></a> does not explicitly mention model selection techniques. However, you could implement them right after estimating your modelling parameters (i.e., after the <strong>Estimation</strong> stage). Finally, this practice would make the workflow iterative between the <strong>Estimation</strong> and <strong>Results</strong> stages.</p>
</div>
<div class="warning admonition">
<p class="admonition-title">Note</p>
<p>As a side note, at the end of this stage, we can also apply <strong>model diagnostics</strong> analogous to <strong>the ones for OLS</strong> but for other types of regression models. Nevertheless, given the time constraints, these tools will not be covered during lecture time. Still, when possible, I will provide <strong>optional</strong> notes outlining them.</p>
</div>
</section>
<section id="storytelling">
<h3>1.7. Storytelling<a class="headerlink" href="#storytelling" title="Permalink to this heading">#</a></h3>
<p>The final stage of this workflow is what we call <strong>Storytelling</strong>. This stage involves communicating our inferential or predictive findings to a specific audience of stakeholders (either technical or non-technical). This is why a course such as <a class="reference external" href="https://github.com/UBC-MDS/DSCI_542_comm-arg"><strong>DSCI 542</strong></a> (Communication and Argumentation) is crucial in MDS.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>We will sometimes dig into this stage in our warmup exercises during the lab sessions. Moreover, <a class="reference external" href="https://github.com/UBC-MDS/DSCI_554_exper-causal-inf"><strong>DSCI 554</strong></a> (Experimentation and Causal Inference) will fully explore this stage.</p>
</div>
</section>
</section>
<section id="review-of-ordinary-least-squares-regression">
<h2>2. Review of Ordinary Least-squares Regression<a class="headerlink" href="#review-of-ordinary-least-squares-regression" title="Permalink to this heading">#</a></h2>
<p>Let us quickly review the OLS model before proceeding to alternative regression approaches.</p>
<p>In <strong>DSCI 561</strong>, you learned comprehensive material about OLS regression. You might wonder:</p>
<blockquote>
<div><p>Why are we using the term “<strong>ordinary</strong>”?</p>
</div></blockquote>
<p>The term “<strong>ordinary</strong>” refers to a linear regression model with a <strong>response</strong> (also known as <strong>endogenous variable</strong>) of <strong>continuous nature</strong>. From <strong>DSCI 551</strong>, recall that a continuous variable can take on an infinite number of real values in a given range. This response is subject to <strong>regressors</strong> (also known as <strong>exogenous variables</strong>, <strong>explanatory variables</strong>, <strong>features</strong>, or <strong>predictors</strong>). Note that the regressors can be of a continuous or discrete nature. When the regressors are <strong>discrete</strong> and <strong>factor-type</strong> (i.e., with different categories), they could be:</p>
<ul class="simple">
<li><p><strong>Nominal.</strong> In this factor-type variable, we have categories that do not follow any specific order. For example, a clinical trial with a factor of three treatments: <em>placebo</em>, <em>treatment A</em>, and <em>treatment B</em>.</p></li>
<li><p><strong>Ordinal.</strong> The categories, in this case, follow a specific hierarchical order. A typical example is the <em>Likert scale</em> of survey items: <em>strongly disagree</em>, <em>disagree</em>, <em>neutral</em>, <em>agree</em>, and <em>strongly agree</em>.</p></li>
</ul>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The OLS case is a classic starting learning topic in Regression Analysis. In fact, we might call this model “<em>classical linear regression</em>.”</p>
</div>
<section id="data-modelling-framework">
<h3>2.1. Data Modelling Framework<a class="headerlink" href="#data-modelling-framework" title="Permalink to this heading">#</a></h3>
<p>Let us dig into the general <strong>Data Modelling</strong> stage in OLS regression. Conceptually, the OLS regression model can be expressed as:</p>
<div class="math notranslate nohighlight" id="equation-eq-conceptual-model">
<span class="eqno">(1)<a class="headerlink" href="#equation-eq-conceptual-model" title="Permalink to this equation">#</a></span>\[
\mbox{Response} = \mbox{Systematic Component} + \mbox{Random Component}.
\]</div>
<ul class="simple">
<li><p>The <strong>systematic component</strong> represents the mean of the response <strong>which is conditioned on the regressor values</strong>.</p></li>
<li><p>The <strong>random component</strong> measures the extent to which the observed value of the response might deviate from its mean and is viewed as <strong>random noise</strong>.</p></li>
</ul>
<p>For the <span class="math notranslate nohighlight">\(i\)</span>th observation in our <strong>random sample</strong> or <strong>training data</strong> (<span class="math notranslate nohighlight">\(i = 1, \dots, n\)</span>), the conceptual model <a class="reference internal" href="#equation-eq-conceptual-model">(1)</a> is mathematically represented as:</p>
<div class="math notranslate nohighlight" id="equation-eq-ols-model">
<span class="eqno">(2)<a class="headerlink" href="#equation-eq-ols-model" title="Permalink to this equation">#</a></span>\[
\underbrace{Y_i}_\text{Response}  = \underbrace{\beta_0 + \beta_1 g_1(X_{i, 1}) + \ldots + \beta_k g_k(X_{i,k})}_\text{Systematic Component} + \underbrace{\varepsilon_i.}_\text{Random Component}
\]</div>
<p>Note the following:</p>
<ul class="simple">
<li><p>The response <span class="math notranslate nohighlight">\(Y_i\)</span> is equal to the sum of <span class="math notranslate nohighlight">\(k + 2\)</span> terms.</p></li>
<li><p>The systematic component is the sum of:</p>
<ul>
<li><p>An <strong>unknown intercept</strong> <span class="math notranslate nohighlight">\(\beta_0\)</span> and</p></li>
<li><p><span class="math notranslate nohighlight">\(k\)</span> <strong>regressor functions</strong> <span class="math notranslate nohighlight">\(g_j(X_{i,j})\)</span> multiplied by their respective <strong>unknown regression coefficient</strong> <span class="math notranslate nohighlight">\(\beta_j\)</span> (<span class="math notranslate nohighlight">\(j = 1, \dots, k\)</span>).</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\varepsilon_i\)</span> is the <strong>random noise</strong>.</p></li>
</ul>
<section id="modelling-assumptions">
<h4>2.1.1. Modelling Assumptions<a class="headerlink" href="#modelling-assumptions" title="Permalink to this heading">#</a></h4>
<p>The model in <a class="reference internal" href="#equation-eq-ols-model">(2)</a> above for <span class="math notranslate nohighlight">\(Y_i\)</span> is more detailed as follows:</p>
<ul class="simple">
<li><p>The response <span class="math notranslate nohighlight">\(Y_i\)</span> depends on the <strong>linear combination</strong> of the functions <span class="math notranslate nohighlight">\(g_j(\cdot)\)</span> of <span class="math notranslate nohighlight">\(k\)</span> regressors <span class="math notranslate nohighlight">\(X_{i, j}\)</span> of different types (continuous and discrete).</p></li>
</ul>
<ul class="simple">
<li><p>Each function <span class="math notranslate nohighlight">\(g_j(X_{i,j})\)</span> has an associated regression coefficient. These parameters <span class="math notranslate nohighlight">\(\beta_{1}, \dots, \beta_{k}\)</span> represent how much the response is expected to increase or decrease when the function <span class="math notranslate nohighlight">\(g_j(X_{i,j})\)</span> changes by one unit of the <span class="math notranslate nohighlight">\(j\)</span>th regressor. An additional parameter <span class="math notranslate nohighlight">\(\beta_0\)</span> represents the mean of the response when all the <span class="math notranslate nohighlight">\(k\)</span> functions <span class="math notranslate nohighlight">\(g_j(X_{i,j})\)</span> are zero. All these elements represent the systematic component of the model.</p></li>
</ul>
<ul class="simple">
<li><p>The term <span class="math notranslate nohighlight">\(\varepsilon_i\)</span> is an unobserved random variable and represents the random component. These variables are <strong>usually</strong> assumed to be <strong>normally distributed</strong> with <strong>mean of zero</strong> and a <strong>common variance</strong> <span class="math notranslate nohighlight">\(\sigma^2\)</span> (i.e., <strong>homoscedasticity</strong>). Moreover, all <span class="math notranslate nohighlight">\(\varepsilon_i\)</span>s are assumed to be <strong>independent</strong>.</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Definition of Random Component Assumptions in OLS</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{gather*}
\mathbb{E}(\varepsilon_i) = 0 \\
\text{Var}(\varepsilon_i) = \sigma^2 \\
\varepsilon_i \sim \mathcal{N}(0, \sigma^2) \\
\varepsilon_i \perp \!\!\! \perp \varepsilon_k \; \; \; \; \text{for} \; i \neq k  \; \; \; \; \text{(independence)}.
\end{gather*}\end{split}\]</div>
</div>
<ul class="simple">
<li><p>Hence, <strong>each <span class="math notranslate nohighlight">\(Y_i\)</span> is also assumed to be independent and normally distributed</strong>:</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-y-ols-model-normal">
<span class="eqno">(3)<a class="headerlink" href="#equation-y-ols-model-normal" title="Permalink to this equation">#</a></span>\[
Y_i \mid X_{i, j} \sim \mathcal{N} \big( \beta_0 + \beta_1 g_1(X_{i, 1}) + \ldots + \beta_k g_k(X_{i,k}), \sigma^2 \big).
\]</div>
</section>
<section id="categorical-regressors">
<h4>2.1.2. Categorical Regressors<a class="headerlink" href="#categorical-regressors" title="Permalink to this heading">#</a></h4>
<p>If the <span class="math notranslate nohighlight">\(j\)</span>th explanatory variable of interest is continuous, its observed value is expressed as a single <span class="math notranslate nohighlight">\(x_{i,j}\)</span> for the <span class="math notranslate nohighlight">\(i\)</span>th observation.</p>
<p>Suppose a explanatory variable of interest is nominal. In that case, we will need to use a dummy variable to identify the category to which each observation belongs. For instance, if a discussed categorical explanatory variable of interest has <span class="math notranslate nohighlight">\(u\)</span> categories or levels, we could define <span class="math notranslate nohighlight">\(u - 1\)</span> dummy variables as shown in the coding scheme in <a class="reference internal" href="#dummy-var"><span class="std std-numref">Table 1</span></a>. Note that <strong>Level 1</strong> is taken as the baseline (reference) level: if the <span class="math notranslate nohighlight">\(i\)</span>th observation belongs to <strong>Level 1</strong> then all the dummy variables <span class="math notranslate nohighlight">\(x_{i,1}, \cdots, x_{i,(u - 1)}\)</span> take on the value <span class="math notranslate nohighlight">\(0\)</span>. The choice of baseline has an impact on the interpretation of the regression coefficients. The baseline is related to the role of the intercept <span class="math notranslate nohighlight">\(\beta_0\)</span>.</p>
<table class="table" id="dummy-var">
<caption><span class="caption-number">Table 1 </span><span class="caption-text">Dummy variables in a nominal regressor with <span class="math notranslate nohighlight">\(u\)</span> levels.</span><a class="headerlink" href="#dummy-var" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p><strong>Level</strong></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(x_{i, 1}\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(x_{i, 2}\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\cdots\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(x_{i, (u - 1)}\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\cdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\cdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\ddots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(m\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\cdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="can-we-apply-linear-regression-here">
<h3>2.2. Can We Apply Linear Regression Here?<a class="headerlink" href="#can-we-apply-linear-regression-here" title="Permalink to this heading">#</a></h3>
<p>Having briefly discussed the components of the OLS model in <a class="reference internal" href="#equation-eq-ols-model">(2)</a>, let us explore three 2-<span class="math notranslate nohighlight">\(d\)</span> examples made with simulated data. Thus, we will start with some in-class questions via <a class="reference external" href="https://student.iclicker.com/"><strong>iClicker</strong></a>.</p>
<div class="exercise admonition" id="lecture1-q1">

<p class="admonition-title"><span class="caption-number">Exercise 1 </span></p>
<section id="exercise-content">
<p>In what of the below example(s) can we apply linear regression?</p>
<p><strong>A.</strong> Examples 1 and 2.</p>
<p><strong>B.</strong> Example 1.</p>
<p><strong>C.</strong> Examples 1 and 3.</p>
<p><strong>D.</strong> Examples 1, 2, and 3.</p>
</section>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/d80cf86a1ac096fcf52bb00507a9f2850fe4ecb362c2fb8db62a533792e64fd0.png" src="../_images/d80cf86a1ac096fcf52bb00507a9f2850fe4ecb362c2fb8db62a533792e64fd0.png" />
</div>
</div>
</section>
<section id="what-makes-a-regression-model-linear">
<h3>2.3. What makes a regression model “<em>linear</em>”?<a class="headerlink" href="#what-makes-a-regression-model-linear" title="Permalink to this heading">#</a></h3>
<p>Let us retake the <strong>Example 3</strong> from our previous activity.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/e888237b04ac98ec8c9334f9ede2b8438330e29f48f9f9807f8dea5d046db67b.png" src="../_images/e888237b04ac98ec8c9334f9ede2b8438330e29f48f9f9807f8dea5d046db67b.png" />
</div>
</div>
<p>It turns out that each synthetic data point was generated from a sinusoidal curve plus some normally distributed random noise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
Y_i &amp;= \beta_0 + \beta_1 g(X_i) + \varepsilon_i \\
&amp;= \beta_0 + \beta_1 \sin(X_i) + \varepsilon_i.
\end{align*}\end{split}\]</div>
<p>As we can see, this example can be modelled as an OLS model <strong>if we transform each <span class="math notranslate nohighlight">\(x_i\)</span> in the training set as <span class="math notranslate nohighlight">\(\sin(x_i)\)</span></strong>.</p>
<p>Now, let us generalize all these previouse ideas. When a linear regression model has more than one regressor, then we call it <strong>multiple linear regression model</strong>.</p>
<div class="tip admonition">
<p class="admonition-title">Definition of Linearity in Regression Models</p>
<p>The classical OLS model, from <strong>DSCI 561</strong>, implicates the identity function <span class="math notranslate nohighlight">\(g_j(X_{i, j}) = X_{i, j}\)</span> in equation
<a class="reference internal" href="#equation-eq-ols-model">(2)</a>. This leads to:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
Y_i &amp;= \beta_0 + \beta_1 g_1(X_{i, 1}) + \ldots + \beta_k g_k(X_{i,k}) + \varepsilon_i \\
&amp;= \beta_0 + \beta_1 X_{i,1} + \ldots + \beta_k X_{i,k} + \varepsilon_i.
\end{align*}\end{split}\]</div>
<p>Note the model is “linear” on the parameters (i.e., regression terms), not the regressors.</p>
</div>
<p>Finally, suppose we have a training dataset of <span class="math notranslate nohighlight">\(n\)</span> observations; i.e., for <span class="math notranslate nohighlight">\(i = 1, \dots, n\)</span>. Then all the <span class="math notranslate nohighlight">\(X_{i, j}\)</span> become observed values <span class="math notranslate nohighlight">\(x_{i, j}\)</span> (note the lowercase), leading to the following conditional expected value of the OLS model above as:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}(Y_i \mid X_{i,j} = x_{i,j}) = \beta_0 + \beta_1 x_{i,1} + \ldots + \beta_k x_{i,k} \; \; \; \; \text{since} \; \; \; \; \mathbb{E}(\varepsilon_i) = 0.
\]</div>
<p>We can see that the regression coefficients’ interpretation is targeted to explain each regressor’s numerical <strong>association</strong> (<strong>or effect if we are conducting an experiment!</strong>) on the mean of the response, <strong>if we fulfill the assumptions on the random component</strong> <span class="math notranslate nohighlight">\(\varepsilon_i\)</span>.</p>
</section>
<section id="id1">
<h3>2.4. Estimation<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<p>The next matter to address is how to estimate our OLS parameters since these are unknown. In order to fit a linear regression model for a given training dataset of <span class="math notranslate nohighlight">\(n\)</span> observations, we have to estimate the <span class="math notranslate nohighlight">\(k + 2\)</span> parameters <span class="math notranslate nohighlight">\(\beta_0, \beta_1, \dots, \beta_k, \sigma^2\)</span> by <strong>minimizing the sum of squared residuals</strong> (i.e., <strong>least-squares estimation</strong>) or <strong>maximizing the likelihood function of the sample</strong>.</p>
<p>The <strong>likelihood function</strong> is the <strong>joint probability density function (PDF)</strong> of the observed data as a function of the unknown parameters <span class="math notranslate nohighlight">\(\beta_0, \beta_1, \dots, \beta_k, \sigma^2\)</span> we are willing to estimate. A particular distribution is assumed for the individual observations. MLE aims to find the values of those parameters for which the observed data is more likely. The likelihood function for the multiple linear regression model is described as follows:</p>
<ul class="simple">
<li><p>We assume a random sample of <span class="math notranslate nohighlight">\(n\)</span> elements. Thus, the Normal <span class="math notranslate nohighlight">\(Y_i\)</span>s are independent as in <a class="reference internal" href="#equation-y-ols-model-normal">(3)</a>, which allows us to obtain the sample’s joint PDF.</p></li>
</ul>
<ul class="simple">
<li><p>The joint PDF is obtained by multiplying the standalone <span class="math notranslate nohighlight">\(n\)</span> Normal PDFs altogether. This joint PDF is mathematically equal to the likelihood function of the observed data.</p></li>
</ul>
<ul class="simple">
<li><p>The MLE method takes the first partial derivatives of the <strong>log-likelihood function</strong> with respect to <span class="math notranslate nohighlight">\(\beta_0, \beta_1, \dots, \beta_k, \sigma^2\)</span>. Then, we set these derivatives equal to zero and isolate the corresponding terms. This procedure yields the maximum likelihood estimates.</p></li>
</ul>
<ul class="simple">
<li><p>The case of simple linear regression (as in <span class="math notranslate nohighlight">\(\beta_0\)</span>, <span class="math notranslate nohighlight">\(\beta_1\)</span>, and <span class="math notranslate nohighlight">\(\sigma^2\)</span>) can be handled in scalar notation. However, in the presence of a considerable number of coefficients, it is more efficient to work with the model in matrix notation. Then, matrix calculus comes into play.</p></li>
</ul>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Now, we might wonder:</p>
<blockquote>
<div><p>How is MLE related to OLS?</p>
</div></blockquote>
<p>This is the point were the assumptions on the error component <span class="math notranslate nohighlight">\(\varepsilon_i\)</span> have a key role in MLE. If we make the corresponding mathematical derivations, <strong>it turns out that maximizing the log-likelihood function is equivalent to minimizing the sum of squared residuals</strong>.</p>
</div>
</section>
<section id="inference">
<h3>2.5. Inference<a class="headerlink" href="#inference" title="Permalink to this heading">#</a></h3>
<p>As we discussed in <a class="reference internal" href="#ds-workflow-preamble"><span class="std std-ref">1. A Preamble on our Data Science Workflow</span></a>, the estimated model can be used for two purposes: <strong>inference</strong> and <strong>prediction</strong>.</p>
<p>In terms of inference, we use the fitted model to identify the relationship between the response and regressors. We will need the <span class="math notranslate nohighlight">\(j\)</span>th estimated regression coefficient <span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span> and its corresponding variability which is reflected in the <strong>standard error</strong> of the estimate, <span class="math notranslate nohighlight">\(\mbox{se} \left( \hat{\beta}_j \right)\)</span>. To determine the statistical significance of <span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span>, we use the <strong>test statistic</strong></p>
<div class="math notranslate nohighlight">
\[t_j = \frac{\hat{\beta}_j}{\mbox{se} \left( \hat{\beta}_j \right)}\]</div>
<p>to test the hypotheses</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{gather*}
H_0: \beta_j = 0 \\
H_a: \beta_j \neq 0.
\end{gather*}\end{split}\]</div>
<p>A statistic like <span class="math notranslate nohighlight">\(t_j\)</span> is referred to as a <span class="math notranslate nohighlight">\(t\)</span>-value. It has a <span class="math notranslate nohighlight">\(t\)</span>-distribution <strong>under the null hypothesis</strong> <span class="math notranslate nohighlight">\(H_0\)</span> with <span class="math notranslate nohighlight">\(n - k - 1\)</span> degrees of freedom.</p>
<p>We can obtain the corresponding <span class="math notranslate nohighlight">\(p\)</span>-values for each <span class="math notranslate nohighlight">\(\beta_j\)</span> associated to the <span class="math notranslate nohighlight">\(t\)</span>-values under the null hypothesis <span class="math notranslate nohighlight">\(H_0\)</span>. <strong>The smaller the <span class="math notranslate nohighlight">\(p\)</span>-value, the stronger the evidence against the null hypothesis <span class="math notranslate nohighlight">\(H_0\)</span> in our sample</strong>. Hence, small <span class="math notranslate nohighlight">\(p\)</span>-values (less than the significance level <span class="math notranslate nohighlight">\(\alpha\)</span>) indicate that the data provides evidence in favour of association (or <strong>causation</strong> if that is the case) between the response variable and the <span class="math notranslate nohighlight">\(j\)</span>th regressor.</p>
<p>Similarly, given a specified <span class="math notranslate nohighlight">\((1-\alpha) \times 100\%\)</span> level of confidence, we can construct <strong>confidence intervals</strong> for the corresponding true value of <span class="math notranslate nohighlight">\(\beta_j\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_j \pm t_{\alpha/2, n - k - 1}\mbox{se} \left( \hat{\beta}_j \right),
\]</div>
<p>where <span class="math notranslate nohighlight">\(t_{\alpha/2, n - k - 1}\)</span> is the upper <span class="math notranslate nohighlight">\(\alpha/2\)</span> quantile of the <span class="math notranslate nohighlight">\(t\)</span>-distribution with <span class="math notranslate nohighlight">\(n - k - 1\)</span> degrees of freedom.</p>
</section>
<section id="violations-of-assumptions">
<h3>2.6. Violations of Assumptions<a class="headerlink" href="#violations-of-assumptions" title="Permalink to this heading">#</a></h3>
<p>Recall the multiple linear regression model is defined as</p>
<div class="math notranslate nohighlight">
\[
Y_i = \beta_0 + \beta_1 X_{i,1} + \ldots + \beta_k X_{i,k} + \varepsilon_i,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\varepsilon_i\)</span> (random component) is subject to these assumptions:</p>
<div class="math notranslate nohighlight" id="equation-ols-assumptions">
<span class="eqno">(4)<a class="headerlink" href="#equation-ols-assumptions" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{gather}
\mathbb{E}(\varepsilon_i) = 0 \\
\text{Var}(\varepsilon_i) = \sigma^2 \\
\varepsilon_i \sim \mathcal{N}(0, \sigma^2) \\
\varepsilon_i \perp \!\!\! \perp \varepsilon_k \; \; \; \; \text{for} \; i \neq k  \; \; \; \; \text{(independence)}.
\end{gather}\end{split}\]</div>
<p>Now, <strong>what would happen if the assumptions in <a class="reference internal" href="#equation-ols-assumptions">(4)</a> are violated?</strong> The below <strong>diagnostic plots</strong>  belong to <strong>simulated data</strong> whose OLS model violates the <strong>normality assumption</strong>.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/b4143877920f2da52a3ee4365883e9f5b0f7875c793381a7a3e2f6b7a23bc5a3.png" src="../_images/b4143877920f2da52a3ee4365883e9f5b0f7875c793381a7a3e2f6b7a23bc5a3.png" />
</div>
</div>
<p>Recall the <strong><span class="math notranslate nohighlight">\(Q\)</span>-<span class="math notranslate nohighlight">\(Q\)</span> plot</strong> and <strong>histogram of residuals</strong> are graphical tools that help us to assess the normality assumption as follows:</p>
<ul class="simple">
<li><p>In the case of the <span class="math notranslate nohighlight">\(Q\)</span>-<span class="math notranslate nohighlight">\(Q\)</span> plot, the ideal result is having all the data points lying on the 45° degree dotted line. This result means that all <strong>standardized residuals</strong> coming from the fitted model are equal to the theoretical quantiles coming from the <strong>Standard Normal distribution</strong>, i.e., <span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span>. However, for the case above, a considerable proportion of these data points is not lying on the 45° degree dotted line, suggesting <strong>non-normality</strong>.</p></li>
</ul>
<ul class="simple">
<li><p>For the histogram of residuals, we would expect a <strong>bell-shaped</strong> form as in the Normal distribution. Nonetheless, the plot above suggests a right-skewed distribution (also known as positive skewed).</p></li>
</ul>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>A distributional misspecification (i.e., assuming normality when it is not the case) has severe implications for the associated tests (<span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(F\)</span>-tests). The distributions of the test statistics under <span class="math notranslate nohighlight">\(H_0\)</span> rely on the normality assumption!</p>
</div>
<p>On the other hand, <strong>homoscedasticity</strong> can be assessed via the diagnostic plot of <strong>residuals vs. fitted values</strong>. The ideal result would show a uniform cloud of data points. However, the plot below (coming from an OLS model fitted with simulated data) shows a clear pattern composed of two funnel shapes. This pattern indicates non-constant variance, i.e., <strong>heteroscedasticity</strong>.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/8b72733f4ef4fe860156902107e048624a098c6ef2502bf0dfb9d55ecf2f1f00.png" src="../_images/8b72733f4ef4fe860156902107e048624a098c6ef2502bf0dfb9d55ecf2f1f00.png" />
</div>
</div>
<p>Below, you can find further information on the implications of assumption violations.</p>
<section id="distributional-misspecification">
<h4>2.6.1. Distributional Misspecification<a class="headerlink" href="#distributional-misspecification" title="Permalink to this heading">#</a></h4>
<p>Fulfilling the model’s assumptions <a class="reference internal" href="#equation-ols-assumptions">(4)</a> on the errors <span class="math notranslate nohighlight">\(\varepsilon_i\)</span> considerably impacts the statistical tests of the OLS model.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The distributions of the test statistics, such as the <span class="math notranslate nohighlight">\(t\)</span> or the <span class="math notranslate nohighlight">\(F\)</span>-values, heavily rely on the normality of the <span class="math notranslate nohighlight">\(\varepsilon_i\)</span>s. Suppose we do not fulfil the normality on the random component. In that case, these test statistics will not be reliable unless <strong>our sample size <span class="math notranslate nohighlight">\(n\)</span> is large enough</strong> (i.e., <strong>an asymptotical approximation</strong>). Not fulfilling the normality assumptions would put us at stake in drawing misleading statistical conclusions on significance.</p>
<p>Recall that hypothesis testing assumes a particular distribution under the null hypothesis <span class="math notranslate nohighlight">\(H_0\)</span>, which is related to these random components’ normality for this model in the <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(F\)</span>-tests.</p>
</div>
</section>
<section id="random-components-with-non-zero-mean">
<h4>2.6.2. Random Components with Non-Zero Mean<a class="headerlink" href="#random-components-with-non-zero-mean" title="Permalink to this heading">#</a></h4>
<p>Suppose we misspecified the mean in our OLS regression model for our random components, i.e.,</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}(\varepsilon_i) = c \neq 0.\]</div>
<p>This misspecification would be a mild violation. It will be absorbed by the intercept <span class="math notranslate nohighlight">\(\beta_0\)</span> leading to</p>
<div class="math notranslate nohighlight">
\[\beta_0^* = \beta_0 + c,\]</div>
<p>and reflected in the model estimate for the intercept.</p>
<div class="tip admonition">
<p class="admonition-title">Definition of Lurking Variable</p>
<p>Note that this <span class="math notranslate nohighlight">\(c\)</span> is constant over all the <span class="math notranslate nohighlight">\(n\)</span> <span class="math notranslate nohighlight">\(\varepsilon_i\)</span>s. Nonetheless, suppose there is a further regressor <span class="math notranslate nohighlight">\(X_{i, k + 1}\)</span> not taken into account. In that case, we are at stake in obtaining <strong>biased model estimates</strong> and misleading statistical conclusions on significance.</p>
<p>Therefore, we define this further regressor <span class="math notranslate nohighlight">\(X_{i, k + 1}\)</span> as a <strong>lurking variable</strong>.</p>
</div>
</section>
<section id="heterocedasticity">
<h4>2.6.3. Heterocedasticity<a class="headerlink" href="#heterocedasticity" title="Permalink to this heading">#</a></h4>
<p>We already defined homoscedasticity as the fact that all <span class="math notranslate nohighlight">\(\varepsilon_i\)</span>s have <span class="math notranslate nohighlight">\(\sigma^2\)</span> as a common variance. However, this assumption commonly gets violated in multiple linear regression and is called <strong>heteroscedasticity</strong>: the variance of the <span class="math notranslate nohighlight">\(\varepsilon_i\)</span>s is not constant.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>A common approach to solve heterocedasticity is a response transformation, usually logarithmical if it is positive.</p>
</div>
</section>
<section id="correlated-random-components">
<h4>2.6.4. Correlated Random Components<a class="headerlink" href="#correlated-random-components" title="Permalink to this heading">#</a></h4>
<p>When we have correlated random components, we are also at the stake of assuming misspecified distributions on our statistical tests. Alternative modelling could deal with this matter (e.g., <strong>mixed-effects models</strong> to be covered in this course). Again, this correlation leads to misspecified distributions in the <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(F\)</span>-tests since the test statistics heavily rely on independence under the null hypothesis <span class="math notranslate nohighlight">\(H_0\)</span>.</p>
<div class="warning admonition">
<p class="admonition-title">Note</p>
<p>The independence between random components in OLS could be confirmed via a <strong>Durbin-Watson test</strong>. You can find more information about this test <a class="reference external" href="https://www.statology.org/durbin-watson-test/"><strong>here</strong></a>.</p>
</div>
</section>
</section>
</section>
<section id="when-ordinary-least-squares-regression-does-not-suffice">
<h2>3. When Ordinary Least-squares Regression Does Not Suffice<a class="headerlink" href="#when-ordinary-least-squares-regression-does-not-suffice" title="Permalink to this heading">#</a></h2>
<p>The OLS model from <strong>DSCI 561</strong> allows the response to take on any real number. Nonetheless, this is not entirely true in many real-life datasets.</p>
<p>We usually encounter cases where the response’s range is restricted. Therefore, this linear regression model is not suitable. Thus, <strong>what can we do about it?</strong></p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The statistical literature offers an interesting set of regression models that could deal with different types of responses in real life. This set of models will be introduced in this course while providing illustrative examples we might encounter in data analysis.</p>
</div>
<p>We could list some examples where the response is not suitable for the OLS model:</p>
<ul class="simple">
<li><p><strong>Non-negative values.</strong> The median value of owner-occupied homes in USD 1000’s (<code class="docutils literal notranslate"><span class="pre">medv</span></code>) in the dataset <code class="docutils literal notranslate"><span class="pre">BostonHousing</span></code> <a class="reference external" href="https://www.sciencedirect.com/science/article/pii/0095069678900062">(Harrison and Rubinfeld, 1978)</a>.</p></li>
</ul>
<div class="admonition-the-boston-housing-dataset admonition">
<p class="admonition-title">The Boston Housing Dataset </p>
<p>The dataset <code class="docutils literal notranslate"><span class="pre">BostonHousing</span></code> contains information of 506 tracts of Boston from the 1970 US Census. Suppose we want to make inference or predict the response <code class="docutils literal notranslate"><span class="pre">medv</span></code>, subject to the other 13 regressors in the dataset. In this case, the nature of the response does not allow it to take on negative values.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">data</span><span class="p">(</span><span class="n">BostonHousing</span><span class="p">)</span>
<span class="nf">str</span><span class="p">(</span><span class="n">BostonHousing</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;data.frame&#39;:	506 obs. of  14 variables:
 $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...
 $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...
 $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...
 $ chas   : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ...
 $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...
 $ rm     : num  6.58 6.42 7.18 7 7.15 ...
 $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...
 $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...
 $ rad    : num  1 2 2 3 3 3 5 5 5 5 ...
 $ tax    : num  296 242 242 222 222 222 311 311 311 311 ...
 $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...
 $ b      : num  397 397 393 395 397 ...
 $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...
 $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><strong>Binary outcomes (Success or Failure).</strong> Whether a tumour is <code class="docutils literal notranslate"><span class="pre">benign</span></code> or <code class="docutils literal notranslate"><span class="pre">malignant</span></code> (<code class="docutils literal notranslate"><span class="pre">Class</span></code>) in the dataset <code class="docutils literal notranslate"><span class="pre">BreastCancer</span></code> <a class="reference external" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC55130/">(Wolberg and Mangasarian, 1990)</a>.</p></li>
</ul>
<div class="admonition-the-breast-cancer-dataset admonition">
<p class="admonition-title">The Breast Cancer Dataset </p>
<p>The dataset <code class="docutils literal notranslate"><span class="pre">BreastCancer</span></code> contains information of 699 biopsy results. Suppose we want to make inference or predict the response <code class="docutils literal notranslate"><span class="pre">Class</span></code>, subject to the other 9 regressors in the dataset (except <code class="docutils literal notranslate"><span class="pre">Id</span></code>). The response is discrete of binary type.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">data</span><span class="p">(</span><span class="n">BreastCancer</span><span class="p">)</span>
<span class="nf">str</span><span class="p">(</span><span class="n">BreastCancer</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;data.frame&#39;:	699 obs. of  11 variables:
 $ Id             : chr  &quot;1000025&quot; &quot;1002945&quot; &quot;1015425&quot; &quot;1016277&quot; ...
 $ Cl.thickness   : Ord.factor w/ 10 levels &quot;1&quot;&lt;&quot;2&quot;&lt;&quot;3&quot;&lt;&quot;4&quot;&lt;..: 5 5 3 6 4 8 1 2 2 4 ...
 $ Cell.size      : Ord.factor w/ 10 levels &quot;1&quot;&lt;&quot;2&quot;&lt;&quot;3&quot;&lt;&quot;4&quot;&lt;..: 1 4 1 8 1 10 1 1 1 2 ...
 $ Cell.shape     : Ord.factor w/ 10 levels &quot;1&quot;&lt;&quot;2&quot;&lt;&quot;3&quot;&lt;&quot;4&quot;&lt;..: 1 4 1 8 1 10 1 2 1 1 ...
 $ Marg.adhesion  : Ord.factor w/ 10 levels &quot;1&quot;&lt;&quot;2&quot;&lt;&quot;3&quot;&lt;&quot;4&quot;&lt;..: 1 5 1 1 3 8 1 1 1 1 ...
 $ Epith.c.size   : Ord.factor w/ 10 levels &quot;1&quot;&lt;&quot;2&quot;&lt;&quot;3&quot;&lt;&quot;4&quot;&lt;..: 2 7 2 3 2 7 2 2 2 2 ...
 $ Bare.nuclei    : Factor w/ 10 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 10 2 4 1 10 10 1 1 1 ...
 $ Bl.cromatin    : Factor w/ 10 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 3 3 3 3 3 9 3 3 1 2 ...
 $ Normal.nucleoli: Factor w/ 10 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 2 1 7 1 7 1 1 1 1 ...
 $ Mitoses        : Factor w/ 9 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 1 1 1 1 1 5 1 ...
 $ Class          : Factor w/ 2 levels &quot;benign&quot;,&quot;malignant&quot;: 1 1 1 1 1 2 1 1 1 1 ...
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><strong>Count data.</strong> The number of physician office visits (<code class="docutils literal notranslate"><span class="pre">visits</span></code>) in the dataset <code class="docutils literal notranslate"><span class="pre">NMES1988</span></code> <a class="reference external" href="https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291099-1255%28199705%2912%3A3%3C313%3A%3AAID-JAE440%3E3.0.CO%3B2-G">(Deb and Trivedi, 1997)</a>. This response takes on count values (<span class="math notranslate nohighlight">\(0, 1, 2, 3, \dots\)</span>).</p></li>
</ul>
<div class="admonition-the-us-national-medical-expenditure-survey-dataset admonition">
<p class="admonition-title">The US National Medical Expenditure Survey Dataset </p>
<p>The <code class="docutils literal notranslate"><span class="pre">NMES1988</span></code> dataset contains cross-sectional data from the US National Medical Expenditure Survey (NMES) between 1987 and 1988. It is a sample of 4,406 individuals of ages 66 and above covered by Medicare with 19 different variables. Suppose we are interested in making inference or predicting the number of <code class="docutils literal notranslate"><span class="pre">visits</span></code> subject to regressors <code class="docutils literal notranslate"><span class="pre">age</span></code>, <code class="docutils literal notranslate"><span class="pre">gender</span></code>, and <code class="docutils literal notranslate"><span class="pre">income</span></code>.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">data</span><span class="p">(</span><span class="n">NMES1988</span><span class="p">)</span>
<span class="nf">str</span><span class="p">(</span><span class="n">NMES1988</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;data.frame&#39;:	4406 obs. of  19 variables:
 $ visits   : int  5 1 13 16 3 17 9 3 1 0 ...
 $ nvisits  : int  0 0 0 0 0 0 0 0 0 0 ...
 $ ovisits  : int  0 2 0 5 0 0 0 0 0 0 ...
 $ novisits : int  0 0 0 0 0 0 0 0 0 0 ...
 $ emergency: int  0 2 3 1 0 0 0 0 0 0 ...
 $ hospital : int  1 0 3 1 0 0 0 0 0 0 ...
 $ health   : Factor w/ 3 levels &quot;poor&quot;,&quot;average&quot;,..: 2 2 1 1 2 1 2 2 2 2 ...
  ..- attr(*, &quot;contrasts&quot;)= num [1:3, 1:2] 1 0 0 0 0 1
  .. ..- attr(*, &quot;dimnames&quot;)=List of 2
  .. .. ..$ : chr [1:3] &quot;poor&quot; &quot;average&quot; &quot;excellent&quot;
  .. .. ..$ : chr [1:2] &quot;poor&quot; &quot;excellent&quot;
 $ chronic  : int  2 2 4 2 2 5 0 0 0 0 ...
 $ adl      : Factor w/ 2 levels &quot;normal&quot;,&quot;limited&quot;: 1 1 2 2 2 2 1 1 1 1 ...
 $ region   : Factor w/ 4 levels &quot;northeast&quot;,&quot;midwest&quot;,..: 4 4 4 4 4 4 2 2 2 2 ...
  ..- attr(*, &quot;contrasts&quot;)= num [1:4, 1:3] 1 0 0 0 0 1 0 0 0 0 ...
  .. ..- attr(*, &quot;dimnames&quot;)=List of 2
  .. .. ..$ : chr [1:4] &quot;northeast&quot; &quot;midwest&quot; &quot;west&quot; &quot;other&quot;
  .. .. ..$ : chr [1:3] &quot;northeast&quot; &quot;midwest&quot; &quot;west&quot;
 $ age      : num  6.9 7.4 6.6 7.6 7.9 6.6 7.5 8.7 7.3 7.8 ...
 $ afam     : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 1 2 1 1 1 1 1 1 1 ...
 $ gender   : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 1 2 1 1 1 1 1 1 ...
 $ married  : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 2 1 2 2 1 1 1 1 1 ...
 $ school   : int  6 10 10 3 6 7 8 8 8 8 ...
 $ income   : num  2.881 2.748 0.653 0.659 0.659 ...
 $ employed : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 1 1 1 1 1 1 1 1 1 ...
 $ insurance: Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 2 1 2 2 1 2 2 2 2 ...
 $ medicaid : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 2 1 1 2 1 1 1 1 ...
</pre></div>
</div>
</div>
</div>
</section>
<section id="paving-the-way-to-generalized-linear-models">
<h2>4. Paving the Way to Generalized Linear Models<a class="headerlink" href="#paving-the-way-to-generalized-linear-models" title="Permalink to this heading">#</a></h2>
<p>When we are using a set of regressors (or <strong>predictors</strong>) to explain (or <strong>predict</strong>) our response, we have to establish a mathematical relationship between them. This is called a <strong>functional form</strong> (i.e., <strong>model function</strong>). For instance, in the case of the regressor <span class="math notranslate nohighlight">\(X\)</span> and response <span class="math notranslate nohighlight">\(Y\)</span>, we could have:</p>
<ul class="simple">
<li><p><strong>Linear:</strong> <span class="math notranslate nohighlight">\(Y = \beta_0 + \beta_1 X\)</span>.</p></li>
<li><p><strong>Exponential:</strong> <span class="math notranslate nohighlight">\(Y = e^{\beta_0 + \beta_1 X}\)</span>.</p></li>
<li><p><strong>In general:</strong> <span class="math notranslate nohighlight">\(Y = f(X)\)</span>.</p></li>
</ul>
<section id="nature-of-the-model-function">
<h3>4.1. Nature of the Model Function<a class="headerlink" href="#nature-of-the-model-function" title="Permalink to this heading">#</a></h3>
<p>Once we establish the model function between our variables, we have to specify the nature of it:</p>
<ul class="simple">
<li><p><strong>Deterministic.</strong> For each one of the values of the regressor <span class="math notranslate nohighlight">\(X\)</span>, there is a single value of <span class="math notranslate nohighlight">\(Y\)</span>.</p></li>
<li><p><strong>Stochastic.</strong>  Each value of <span class="math notranslate nohighlight">\(X\)</span> has a probability distribution associated to <span class="math notranslate nohighlight">\(Y\)</span>. Recall the <a class="reference external" href="https://pages.github.ubc.ca/MDS-2023-24/DSCI_561_regr-1_students/lectures/01_intro-lr-class.html#id2"><strong>this plot</strong></a> from <strong>DSCI 561</strong>.</p></li>
</ul>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/371f36cd214a6f7624b7933ce9795b5f24e49355b44a2b7716dc25975822c5a5.png" src="../_images/371f36cd214a6f7624b7933ce9795b5f24e49355b44a2b7716dc25975822c5a5.png" />
</div>
</div>
</section>
<section id="the-regression-problem">
<h3>4.2. The Regression Problem<a class="headerlink" href="#the-regression-problem" title="Permalink to this heading">#</a></h3>
<p>The term <strong>regression</strong> can be extended beyond a linear relationship between the response and regressors. From <a class="reference internal" href="#ds-workflow-preamble"><span class="std std-ref">1. A Preamble on our Data Science Workflow</span></a>, a regression model can be used for two purposes:</p>
<ul class="simple">
<li><p><strong>Inference.</strong> We want to determine whether there is a significant statistical association/causation between the response and regressor (e.g., <span class="math notranslate nohighlight">\(t\)</span>-tests in OLS regression) and estimate the <strong>effect size</strong>.</p>
<ul>
<li><p>There is uncertainty associated with this estimation (confidence intervals).</p></li>
</ul>
</li>
<li><p><strong>Prediction.</strong> Given new values for the regressors, we want to predict the corresponding value of the response subject to the effect estimates.</p>
<ul>
<li><p>There is uncertainty associated with this prediction (prediction intervals).</p></li>
</ul>
</li>
</ul>
</section>
<section id="black-box-models">
<h3>4.3. Black-box Models<a class="headerlink" href="#black-box-models" title="Permalink to this heading">#</a></h3>
<p>A model such as in the case of OLS regression, specifies the functional form between the regressors and the response along with assumptions on the system or phenomenon we aim to model. This allows interpretability.</p>
<p>On the other hand, a <strong>black-box model</strong> is focused on <strong>optimizing predictions</strong> subject to a set of regressors with less attention on the internal model’s process.</p>
</section>
<section id="interpretability-in-linear-models">
<h3>4.4. Interpretability in Linear Models<a class="headerlink" href="#interpretability-in-linear-models" title="Permalink to this heading">#</a></h3>
<p>A crucial characteristic of linear models is their <strong>relative easiness to interpret the effects of the regressors on the response</strong> (via the model’s coefficients). Moreover, their predictive ability is fair in general. An additional takeaway on this class of tools is their ability to go beyond the conditioned modelling of the response’s means (e.g., <strong>medians or certain quantiles</strong>).</p>
</section>
<section id="the-types-of-parametric-assumptions">
<h3>4.5. The Types of Parametric Assumptions<a class="headerlink" href="#the-types-of-parametric-assumptions" title="Permalink to this heading">#</a></h3>
<p>The concept of a <strong>parametric model</strong> varies depending on the field:</p>
<ul class="simple">
<li><p>In <strong>Computer Science</strong>, a parametric model assumes a functional relationship between the regressors and response (e.g., linear).</p></li>
<li><p>In <strong>Statistics</strong>, a parametric model has distributional assumptions on its components.</p></li>
</ul>
</section>
<section id="an-example-where-ols-regression-totally-goes-wrong">
<h3>4.6. An example where OLS regression totally goes wrong<a class="headerlink" href="#an-example-where-ols-regression-totally-goes-wrong" title="Permalink to this heading">#</a></h3>
<p>We have to be careful when using linear models in specific complex datasets if we only appeal to their easiness of interpretability. When we do not capture the right functional form between the regressors and the response, the chances of having a misspecified model are high. For the sake of this example, we will build a simulated dataset similar to the previous <strong>Example 3</strong>. Assume that the true functional form in this 2-<span class="math notranslate nohighlight">\(d\)</span> example is as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
Y_i &amp;= \beta_0 + \beta_1 g(X_i) + \varepsilon_i \\
&amp;= \beta_0 + \beta_1 \sin(X_i) + \varepsilon_i \\
&amp;= 5 + 10 \sin(X_i) + \varepsilon_i
\end{align*}\end{split}\]</div>
<p>with</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{gather*}
\mathbb{E}(\varepsilon_i) = 0 \\
\text{Var}(\varepsilon_i) = \sigma^2 \\
\varepsilon_i \sim \mathcal{N}(0, \sigma^2) \\
\varepsilon_i \perp \!\!\! \perp \varepsilon_k \; \; \; \; \text{for} \; i \neq k  \; \; \; \; \text{(independence)}.
\end{gather*}\end{split}\]</div>
<p>We simulate a dataset of <span class="math notranslate nohighlight">\(n = 234\)</span> observations with <span class="math notranslate nohighlight">\(x_i \in [2, 13.65]\)</span>. <a class="reference external" href="https://www.rdocumentation.org/packages/compositions/versions/2.0-8/topics/rnorm"><strong>Function <code class="docutils literal notranslate"><span class="pre">rnorm()</span></code></strong></a> provides our Normal error components with the parameters specified above. The code below simulates these data and then provides a scatterplot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">set.seed</span><span class="p">(</span><span class="m">123</span><span class="p">)</span>
<span class="n">sin_data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">tibble</span><span class="p">(</span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">seq</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">13.65</span><span class="p">,</span><span class="w"> </span><span class="m">0.05</span><span class="p">),</span><span class="w"> </span><span class="n">Y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">10</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nf">sin</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nf">rnorm</span><span class="p">(</span><span class="nf">length</span><span class="p">(</span><span class="nf">seq</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">13.65</span><span class="p">,</span><span class="w"> </span><span class="m">0.05</span><span class="p">)),</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">1.5</span><span class="p">))</span>
<span class="n">sin_data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A tibble: 234 × 2</caption>
<thead>
	<tr><th scope=col>X</th><th scope=col>Y</th></tr>
	<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>2.00</td><td>13.25226</td></tr>
	<tr><td>2.05</td><td>13.52836</td></tr>
	<tr><td>2.10</td><td>15.97016</td></tr>
	<tr><td>⋮</td><td>⋮</td></tr>
	<tr><td>13.55</td><td>13.18966</td></tr>
	<tr><td>13.60</td><td>13.91343</td></tr>
	<tr><td>13.65</td><td>12.72883</td></tr>
</tbody>
</table>
</div></div>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>From the above code, given that we are running a data simulation, note that the real values for our regression parameters are actually <span class="math notranslate nohighlight">\(\beta_0 = 5\)</span> and <span class="math notranslate nohighlight">\(\beta_1 = 10\)</span>. Recall that we will not know these real values in practice, and <strong>it is in our best interest to estimate them via suitable modelling on our training data</strong>.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">sin_data</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">ggplot</span><span class="p">()</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_point</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">Y</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_smooth</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">Y</span><span class="p">),</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;lm&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">se</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;red&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">ggtitle</span><span class="p">(</span><span class="nf">bquote</span><span class="p">(</span><span class="s">&quot;Linear regression of &quot;</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">beta</span><span class="p">[</span><span class="m">0</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">beta</span><span class="p">[</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">epsilon</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">theme</span><span class="p">(</span>
<span class="w">      </span><span class="n">plot.title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">30</span><span class="p">),</span>
<span class="w">      </span><span class="n">axis.text</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">21</span><span class="p">),</span>
<span class="w">      </span><span class="n">axis.title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">27</span><span class="p">)</span>
<span class="w">    </span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>`geom_smooth()` using formula = &#39;y ~ x&#39;
</pre></div>
</div>
<img alt="../_images/5f61b7be321e092ff5be218b348d08a74f138176741a94a89944dfd520b4846f.png" src="../_images/5f61b7be321e092ff5be218b348d08a74f138176741a94a89944dfd520b4846f.png" />
</div>
</div>
<p>The code above also fits a linear regression, with <code class="docutils literal notranslate"><span class="pre">method</span> <span class="pre">=</span> <span class="pre">&quot;lm&quot;</span></code> via <code class="docutils literal notranslate"><span class="pre">geom_smooth()</span></code>, assuming that the “<em>right functional form</em>” is</p>
<div class="math notranslate nohighlight">
\[Y_i = \beta_0^* + \beta_1^* X_i + \varepsilon_i.\]</div>
<p>Nonetheless, using <code class="docutils literal notranslate"><span class="pre">lm()</span></code> in <code class="docutils literal notranslate"><span class="pre">sin_function_model</span></code>, the fitted regression line is <strong>almost flat</strong> (i.e., <span class="math notranslate nohighlight">\(\hat{\beta}_1^* = -0.033\)</span>) and located on <span class="math notranslate nohighlight">\(y = 4.545\)</span> (i.e., <span class="math notranslate nohighlight">\(\hat{\beta}_0^* = 4.545\)</span>). Note that the slope is not even significant given a <span class="math notranslate nohighlight">\(p \text{-value} = 0.806\)</span>.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The below result is expected in a <strong>misspecified model</strong> since the OLS method attempts to reduce the squared distance between the observed and estimated response values <strong>across all points</strong>. Hence, this attempt has a serious impact on a sinusoidal curve, like in this example.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">sin_function_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">Y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sin_data</span><span class="p">)</span>
<span class="nf">tidy</span><span class="p">(</span><span class="n">sin_function_model</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">mutate_if</span><span class="p">(</span><span class="n">is.numeric</span><span class="p">,</span><span class="w"> </span><span class="n">round</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A tibble: 2 × 5</caption>
<thead>
	<tr><th scope=col>term</th><th scope=col>estimate</th><th scope=col>std.error</th><th scope=col>statistic</th><th scope=col>p.value</th></tr>
	<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>(Intercept)</td><td> 4.545</td><td>1.136</td><td> 4.001</td><td>0.000</td></tr>
	<tr><td>X          </td><td>-0.033</td><td>0.133</td><td>-0.246</td><td>0.806</td></tr>
</tbody>
</table>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">glance</span><span class="p">(</span><span class="n">sin_function_model</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">mutate_if</span><span class="p">(</span><span class="n">is.numeric</span><span class="p">,</span><span class="w"> </span><span class="n">round</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A tibble: 1 × 12</caption>
<thead>
	<tr><th scope=col>r.squared</th><th scope=col>adj.r.squared</th><th scope=col>sigma</th><th scope=col>statistic</th><th scope=col>p.value</th><th scope=col>df</th><th scope=col>logLik</th><th scope=col>AIC</th><th scope=col>BIC</th><th scope=col>deviance</th><th scope=col>df.residual</th><th scope=col>nobs</th></tr>
	<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>0</td><td>-0.004</td><td>6.886</td><td>0.061</td><td>0.806</td><td>1</td><td>-782.543</td><td>1571.086</td><td>1581.452</td><td>11002.15</td><td>232</td><td>234</td></tr>
</tbody>
</table>
</div></div>
</div>
<div class="warning admonition">
<p class="admonition-title">Note</p>
<p>Before concluding anything about the model estimates, we have to make sure that we are setting up an adequate functional form. This is why <a class="reference external" href="https://github.com/UBC-MDS/DSCI_573_feat-model-select"><strong>feature engineering</strong></a> is essential as in <strong>DSCI 573</strong>.</p>
</div>
</section>
<section id="let-us-make-it-right">
<h3>4.7. Let us make it right!<a class="headerlink" href="#let-us-make-it-right" title="Permalink to this heading">#</a></h3>
<p>Having fitted a misspecified model as we previously did, let us now make it right. Still, via <code class="docutils literal notranslate"><span class="pre">sin_data</span></code>, we will fit another OLS. However, this time, we will fit the model with a corresponding regressor sinusoidal transformation on column <code class="docutils literal notranslate"><span class="pre">X</span></code> such that:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">sin_data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sin_data</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">sin_X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">sin</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="n">sin_data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A tibble: 234 × 3</caption>
<thead>
	<tr><th scope=col>X</th><th scope=col>Y</th><th scope=col>sin_X</th></tr>
	<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>2.00</td><td>13.25226</td><td>0.9092974</td></tr>
	<tr><td>2.05</td><td>13.52836</td><td>0.8873624</td></tr>
	<tr><td>2.10</td><td>15.97016</td><td>0.8632094</td></tr>
	<tr><td>⋮</td><td>⋮</td><td>⋮</td></tr>
	<tr><td>13.55</td><td>13.18966</td><td>0.8325135</td></tr>
	<tr><td>13.60</td><td>13.91343</td><td>0.8591618</td></tr>
	<tr><td>13.65</td><td>12.72883</td><td>0.8836626</td></tr>
</tbody>
</table>
</div></div>
</div>
<p>Then, we refit our OLS model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">sin_function_right_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">Y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">sin_X</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sin_data</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Then, let us check our <code class="docutils literal notranslate"><span class="pre">tidy()</span></code> output along with the corresponing 95% confidence intervals, and the <code class="docutils literal notranslate"><span class="pre">glance()</span></code> output:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">tidy</span><span class="p">(</span><span class="n">sin_function_right_model</span><span class="p">,</span><span class="w"> </span><span class="n">conf.int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">mutate_if</span><span class="p">(</span><span class="n">is.numeric</span><span class="p">,</span><span class="w"> </span><span class="n">round</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A tibble: 2 × 7</caption>
<thead>
	<tr><th scope=col>term</th><th scope=col>estimate</th><th scope=col>std.error</th><th scope=col>statistic</th><th scope=col>p.value</th><th scope=col>conf.low</th><th scope=col>conf.high</th></tr>
	<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>(Intercept)</td><td>4.996</td><td>0.093</td><td>53.689</td><td>0</td><td>4.813</td><td> 5.179</td></tr>
	<tr><td>sin_X      </td><td>9.870</td><td>0.136</td><td>72.524</td><td>0</td><td>9.602</td><td>10.139</td></tr>
</tbody>
</table>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">glance</span><span class="p">(</span><span class="n">sin_function_right_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A tibble: 1 × 12</caption>
<thead>
	<tr><th scope=col>r.squared</th><th scope=col>adj.r.squared</th><th scope=col>sigma</th><th scope=col>statistic</th><th scope=col>p.value</th><th scope=col>df</th><th scope=col>logLik</th><th scope=col>AIC</th><th scope=col>BIC</th><th scope=col>deviance</th><th scope=col>df.residual</th><th scope=col>nobs</th></tr>
	<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th></tr>
</thead>
<tbody>
	<tr><td>0.9577549</td><td>0.9575728</td><td>1.415598</td><td>5259.759</td><td>2.080159e-161</td><td>1</td><td>-412.3544</td><td>830.7089</td><td>841.0749</td><td>464.9087</td><td>232</td><td>234</td></tr>
</tbody>
</table>
</div></div>
</div>
<p>Right on! We made an <strong>accurate estimation</strong> of <span class="math notranslate nohighlight">\(\beta_0 = 5\)</span> and <span class="math notranslate nohighlight">\(\beta_1 = 10\)</span>, which is <span class="math notranslate nohighlight">\(\hat{\beta}_0 = 4.996\)</span> and <span class="math notranslate nohighlight">\(\hat{\beta}_1 = 9.870\)</span> whose confidence intervals give us a good insight on the low uncertainty of these point estimates (i.e., we caught the <strong>real regression parameters</strong> within the intervals). Moreover, we have stistical significance on our regression estimates (check the <code class="docutils literal notranslate"><span class="pre">p.value</span></code> column in the <code class="docutils literal notranslate"><span class="pre">tidy()</span></code> output).</p>
<p>Moreover, the <code class="docutils literal notranslate"><span class="pre">adj.r.squared</span></code> is quite acceptable: <span class="math notranslate nohighlight">\(0.9575728\)</span>.</p>
<p>Now, let us plot the <strong>in-sample predictions</strong> from <code class="docutils literal notranslate"><span class="pre">sin_function_right_model</span></code> in blue along with the ones from the misspecified <code class="docutils literal notranslate"><span class="pre">sin_function_model</span></code> in red (we also include the 95% <strong>confidence intervals for prediction</strong> as shaded gray areas):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">ggplot</span><span class="p">(</span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sin_data</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_point</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">Y</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_smooth</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">Y</span><span class="p">),</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;lm&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">formula</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="nf">sin</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;blue&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_smooth</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">Y</span><span class="p">),</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;lm&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">formula</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;red&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">ggtitle</span><span class="p">(</span><span class="s">&quot;Linear regressions&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">theme</span><span class="p">(</span>
<span class="w">      </span><span class="n">plot.title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">30</span><span class="p">),</span>
<span class="w">      </span><span class="n">axis.text</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">21</span><span class="p">),</span>
<span class="w">      </span><span class="n">axis.title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">27</span><span class="p">)</span>
<span class="w">    </span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1c4d6364c6261f44f6a26e62df19fa338a42a504eab3a63fcc15bdc9bccaba77.png" src="../_images/1c4d6364c6261f44f6a26e62df19fa338a42a504eab3a63fcc15bdc9bccaba77.png" />
</div>
</div>
<p>Note our blue estimated regression line via <code class="docutils literal notranslate"><span class="pre">sin_function_right_model</span></code> provides a great fit for our training data, which is not the case for the red estimated regression curve from the misspecified <code class="docutils literal notranslate"><span class="pre">sin_function_model</span></code>. Moreover, our confidence intervals for prediction for <code class="docutils literal notranslate"><span class="pre">sin_function_right_model</span></code> are narrower, indicating a low uncertainty on our predictions (compared to <code class="docutils literal notranslate"><span class="pre">sin_function_model</span></code>).</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>We can repeat the above prediction procedure by using a further test set (different from the training set <code class="docutils literal notranslate"><span class="pre">sin_data</span></code>) via estimates from <code class="docutils literal notranslate"><span class="pre">sin_function_right_model</span></code>. That said, you must transform your raw column <span class="math notranslate nohighlight">\(x\)</span> from this test set to <span class="math notranslate nohighlight">\(\sin(x)\)</span> before obtaining the corresponding prediction <span class="math notranslate nohighlight">\(\hat{y}\)</span>.</p>
<p>Moreover, note this transformation on the regressor <span class="math notranslate nohighlight">\(X\)</span> <strong>is not equivalent</strong> to the link function idea from GLMs. In this example, <strong>we are basically making feature engineering</strong>.</p>
</div>
</section>
<section id="restricted-response-ranges-in-linear-regression">
<h3>4.8. Restricted Response Ranges in Linear Regression<a class="headerlink" href="#restricted-response-ranges-in-linear-regression" title="Permalink to this heading">#</a></h3>
<p>We initially listed different response types where the ranges are restricted. Now, the next matter to address is how to retain the easiness in model’s interpretability. Recall that we might be using the model for inference purposes, and not predictions. Hence, a black-box model will not provide a straightforward interpretation.</p>
<p>We could use the following three modelling alternatives:</p>
<ul class="simple">
<li><p><strong>Data transformations.</strong> For example, logarithmic transformations on the response.</p></li>
<li><p><strong>Scientifically-backed functions.</strong> We rely on subject-matter expertise. This backs regression modelling with actual scientific models. The theoretically-derived setup will indicate a meaningful relationship between response and regressors, whose parameters will be estimated with the model fitting.</p></li>
<li><p><strong>Link functions.</strong> This a core concept in GLMs!</p></li>
</ul>
</section>
<section id="link-function">
<span id="id2"></span><h3>4.9. Link Function<a class="headerlink" href="#link-function" title="Permalink to this heading">#</a></h3>
<p>Recall OLS regression models a continuous response <span class="math notranslate nohighlight">\(Y_i\)</span> (a random variable) via its conditioned mean (or expected value) <span class="math notranslate nohighlight">\(\mu_i\)</span> subject to <span class="math notranslate nohighlight">\(k\)</span> regressors <span class="math notranslate nohighlight">\(X_{i,j}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mu_i = \mathbb{E}(Y_i \mid X_{i,1}, \ldots, X_{i,k}) = \beta_0 + \beta_1 X_{i,1} + \ldots + \beta_k X_{i,k} \; \; \text{since} \; \; \mathbb{E}(\varepsilon_i) = 0.
\]</div>
<p>Nonetheless, for instance, modelling the mean <span class="math notranslate nohighlight">\(\mu_i\)</span> of a discrete-type response (such as binary or a count) is not straightforward. Hence, we rely on a <strong>monotonic</strong> and <strong>differentiable</strong> function <span class="math notranslate nohighlight">\(h(\mu_i)\)</span> called the <strong>link function</strong>:</p>
<div class="math notranslate nohighlight">
\[
h(\mu_i) = \eta_i = \beta_0 + \beta_1 X_{i,1} + \beta_2 X_{i,2} + \ldots + \beta_k X_{i,k} \; \; \; \; \text{for} \; i = 1, \ldots, n.
\]</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The link function <span class="math notranslate nohighlight">\(h(\mu_i)\)</span> is a crucial element in a GLM since it allows us to establish the functional relationship between the response and the regressor in this class of linear model. Note that the form of this link function will also change how we will interpret our estimated regression coefficients <span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span>.</p>
</div>
<div class="warning admonition">
<p class="admonition-title">Note</p>
<p>A GLM regression approach can also be applied to continuous responses. For instance, we can assume <a class="reference external" href="https://cran.r-project.org/web/packages/GlmSimulatoR/vignettes/exploring_links_for_the_gaussian_distribution.html"><strong>Normal</strong></a> or <a class="reference external" href="https://pj.freefaculty.org/guides/stat/Regression-GLM/Gamma/GammaGLM-01.pdf"><strong>Gamma</strong></a>-distributed responses.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Why monotonic and differentiable?</p>
<ul class="simple">
<li><p>The link function needs to be monotonic so we can allow putting the systematic component <span class="math notranslate nohighlight">\(\eta_i\)</span> in terms of the corresponding mean <span class="math notranslate nohighlight">\(\mu_i\)</span>, i.e.:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\mu_i = h^{-1}(\eta_i).\]</div>
<ul class="simple">
<li><p>Furthermore, it needs to be differentiable <strong>since we rely on MLE</strong> to obtain <span class="math notranslate nohighlight">\(\hat{\beta}_0, \hat{\beta}_1, \dots, \hat{\beta}_k\)</span>.</p></li>
</ul>
</div>
<p>That said, a GLM has the components of the conceptual regression model in a training set of <span class="math notranslate nohighlight">\(n\)</span> elements as:</p>
<ul class="simple">
<li><p><strong>Random component.</strong> Each <em>response</em> <span class="math notranslate nohighlight">\(Y_1,\ldots,Y_n\)</span> is a random variable with its respective mean <span class="math notranslate nohighlight">\(\mu_i\)</span>.</p></li>
<li><p><strong>Systematic component.</strong> How the <span class="math notranslate nohighlight">\(k\)</span> regressors come into the model denoted as a <strong>linear combination</strong>:</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-eq-glm-systematic-component">
<span class="eqno">(5)<a class="headerlink" href="#equation-eq-glm-systematic-component" title="Permalink to this equation">#</a></span>\[
\eta_i = \beta_0 + \beta_1 X_{i,1} + \beta_2 X_{i,2} + \ldots + \beta_k X_{i,k} \; \; \; \; \text{for} \; i = 1, \ldots, n.
\]</div>
<ul class="simple">
<li><p><strong>Link function.</strong> The element that connects the <strong>random component</strong> with the <strong>systematic component</strong> <span class="math notranslate nohighlight">\(\eta_i\)</span>. The connection is made through <span class="math notranslate nohighlight">\(h(\mu_i)\)</span>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
h(\mu_i) = \eta_i.
\]</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The <strong>linear model</strong> in <a class="reference internal" href="#equation-eq-glm-systematic-component">(5)</a> does not have an explicit <strong>random component</strong> as in the OLS model with a continuous response. <strong>The randomness is expressed directly in the response <span class="math notranslate nohighlight">\(Y_i\)</span> whose mean is <span class="math notranslate nohighlight">\(\mu_i\)</span>.</strong> Moreover, this randomness is also reflected in the joint probability distribution of our training data when using MLE.</p>
</div>
</section>
</section>
<section id="poisson-regression">
<h2>5. Poisson Regression<a class="headerlink" href="#poisson-regression" title="Permalink to this heading">#</a></h2>
<p>Moving along with GLMs, let us expand the regression mind map as in <a class="reference internal" href="#reg-mindmap-2"><span class="std std-numref">Fig. 4</span></a> to include Binary Logistic regression (the model from <strong>DSCI 561</strong>) along with a new class of GLMs to model count-type responses: <strong>Poisson and Negative Binomial regressions</strong>. Further in this lecture, we will explain what <strong>equidispersion</strong> and <strong>overdispersion</strong> stand for.</p>
<figure class="align-default" id="reg-mindmap-2">
<a class="reference internal image-reference" href="../_images/reg-mindmap-2.png"><img alt="../_images/reg-mindmap-2.png" src="../_images/reg-mindmap-2.png" style="height: 1000px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4 </span><span class="caption-text">Expanded regression modelling mind map.</span><a class="headerlink" href="#reg-mindmap-2" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Both Poisson and Negative Binomial regressions belong to an overall GLM modelling approach called <strong>count regression</strong>. As its name says, this class of GLMs addresses count-type responses (i.e., integers). As in any other GLM, there will be a <strong>link function</strong> which will allow us <strong>to relate the systematic component with the response’s mean</strong>.</p>
<p>Note Poisson regression is the primary approach we should use when handling a count response.</p>
</div>
<section id="the-crabs-dataset">
<h3>5.1. The Crabs Dataset<a class="headerlink" href="#the-crabs-dataset" title="Permalink to this heading">#</a></h3>
<figure class="align-default" id="id9">
<a class="reference internal image-reference" href="../_images/crab.png"><img alt="../_images/crab.png" src="../_images/crab.png" style="height: 350px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5 </span><span class="caption-text">The Crab comes back.</span><a class="headerlink" href="#id9" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Let us introduce the dataset for this new class of GLMs. The data frame <code class="docutils literal notranslate"><span class="pre">crabs</span></code> (<a class="reference external" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1439-0310.1996.tb01099.x">Brockmann, 1996</a> is a dataset detailing the <strong>counts of satellite male crabs</strong> residing around a female crab nest. The code below renames the original response’s name, <code class="docutils literal notranslate"><span class="pre">satell</span></code>, to <code class="docutils literal notranslate"><span class="pre">n_males</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">data</span><span class="p">(</span><span class="n">crabs</span><span class="p">)</span>
<span class="n">crabs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">crabs</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">rename</span><span class="p">(</span><span class="n">n_males</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">satell</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="n">dplyr</span><span class="o">::</span><span class="nf">select</span><span class="p">(</span><span class="o">-</span><span class="n">y</span><span class="p">)</span>
<span class="n">crabs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A data.frame: 173 × 5</caption>
<thead>
	<tr><th scope=col>color</th><th scope=col>spine</th><th scope=col>width</th><th scope=col>n_males</th><th scope=col>weight</th></tr>
	<tr><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th></tr>
</thead>
<tbody>
	<tr><td>medium</td><td>bad </td><td>28.3</td><td>8</td><td>3050</td></tr>
	<tr><td>dark  </td><td>bad </td><td>22.5</td><td>0</td><td>1550</td></tr>
	<tr><td>light </td><td>good</td><td>26.0</td><td>9</td><td>2300</td></tr>
	<tr><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td></tr>
	<tr><td>light </td><td>good  </td><td>28.0</td><td>0</td><td>2625</td></tr>
	<tr><td>darker</td><td>bad   </td><td>27.0</td><td>0</td><td>2625</td></tr>
	<tr><td>medium</td><td>middle</td><td>24.5</td><td>0</td><td>2000</td></tr>
</tbody>
</table>
</div></div>
</div>
<div class="admonition-the-crabs-dataset admonition">
<p class="admonition-title">The Crabs Dataset</p>
<p>The data frame <code class="docutils literal notranslate"><span class="pre">crabs</span></code> contains 173 observations on horseshoe crabs (Limulus polyphemus). The response is the count of male crabs (<code class="docutils literal notranslate"><span class="pre">n_males</span></code>) around a female breeding nest. It is subject to four explanatory variables: <code class="docutils literal notranslate"><span class="pre">color</span></code> of the prosoma with four levels (nominal factor-type), the condition of the posterior <code class="docutils literal notranslate"><span class="pre">spine</span></code> with three levels (nominal factor-type), the continuous variables carapace <code class="docutils literal notranslate"><span class="pre">width</span></code> (mm), and <code class="docutils literal notranslate"><span class="pre">weight</span></code> (g).</p>
</div>
<div class="admonition-main-statistical-inquiries admonition">
<p class="admonition-title">Main Statistical Inquiries</p>
<p>Let us suppose we want to assess the following:</p>
<ul class="simple">
<li><p>Whether <code class="docutils literal notranslate"><span class="pre">n_males</span></code> and <code class="docutils literal notranslate"><span class="pre">width</span></code> are statistically associated and by how much.</p></li>
<li><p>Whether <code class="docutils literal notranslate"><span class="pre">n_males</span></code> and  <code class="docutils literal notranslate"><span class="pre">color</span></code> of the prosoma are statistically associated and by how much.</p></li>
</ul>
</div>
</section>
<section id="id3">
<h3>5.2. Exploratory Data Analysis<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h3>
<p>Before getting into any estimation and inference, performing the corresponding exploratory EDA is necessary. Therefore, we will make a scatterplot of <code class="docutils literal notranslate"><span class="pre">n_males</span></code> versus carapace <code class="docutils literal notranslate"><span class="pre">width</span></code> (see below), <strong>even though <code class="docutils literal notranslate"><span class="pre">n_males</span></code> is not continuous</strong>.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Note the characteristic horizontal pattern in the points below since the <span class="math notranslate nohighlight">\(y\)</span>-axis has repeated counts associated with different <code class="docutils literal notranslate"><span class="pre">width</span></code> values</strong>. This plot pattern paves the way for using a count regression model.</p>
</div>
<div class="exercise admonition" id="lecture1-q2">

<p class="admonition-title"><span class="caption-number">Exercise 2 </span></p>
<section id="exercise-content">
<p>By checking the below plot, do you think <code class="docutils literal notranslate"><span class="pre">n_males</span></code> increases with the carapace <code class="docutils literal notranslate"><span class="pre">width</span></code>?</p>
<p><strong>A.</strong> Of course!</p>
<p><strong>B.</strong> Not really.</p>
<p><strong>C.</strong> I am pretty uncertain about an increasing or decreasing pattern.</p>
</section>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.height</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">9</span><span class="p">,</span><span class="w"> </span><span class="n">repr.plot.width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span><span class="p">)</span>

<span class="n">plot_crabs_vs_width</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">crabs</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ggplot</span><span class="p">()</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_point</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">width</span><span class="p">,</span><span class="w"> </span><span class="n">n_males</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">labs</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Number of Male Crabs&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Carapace Width (mm)&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">ggtitle</span><span class="p">(</span><span class="s">&quot;Scatterplot of Number of Male Crabs Versus Carapace Width&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">theme</span><span class="p">(</span>
<span class="w">    </span><span class="n">plot.title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">31</span><span class="p">,</span><span class="w"> </span><span class="n">face</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;bold&quot;</span><span class="p">),</span>
<span class="w">    </span><span class="n">axis.text</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">21</span><span class="p">),</span>
<span class="w">    </span><span class="n">axis.title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">27</span><span class="p">)</span>
<span class="w">  </span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">scale_x_continuous</span><span class="p">(</span><span class="n">limits</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">20</span><span class="p">,</span><span class="w"> </span><span class="m">35</span><span class="p">),</span><span class="w"> </span><span class="n">breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">35</span><span class="p">,</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2.5</span><span class="p">))</span>
<span class="n">plot_crabs_vs_width</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dc8597c22f337b9f03b7373776c11a900967bcdbcb7bf9440277054e4213c94f.png" src="../_images/dc8597c22f337b9f03b7373776c11a900967bcdbcb7bf9440277054e4213c94f.png" />
</div>
</div>
<p>Therefore, since the scatterplot above is too hard to visualize, let us make a clever visualization. Firstly, we could calculate the average <code class="docutils literal notranslate"><span class="pre">n_males</span></code> using a few carapace <code class="docutils literal notranslate"><span class="pre">width</span></code> groups.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">group_avg_width</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">crabs</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">intervals</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">cut</span><span class="p">(</span><span class="n">crabs</span><span class="o">$</span><span class="n">width</span><span class="p">,</span><span class="w"> </span><span class="n">breaks</span><span class="o">=</span><span class="m">10</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">group_by</span><span class="p">(</span><span class="n">intervals</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">summarise</span><span class="p">(</span><span class="n">mean</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span><span class="n">n_males</span><span class="p">),</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">n</span><span class="p">())</span><span class="w"> </span>
<span class="n">group_avg_width</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A tibble: 10 × 3</caption>
<thead>
	<tr><th scope=col>intervals</th><th scope=col>mean</th><th scope=col>n</th></tr>
	<tr><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th></tr>
</thead>
<tbody>
	<tr><td>(21,22.2]  </td><td>0.000000</td><td> 2</td></tr>
	<tr><td>(22.2,23.5]</td><td>1.000000</td><td>14</td></tr>
	<tr><td>(23.5,24.8]</td><td>1.769231</td><td>26</td></tr>
	<tr><td>⋮</td><td>⋮</td><td>⋮</td></tr>
	<tr><td>(29.8,31]  </td><td>4.857143</td><td>7</td></tr>
	<tr><td>(31,32.2]  </td><td>3.000000</td><td>2</td></tr>
	<tr><td>(32.2,33.5]</td><td>7.000000</td><td>1</td></tr>
</tbody>
</table>
</div></div>
</div>
<p>Then, we create another scatterplot using these <code class="docutils literal notranslate"><span class="pre">n_males</span></code> averages by <code class="docutils literal notranslate"><span class="pre">width</span></code> bins (as shown in the code below). Now it is easier to visualize and state, <strong>descriptively</strong>, that there is a positive relationship between carapace <code class="docutils literal notranslate"><span class="pre">width</span></code> and <code class="docutils literal notranslate"><span class="pre">n_males</span></code>.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Nevertheless, we need to find a suitable regression model to statistically confirm this descriptive conclusion!</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">crabs_avg_width_plot</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">group_avg_width</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ggplot</span><span class="p">()</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_point</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">intervals</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="p">),</span><span class="w"> </span><span class="n">colour</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;red&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">4</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">labs</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Mean Number of Male Crabs&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Carapace Width Interval (mm)&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">ggtitle</span><span class="p">(</span><span class="s">&quot;Mean Number of Male Crabs Versus Carapace Width by Interval&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">theme</span><span class="p">(</span>
<span class="w">    </span><span class="n">plot.title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">31</span><span class="p">,</span><span class="w"> </span><span class="n">face</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;bold&quot;</span><span class="p">),</span>
<span class="w">    </span><span class="n">axis.text</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">21</span><span class="p">),</span>
<span class="w">    </span><span class="n">axis.title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">27</span><span class="p">)</span>
<span class="w">  </span><span class="p">)</span>
<span class="n">crabs_avg_width_plot</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/c43e584a60c2953d90a8ba45d1f6cedc03ac966b829e4365a20351a9e15a9b34.png" src="../_images/c43e584a60c2953d90a8ba45d1f6cedc03ac966b829e4365a20351a9e15a9b34.png" />
</div>
</div>
<p>Finally, given that <code class="docutils literal notranslate"><span class="pre">n_males</span></code> is a count-type response, a more appropriate standalone plot is a <strong>bar chart</strong> (as shown in the code below). This bar chart is giving us visual evidence of a possible <strong>Poisson random variable</strong>, note the <strong>right-skewness</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">crabs_avg_width_bar_chart</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">crabs</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ggplot</span><span class="p">()</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_bar</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="nf">as.factor</span><span class="p">(</span><span class="n">n_males</span><span class="p">)),</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;grey&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;black&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">theme</span><span class="p">(</span>
<span class="w">    </span><span class="n">plot.title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">31</span><span class="p">,</span><span class="w"> </span><span class="n">face</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;bold&quot;</span><span class="p">),</span>
<span class="w">    </span><span class="n">axis.text</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">21</span><span class="p">),</span>
<span class="w">    </span><span class="n">axis.title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">27</span><span class="p">)</span>
<span class="w">  </span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">ggtitle</span><span class="p">(</span><span class="s">&quot;Bar Chart of Counts by Numbers of Male Crabs&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">labs</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Number of Male Crabs&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Count&quot;</span><span class="p">)</span>
<span class="n">crabs_avg_width_bar_chart</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/0d5c43a85f382ebbf55506c79793c81776da3eee64380cd805b51edbc10618e0.png" src="../_images/0d5c43a85f382ebbf55506c79793c81776da3eee64380cd805b51edbc10618e0.png" />
</div>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The distribution of the counts in the response <code class="docutils literal notranslate"><span class="pre">n_males</span></code> is suggesting a possible Poisson distribution. Hence we might use Poisson regression to assess whether carapace <code class="docutils literal notranslate"><span class="pre">width</span></code> is related to <code class="docutils literal notranslate"><span class="pre">n_males</span></code> and quantify the magnitude of the regressor’s association (as well as the respective uncertainty associated with its estimation).</p>
</div>
</section>
<section id="id4">
<h3>5.3. Data Modelling Framework<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h3>
<p>Besides OLS and Binary Logistic regressions, another alternative is count data modelling, as in <strong>Poisson regression</strong>. Unlike Binary Logistic regression, <strong>we use counts as a response variable</strong>. Hence, we have to modify the modelling framework to consider this fact. Poisson regression would be the primary resource when it comes to modelling counts. Note this model also fits into the GLM class.</p>
<p>Recall that the random component in the OLS regression model, namely <span class="math notranslate nohighlight">\(\varepsilon_i\)</span>, is assumed to be Normal, making the response <span class="math notranslate nohighlight">\(Y_i\)</span> Normal as in <a class="reference internal" href="#equation-y-ols-model-normal">(3)</a>.</p>
<p>Therefore:</p>
<blockquote>
<div><p>What is the distributional key difference between the Poisson and the OLS regression models in terms of the response?</p>
</div></blockquote>
<p>First of all, we have to specify what a Poisson random variable is. Recall <a class="reference external" href="https://ubc-mds.github.io/DSCI_551_stat-prob-dsci/lectures/parametric-families.html#distribution-families"><strong>DSCI 551</strong></a>, a Poisson random variable refers to discrete data with non-negative integer values that count something. <strong>These counts could happen during a given timeframe or even a space such as a geographic unit!</strong></p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>A particularity of a Poisson random variable is that its mean equals its variance (an inconsistency in <a class="reference external" href="https://www.merriam-webster.com/dictionary/dimensional%20analysis"><strong>Dimensional Analysis</strong></a>!). Thus, any factor that affects the mean will also affect the variance. This fact could be a potential drawback for using a Poisson regression model. Nonetheless, an alternative count modelling option could overcome this potential issue, which we will explain further.</p>
</div>
<p>The Poisson regression model assumes a random sample of <span class="math notranslate nohighlight">\(n\)</span> count observations <span class="math notranslate nohighlight">\(Y_i\)</span>s, hence <strong>independent</strong> (<strong>but not identically distributed!</strong>), which have the following distribution:</p>
<div class="math notranslate nohighlight">
\[Y_i \sim \text{Poisson}(\lambda_i).\]</div>
<p>Each <span class="math notranslate nohighlight">\(i\)</span>th observation has its own <span class="math notranslate nohighlight">\(\mathbb{E}(Y_i) = \lambda_i &gt; 0\)</span>, which also implicates <span class="math notranslate nohighlight">\(\text{Var}(Y_i) = \lambda_i &gt; 0\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Definition of Equidispersion</p>
<p>The equality of the expected value and variance in a random variable is called <strong>equidispersion</strong>.</p>
</div>
<p>Parameter <span class="math notranslate nohighlight">\(\lambda_i\)</span> is the risk of an event occurrence, coming from the definition of the Poisson random variable, <strong>in a given timeframe or even a space</strong>.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>We have to highlight another particularity in the Poisson distribution: <strong><span class="math notranslate nohighlight">\(\lambda_i\)</span> is a continuous distributional parameter!</strong></p>
</div>
<p>For our <code class="docutils literal notranslate"><span class="pre">crabs</span></code> dataset, the events are the number of male crabs (<code class="docutils literal notranslate"><span class="pre">n_males</span></code>) around a space: <strong>the female breeding nest</strong>. Since we want to make inference on whether the carapace <code class="docutils literal notranslate"><span class="pre">width</span></code> is related to the response <code class="docutils literal notranslate"><span class="pre">n_males</span></code>, then we could use Poisson regression.</p>
<p>Since the Poisson Regression model is also a GLM, we need to set up a <a class="reference internal" href="#link-function"><span class="std std-ref"><strong>link function</strong></span></a> <span class="math notranslate nohighlight">\(h(\lambda_i)\)</span> for the mean.</p>
<p>Let <span class="math notranslate nohighlight">\(X_{\texttt{width}_i}\)</span> be the <span class="math notranslate nohighlight">\(i\)</span>th value for the regressor <code class="docutils literal notranslate"><span class="pre">width</span></code> in our dataset. An easy modelling solution would be an <strong>identity</strong> link function as in</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-poisson">
<span class="eqno">(6)<a class="headerlink" href="#equation-eq-naive-poisson" title="Permalink to this equation">#</a></span>\[
h(\lambda_i) = \lambda_i = \beta_0 + \beta_1 X_{\texttt{width}_i}.
\]</div>
<p>However, again, <strong>we have a response range issue!</strong></p>
<p>The model <a class="reference internal" href="#equation-eq-naive-poisson">(6)</a> for <span class="math notranslate nohighlight">\(h(\lambda_i)\)</span> has a significant drawback: <strong>the right-hand side is allowed to take on even negative values</strong>, which does not align with the nature of the parameter <span class="math notranslate nohighlight">\(\lambda_i\)</span> (<strong>that always has to be non-negative</strong>).</p>
<p>Recall the essential characteristic of a GLM that should come into play for a link function. In this class of models, the direct relationship between the original response and the regressors may be <strong>non-linear</strong> as in <span class="math notranslate nohighlight">\(h(\lambda_i)\)</span>. Hence, instead of using the identity link function <span class="math notranslate nohighlight">\(\lambda_i\)</span>, <strong>we will use the natural logarithm of the mean: <span class="math notranslate nohighlight">\(\log(\lambda_i)\)</span></strong>.</p>
<p>Before continuing with the <code class="docutils literal notranslate"><span class="pre">crabs</span></code> dataset, let us generalize the Poisson regression model with <span class="math notranslate nohighlight">\(k\)</span> regressors as:</p>
<div class="math notranslate nohighlight" id="equation-poisson-model">
<span class="eqno">(7)<a class="headerlink" href="#equation-poisson-model" title="Permalink to this equation">#</a></span>\[\begin{equation*}
h(\lambda_i) = \log(\lambda_i) = \beta_0 + \beta_1 X_{i,1} + \dots + \beta_k X_{i,k}.
\end{equation*}\]</div>
<p>In model <a class="reference internal" href="#equation-poisson-model">(7)</a>, each one of the <span class="math notranslate nohighlight">\(k\)</span> regression coefficients <span class="math notranslate nohighlight">\(\beta_{1}, \dots, \beta_{k}\)</span> represents <strong>the expected change in the natural logarithm of the mean <span class="math notranslate nohighlight">\(\lambda_i\)</span> per unit change in their respective regressors <span class="math notranslate nohighlight">\(X_{i,1}, \dots, X_{i,k}\)</span></strong>. Nonetheless, we could make more sense in the interpretation by exponentiating <a class="reference internal" href="#equation-poisson-model">(7)</a>:</p>
<div class="math notranslate nohighlight">
\[
\lambda_i = \exp{(\beta_0 + \beta_1 X_{i,1} + \dots + \beta_k X_{i,k})},
\]</div>
<p>where an increase in one unit in any of the <span class="math notranslate nohighlight">\(k\)</span> regressors (<strong>while keeping the rest of them constant</strong>) <strong>multiplies the mean <span class="math notranslate nohighlight">\(\lambda_i\)</span> by a factor <span class="math notranslate nohighlight">\(\exp{(\beta_j)}\)</span>, for all <span class="math notranslate nohighlight">\(j = 1, \dots, k\)</span></strong>.</p>
<p>In the <code class="docutils literal notranslate"><span class="pre">crabs</span></code> dataset with <code class="docutils literal notranslate"><span class="pre">width</span></code> as a regressor, the Poisson regression model is depicted as:</p>
<div class="math notranslate nohighlight">
\[
h(\lambda_i) = \log(\lambda_i) = \beta_0 + \beta_1 X_{\texttt{width}_i}.
\]</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>As a side note, we have to clarify that the <strong>systematic component</strong> in the Poisson regression model is explicitly depicted by the regressors and their coefficients as in multiple linear regression. The <strong>random component</strong> is implicitly contained in each random variable</p>
<div class="math notranslate nohighlight">
\[Y_i \sim \text{Poisson}(\lambda_i).\]</div>
</div>
</section>
<section id="id5">
<h3>5.4. Estimation<a class="headerlink" href="#id5" title="Permalink to this heading">#</a></h3>
<p>Under a general framework with <span class="math notranslate nohighlight">\(k\)</span> regressors, the <strong>regression parameters</strong> <span class="math notranslate nohighlight">\(\beta_0, \beta_1, \dots, \beta_k\)</span> in the model are also unknown. In order to estimate them, we will use function <code class="docutils literal notranslate"><span class="pre">glm()</span></code> and its argument <code class="docutils literal notranslate"><span class="pre">family</span> <span class="pre">=</span> <span class="pre">poisson</span></code> (required to specify the Poisson nature of the response), which obtains the estimates <span class="math notranslate nohighlight">\(\hat{\beta}_0, \hat{\beta}_1, \dots \hat{\beta}_k\)</span>.</p>
<p>The estimates are obtained through <strong>maximum likelihood</strong> where we assume a <strong>Poisson joint probability mass function</strong> of the <span class="math notranslate nohighlight">\(n\)</span> responses <span class="math notranslate nohighlight">\(Y_i\)</span>.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>For the sake of coding clarity, you could also use <code class="docutils literal notranslate"><span class="pre">family</span> <span class="pre">=</span> <span class="pre">poisson(link</span> <span class="pre">=</span> <span class="pre">&quot;log&quot;)</span></code>. Nevertheless, <code class="docutils literal notranslate"><span class="pre">link</span> <span class="pre">=</span> <span class="pre">&quot;log&quot;</span></code> is a default in <code class="docutils literal notranslate"><span class="pre">glm()</span></code> for Poisson regression. Thus, <code class="docutils literal notranslate"><span class="pre">family</span> <span class="pre">=</span> <span class="pre">poisson</span></code> suffices when using the logarithmic link function.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">poisson_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">glm</span><span class="p">(</span><span class="n">n_males</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">width</span><span class="p">,</span><span class="w"> </span><span class="n">family</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">poisson</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">crabs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id6">
<h3>5.5. Inference<a class="headerlink" href="#id6" title="Permalink to this heading">#</a></h3>
<p><strong>The fitted regression model will be used to identify the relationship between the logarithm of the response’s mean and regressors.</strong> To determine the <strong>statistical significance</strong> of <span class="math notranslate nohighlight">\(\beta_j\)</span> in this model, we also use the <strong>Wald statistic</strong>:</p>
<div class="math notranslate nohighlight">
\[
z_j = \frac{\hat{\beta}_j}{\mbox{se} \left( \hat{\beta}_j \right)}
\]</div>
<p>to test the hypotheses</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{gather*}
H_0: \beta_j = 0 \\
H_a: \beta_j \neq 0;
\end{gather*}\end{split}\]</div>
<p>where the <strong>null hypothesis</strong> <span class="math notranslate nohighlight">\(H_0\)</span> indicates that the <span class="math notranslate nohighlight">\(j\)</span>th regressor corresponding to <span class="math notranslate nohighlight">\(\beta_j\)</span> does not have any association on the response variable in the model, and the <strong>alternative hypothesis</strong> <span class="math notranslate nohighlight">\(H_a\)</span> otherwise. Provided the sample size <span class="math notranslate nohighlight">\(n\)</span> is large enough, <span class="math notranslate nohighlight">\(z_j\)</span> has an <strong>approximately Standard Normal distribution</strong> under <span class="math notranslate nohighlight">\(H_0\)</span>.</p>
<p><code class="docutils literal notranslate"><span class="pre">R</span></code> provides the corresponding <strong><span class="math notranslate nohighlight">\(p\)</span>-values</strong> for each <span class="math notranslate nohighlight">\(\beta_j\)</span>. The smaller the <span class="math notranslate nohighlight">\(p\)</span>-value, the stronger the evidence against the null hypothesis <span class="math notranslate nohighlight">\(H_0\)</span>. As in the previous regression models, we would set a predetermined significance level <span class="math notranslate nohighlight">\(\alpha\)</span> to infer if the <span class="math notranslate nohighlight">\(p\)</span>-value is small enough. If the <span class="math notranslate nohighlight">\(p\)</span>-value is smaller than the predetermined level <span class="math notranslate nohighlight">\(\alpha\)</span>, then we could claim that there is evidence to reject the null hypothesis. Hence, <span class="math notranslate nohighlight">\(p\)</span>-values that are small enough indicate that the data provides evidence in favour of <strong>association</strong> (<strong>or causation in the case of an experimental study!</strong>) between the response variable and the <span class="math notranslate nohighlight">\(j\)</span>th regressor.</p>
<p>Furthermore, given a specified level of confidence where <span class="math notranslate nohighlight">\(\alpha\)</span> is the significance level, we can construct approximate <span class="math notranslate nohighlight">\((1 - \alpha) \times 100\%\)</span> <strong>confidence intervals</strong> for the corresponding true value of <span class="math notranslate nohighlight">\(\beta_j\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_j \pm z_{\alpha/2}\mbox{se} \left( \hat{\beta}_j \right),
\]</div>
<p>where <span class="math notranslate nohighlight">\(z_{\alpha/2}\)</span> is the upper <span class="math notranslate nohighlight">\(\alpha/2\)</span> quantile of the <strong>Standard Normal distribution</strong>.</p>
<p>Now, we can answer the following:</p>
<blockquote>
<div><p>Is caparace width statistically associated to the logarithm of mean of the counts of satellite male crabs residing around a female crab nest?</p>
</div></blockquote>
<p>We can also use the function <code class="docutils literal notranslate"><span class="pre">tidy()</span></code> from the <code class="docutils literal notranslate"><span class="pre">broom</span></code> package along with argument <code class="docutils literal notranslate"><span class="pre">conf.int</span> <span class="pre">=</span> <span class="pre">TRUE</span></code> to get the 95% confidence intervals <strong>by default</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">tidy</span><span class="p">(</span><span class="n">poisson_model</span><span class="p">,</span><span class="w"> </span><span class="n">conf.int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">mutate_if</span><span class="p">(</span><span class="n">is.numeric</span><span class="p">,</span><span class="w"> </span><span class="n">round</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A tibble: 2 × 7</caption>
<thead>
	<tr><th scope=col>term</th><th scope=col>estimate</th><th scope=col>std.error</th><th scope=col>statistic</th><th scope=col>p.value</th><th scope=col>conf.low</th><th scope=col>conf.high</th></tr>
	<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>(Intercept)</td><td>-3.305</td><td>0.542</td><td>-6.095</td><td>0</td><td>-4.366</td><td>-2.241</td></tr>
	<tr><td>width      </td><td> 0.164</td><td>0.020</td><td> 8.216</td><td>0</td><td> 0.125</td><td> 0.203</td></tr>
</tbody>
</table>
</div></div>
</div>
<p>Our sample gives us evidence to reject <span class="math notranslate nohighlight">\(H_0\)</span> (<span class="math notranslate nohighlight">\(p\text{-value} &lt; .001\)</span>). So carapace <code class="docutils literal notranslate"><span class="pre">width</span></code> is statistically associated to the logarithm of the mean of <code class="docutils literal notranslate"><span class="pre">n_males</span></code>.</p>
<p>Now, it is time to plot the fitted values coming from <code class="docutils literal notranslate"><span class="pre">poisson_model</span></code> to check whether it provides a positive relationship between carapace <code class="docutils literal notranslate"><span class="pre">width</span></code> and the <strong>original scale</strong> of the response <code class="docutils literal notranslate"><span class="pre">n_males</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">plot_crabs_vs_width</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">plot_crabs_vs_width</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_smooth</span><span class="p">(</span>
<span class="w">    </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">crabs</span><span class="p">,</span><span class="w"> </span><span class="nf">aes</span><span class="p">(</span><span class="n">width</span><span class="p">,</span><span class="w"> </span><span class="n">n_males</span><span class="p">),</span>
<span class="w">    </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;glm&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">formula</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x</span><span class="p">,</span>
<span class="w">    </span><span class="n">method.args</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">family</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">poisson</span><span class="p">),</span><span class="w"> </span><span class="n">se</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span>
<span class="w">  </span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">ggtitle</span><span class="p">(</span><span class="s">&quot;Poisson Regression&quot;</span><span class="p">)</span>
<span class="n">plot_crabs_vs_width</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/d093c41c1da6127fa616d22d8d4b4bc84f592e39d3e29856f01730740c83c129.png" src="../_images/d093c41c1da6127fa616d22d8d4b4bc84f592e39d3e29856f01730740c83c129.png" />
</div>
</div>
<p>The blue line in the plot above is the fitted Poisson regression of <code class="docutils literal notranslate"><span class="pre">n_males</span></code> versus carapace <code class="docutils literal notranslate"><span class="pre">width</span></code>. <strong>The positive relationship is now clear with this regression line.</strong></p>
</section>
<section id="coefficient-interpretation">
<h3>5.6. Coefficient Interpretation<a class="headerlink" href="#coefficient-interpretation" title="Permalink to this heading">#</a></h3>
<p>Let us fit a second model with two regressors: <code class="docutils literal notranslate"><span class="pre">width</span></code> (<span class="math notranslate nohighlight">\(X_{\texttt{width}_i}\)</span>) and <code class="docutils literal notranslate"><span class="pre">color</span></code> (<span class="math notranslate nohighlight">\(X_{\texttt{color_darker}_i}\)</span>, <span class="math notranslate nohighlight">\(X_{\texttt{color_light}_i}\)</span>, and <span class="math notranslate nohighlight">\(X_{\texttt{color_medium}_i}\)</span>) for the <span class="math notranslate nohighlight">\(i\)</span>th observation:</p>
<div class="math notranslate nohighlight">
\[
h(\lambda_i) = \log (\lambda_i) = \beta_0 + \beta_1 X_{\texttt{width}_i} + \beta_2 X_{\texttt{color_darker}_i} + \beta_3 X_{\texttt{color_light}_i} + \beta_4 X_{\texttt{color_medium}_i}.
\]</div>
<p>The explanatory variable <code class="docutils literal notranslate"><span class="pre">color</span></code> is of <strong>factor-type</strong> (<strong>discrete</strong>) and <strong>nominal</strong> (its levels do not follow any specific order). Moreover, it has a baseline: <code class="docutils literal notranslate"><span class="pre">dark</span></code>. <strong>We can check the baseline level, via <code class="docutils literal notranslate"><span class="pre">levels()</span></code>, which is on the left-hand side.</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">levels</span><span class="p">(</span><span class="n">crabs</span><span class="o">$</span><span class="n">color</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>
.list-inline {list-style: none; margin:0; padding: 0}
.list-inline>li {display: inline-block}
.list-inline>li:not(:last-child)::after {content: "\00b7"; padding: 0 .5ex}
</style>
<ol class=list-inline><li>'dark'</li><li>'darker'</li><li>'light'</li><li>'medium'</li></ol>
</div></div>
</div>
<p>Using explanatory variables such as <code class="docutils literal notranslate"><span class="pre">color</span></code> involves using <strong>dummy variables</strong> shown in <a class="reference internal" href="#dummy-var"><span class="std std-numref">Table 1</span></a>, such as in Binary Logistic regression. For example, the explanatory variable <code class="docutils literal notranslate"><span class="pre">color</span></code> has four levels; thus, this Poisson regression model will incorporate three dummy variables: <span class="math notranslate nohighlight">\(X_{\texttt{color_darker}_i}\)</span>, <span class="math notranslate nohighlight">\(X_{\texttt{color_light}_i}\)</span>, and <span class="math notranslate nohighlight">\(X_{\texttt{color_medium}_i}\)</span>. Depending on the <code class="docutils literal notranslate"><span class="pre">color</span></code>, these dummy variables take on the following values:</p>
<ul class="simple">
<li><p>When <code class="docutils literal notranslate"><span class="pre">color</span></code> is <code class="docutils literal notranslate"><span class="pre">darker</span></code>, then <span class="math notranslate nohighlight">\(X_{\texttt{color_darker}_i} = 1\)</span> while the other two dummy variables <span class="math notranslate nohighlight">\(X_{\texttt{color_light}_i} = X_{\texttt{color_medium}_i} = 0\)</span>.</p></li>
<li><p>When <code class="docutils literal notranslate"><span class="pre">color</span></code> is <code class="docutils literal notranslate"><span class="pre">light</span></code>, then <span class="math notranslate nohighlight">\(X_{\texttt{color_light}_i} = 1\)</span> while the other two dummy variables <span class="math notranslate nohighlight">\(X_{\texttt{color_darker}_i} = X_{\texttt{color_medium}_i} = 0\)</span>.</p></li>
<li><p>When <code class="docutils literal notranslate"><span class="pre">color</span></code> is <code class="docutils literal notranslate"><span class="pre">medium</span></code>, then <span class="math notranslate nohighlight">\(X_{\texttt{color_medium}_i} = 1\)</span> while the other two dummy variables <span class="math notranslate nohighlight">\(X_{\texttt{color_darker}_i} = X_{\texttt{color_light}_i} = 0\)</span>.</p></li>
</ul>
<p>Note that the level <code class="docutils literal notranslate"><span class="pre">dark</span></code> is depicted as the baseline here. Hence, the interpretation of the coefficients in the model for each dummy variable will be to this baseline.</p>
<p>Now, let us fit this second Poisson regression model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">poisson_model_2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">glm</span><span class="p">(</span><span class="n">n_males</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">width</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">color</span><span class="p">,</span><span class="w"> </span><span class="n">family</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">poisson</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">crabs</span><span class="p">)</span>
<span class="nf">tidy</span><span class="p">(</span><span class="n">poisson_model_2</span><span class="p">,</span><span class="w"> </span><span class="n">conf.int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">mutate_if</span><span class="p">(</span><span class="n">is.numeric</span><span class="p">,</span><span class="w"> </span><span class="n">round</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A tibble: 5 × 7</caption>
<thead>
	<tr><th scope=col>term</th><th scope=col>estimate</th><th scope=col>std.error</th><th scope=col>statistic</th><th scope=col>p.value</th><th scope=col>conf.low</th><th scope=col>conf.high</th></tr>
	<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>(Intercept)</td><td>-3.086</td><td>0.557</td><td>-5.536</td><td>0.000</td><td>-4.178</td><td>-1.993</td></tr>
	<tr><td>width      </td><td> 0.149</td><td>0.021</td><td> 7.166</td><td>0.000</td><td> 0.108</td><td> 0.190</td></tr>
	<tr><td>colordarker</td><td>-0.011</td><td>0.180</td><td>-0.061</td><td>0.951</td><td>-0.373</td><td> 0.336</td></tr>
	<tr><td>colorlight </td><td> 0.436</td><td>0.176</td><td> 2.474</td><td>0.013</td><td> 0.083</td><td> 0.776</td></tr>
	<tr><td>colormedium</td><td> 0.237</td><td>0.118</td><td> 2.003</td><td>0.045</td><td> 0.009</td><td> 0.473</td></tr>
</tbody>
</table>
</div></div>
</div>
<p>We can see that <code class="docutils literal notranslate"><span class="pre">width</span></code>, <code class="docutils literal notranslate"><span class="pre">colorlight</span></code>, and <code class="docutils literal notranslate"><span class="pre">colormedium</span></code> are significant according to the <code class="docutils literal notranslate"><span class="pre">p.value</span></code> column (with <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span>).</p>
<p>First, let us focus on the coefficient corresponding to carapace <code class="docutils literal notranslate"><span class="pre">width</span></code>, <strong>while keeping <code class="docutils literal notranslate"><span class="pre">color</span></code> constant</strong>. Consider an observation with a given value <span class="math notranslate nohighlight">\(X_{\texttt{width}} = \texttt{w}\)</span> mm, and another observation with a given <span class="math notranslate nohighlight">\(X_{\texttt{width + 1}} = \texttt{w} + 1\)</span> mm (i.e., an increase of <span class="math notranslate nohighlight">\(1\)</span> mm). Then we have their corresponding regression equations:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{gather*}
\log \left( \lambda_{\texttt{width}} \right) = \beta_0 + \beta_1 \overbrace{\texttt{w}}^{X_{\texttt{width}}} + \overbrace{\beta_2 X_{\texttt{color_darker}} + \beta_3 X_{\texttt{color_light}} + \beta_4 X_{\texttt{color_medium}}}^{\text{Constant}} \\
\log \left( \lambda_{\texttt{width + 1}} \right) = \beta_0 + \beta_1 \underbrace{(\texttt{w} + 1)}_{X_{\texttt{width + 1}}} + \underbrace{\beta_2 X_{\texttt{color_darker}} + \beta_3 X_{\texttt{color_light}} + \beta_4 X_{\texttt{color_medium}}.}_{\text{Constant}}
\end{gather*}\end{split}\]</div>
<p>We take the difference between both equations as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
\log \left( \lambda_{\texttt{width + 1}} \right) - \log \left( \lambda_{\texttt{width1}} \right) &amp;= \beta_1 (\texttt{w} + 1) - \beta_1 \texttt{w} \\
&amp;= \beta_1.
\end{align*}\end{split}\]</div>
<p>We apply the logarithm property for a ratio:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
\log \left( \frac{\lambda_{\texttt{width + 1}} }{\lambda_{\texttt{width}}} \right) &amp;= \log \left( \lambda_{\texttt{width + 1}} \right) - \log \left( \lambda_{\texttt{width}} \right) \\
&amp;= \beta_1.
\end{align*}\end{split}\]</div>
<p>Finally, we have to exponentiate the previous equation:</p>
<div class="math notranslate nohighlight">
\[
\frac{\lambda_{\texttt{width + 1}} }{\lambda_{\texttt{width}}} = e^{\beta_1}.
\]</div>
<p>The expression <span class="math notranslate nohighlight">\(\frac{\lambda_{\texttt{width + 1}} }{\lambda_{\texttt{width}}} = e^{\beta_1}\)</span> indicates that the response varies in a <strong>multiplicative way</strong> when increased 1 mm in carapace <code class="docutils literal notranslate"><span class="pre">width</span></code>.</p>
<p>Therefore, by using the estimate <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> (note the hat notation) coming from the model <code class="docutils literal notranslate"><span class="pre">poisson_model_2</span></code>, we calculate this multiplicative effect as follows (via <code class="docutils literal notranslate"><span class="pre">exponentiate</span> <span class="pre">=</span> <span class="pre">TRUE</span></code> in <code class="docutils literal notranslate"><span class="pre">tidy()</span></code>):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">tidy</span><span class="p">(</span><span class="n">poisson_model_2</span><span class="p">,</span><span class="w"> </span><span class="n">exponentiate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">conf.int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">mutate_if</span><span class="p">(</span><span class="n">is.numeric</span><span class="p">,</span><span class="w"> </span><span class="n">round</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A tibble: 5 × 7</caption>
<thead>
	<tr><th scope=col>term</th><th scope=col>estimate</th><th scope=col>std.error</th><th scope=col>statistic</th><th scope=col>p.value</th><th scope=col>conf.low</th><th scope=col>conf.high</th></tr>
	<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>(Intercept)</td><td>0.05</td><td>0.56</td><td>-5.54</td><td>0.00</td><td>0.02</td><td>0.14</td></tr>
	<tr><td>width      </td><td>1.16</td><td>0.02</td><td> 7.17</td><td>0.00</td><td>1.11</td><td>1.21</td></tr>
	<tr><td>colordarker</td><td>0.99</td><td>0.18</td><td>-0.06</td><td>0.95</td><td>0.69</td><td>1.40</td></tr>
	<tr><td>colorlight </td><td>1.55</td><td>0.18</td><td> 2.47</td><td>0.01</td><td>1.09</td><td>2.17</td></tr>
	<tr><td>colormedium</td><td>1.27</td><td>0.12</td><td> 2.00</td><td>0.05</td><td>1.01</td><td>1.60</td></tr>
</tbody>
</table>
</div></div>
</div>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(\frac{\hat{\lambda}_{\texttt{width + 1}} }{\hat{\lambda}_{\texttt{width}}} = e^{\hat{\beta}_1} = 1.16\)</span> indicates that <strong>the mean count of male crabs (<code class="docutils literal notranslate"><span class="pre">n_males</span></code>) around a female breeding nest</strong> increases by <span class="math notranslate nohighlight">\(16\%\)</span> when increasing the carapace <code class="docutils literal notranslate"><span class="pre">width</span></code> by <span class="math notranslate nohighlight">\(1\)</span> mm, <strong>while keeping <code class="docutils literal notranslate"><span class="pre">color</span></code> constant</strong>.</p>
</div></blockquote>
<p>The interpretation of the significant coefficients corresponding to <code class="docutils literal notranslate"><span class="pre">color</span></code> (<code class="docutils literal notranslate"><span class="pre">colorlight</span></code> and <code class="docutils literal notranslate"><span class="pre">colormedium</span></code> with <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span>) is associated to the baseline level <code class="docutils literal notranslate"><span class="pre">dark</span></code>.</p>
<p>Consider two observations, one with <code class="docutils literal notranslate"><span class="pre">dark</span></code> <code class="docutils literal notranslate"><span class="pre">color</span></code> of the prosoma (the baseline) and another with <code class="docutils literal notranslate"><span class="pre">light</span></code> <code class="docutils literal notranslate"><span class="pre">color</span></code>. Their corresponding responses are denoted as <span class="math notranslate nohighlight">\(\lambda_{\texttt{D}}\)</span> (for <code class="docutils literal notranslate"><span class="pre">dark</span></code>) and <span class="math notranslate nohighlight">\(\lambda_{\texttt{L}}\)</span> (for <code class="docutils literal notranslate"><span class="pre">light</span></code>). While holding <span class="math notranslate nohighlight">\(X_{\texttt{width}}\)</span> constant, their regression equations are:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{gather*}
\log \left( \lambda_{\texttt{D}} \right) = \beta_0 + \overbrace{\beta_1 X_{\texttt{width}}}^{\text{Constant}} + \beta_2 X_{\texttt{color_darker}_{\texttt{D}}} + \beta_3 X_{\texttt{color_light}_{\texttt{D}}} + \beta_4 X_{\texttt{color_medium}_{\texttt{D}}} \\
\log \left( \lambda_{\texttt{L}} \right) = \beta_0 + \underbrace{\beta_1 X_{\texttt{width}}}_{\text{Constant}} + \beta_2 X_{\texttt{color_darker}_{\texttt{L}}} + \beta_3 X_{\texttt{color_light}_{\texttt{L}}} + \beta_4 X_{\texttt{color_medium}_{\texttt{L}}}
\end{gather*}\end{split}\]</div>
<p>The corresponding <code class="docutils literal notranslate"><span class="pre">color</span></code> indicator variables for both <span class="math notranslate nohighlight">\(\lambda_{\texttt{D}}\)</span> and <span class="math notranslate nohighlight">\(\lambda_{\texttt{L}}\)</span> take on these values:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
\log \left( \lambda_{\texttt{D}} \right) &amp;= \beta_0 + \overbrace{\beta_1 X_{\texttt{width}}}^{\text{Constant}} + \beta_2 X_{\texttt{color_darker}_{\texttt{D}}} + \beta_3 X_{\texttt{color_light}_{\texttt{D}}} + \beta_4 X_{\texttt{color_medium}_{\texttt{D}}} \\
&amp;= \beta_0 + \beta_1 X_{\texttt{width}}+ \beta_2 \times 0 + \beta_3 \times 0 + \beta_4 \times 0 \\
&amp;= \beta_0 + \beta_1 X_{\texttt{width}}
\end{align*}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
\log \left( \lambda_{\texttt{L}} \right) &amp;= \beta_0 + \beta_1 X_{\texttt{width}} + \beta_2 X_{\texttt{color_darker}_{\texttt{L}}} + \beta_3 X_{\texttt{color_light}_{\texttt{L}}} + \beta_4 X_{\texttt{color_medium}_{\texttt{L}}} \\
&amp;= \beta_0 + \beta_1 X_{\texttt{width}}+ \beta_2 \times 0 + \beta_3 \times 1 + \beta_4 \times 0 \\
&amp;= \beta_0 + \underbrace{\beta_1 X_{\texttt{width}}}_{\text{Constant}} + \beta_3.
\end{align*}\end{split}\]</div>
<p>Therefore, <strong>what is the association of the level <code class="docutils literal notranslate"><span class="pre">light</span></code> with respect to <code class="docutils literal notranslate"><span class="pre">dark</span></code>?</strong> Let us take the differences again:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
\log \left( \frac{\lambda_{\texttt{L}} }{\lambda_{\texttt{D}}} \right) &amp;= \log \left( \lambda_{\texttt{L}} \right) - \log \left( \lambda_{\texttt{D}} \right) \\
&amp;= \beta_3.
\end{align*}\end{split}\]</div>
<p>Then, we exponentiate the previous equation:</p>
<div class="math notranslate nohighlight">
\[
\frac{\lambda_{\texttt{L}} }{\lambda_{\texttt{D}}} = e^{\beta_3}.
\]</div>
<p>The expression <span class="math notranslate nohighlight">\(\frac{\lambda_{\texttt{L}} }{\lambda_{\texttt{D}}} = e^{\beta_3}\)</span> indicates that the response varies in a <strong>multiplicative way</strong> when the <code class="docutils literal notranslate"><span class="pre">color</span></code> of the prosoma changes from <code class="docutils literal notranslate"><span class="pre">dark</span></code> to <code class="docutils literal notranslate"><span class="pre">light</span></code>.</p>
<p>Therefore, by using the estimate <span class="math notranslate nohighlight">\(\hat{\beta}_3\)</span> (note the hat notation) coming from the model <code class="docutils literal notranslate"><span class="pre">poisson_model_2</span></code>, we calculate this multiplicative effect as follows (via <code class="docutils literal notranslate"><span class="pre">exponentiate</span> <span class="pre">=</span> <span class="pre">TRUE</span></code> in <code class="docutils literal notranslate"><span class="pre">tidy()</span></code>)::</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">tidy</span><span class="p">(</span><span class="n">poisson_model_2</span><span class="p">,</span><span class="w"> </span><span class="n">exponentiate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">conf.int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">mutate_if</span><span class="p">(</span><span class="n">is.numeric</span><span class="p">,</span><span class="w"> </span><span class="n">round</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A tibble: 5 × 7</caption>
<thead>
	<tr><th scope=col>term</th><th scope=col>estimate</th><th scope=col>std.error</th><th scope=col>statistic</th><th scope=col>p.value</th><th scope=col>conf.low</th><th scope=col>conf.high</th></tr>
	<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>(Intercept)</td><td>0.05</td><td>0.56</td><td>-5.54</td><td>0.00</td><td>0.02</td><td>0.14</td></tr>
	<tr><td>width      </td><td>1.16</td><td>0.02</td><td> 7.17</td><td>0.00</td><td>1.11</td><td>1.21</td></tr>
	<tr><td>colordarker</td><td>0.99</td><td>0.18</td><td>-0.06</td><td>0.95</td><td>0.69</td><td>1.40</td></tr>
	<tr><td>colorlight </td><td>1.55</td><td>0.18</td><td> 2.47</td><td>0.01</td><td>1.09</td><td>2.17</td></tr>
	<tr><td>colormedium</td><td>1.27</td><td>0.12</td><td> 2.00</td><td>0.05</td><td>1.01</td><td>1.60</td></tr>
</tbody>
</table>
</div></div>
</div>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(\frac{\hat{\lambda}_{\texttt{L}} }{\hat{\lambda}_{\texttt{D}}} = e^{\hat{\beta}_3} = 1.55\)</span> indicates that <strong>the mean count of male crabs (<code class="docutils literal notranslate"><span class="pre">n_males</span></code>) around a female breeding nest</strong> increases by <span class="math notranslate nohighlight">\(55\%\)</span> when the <code class="docutils literal notranslate"><span class="pre">color</span></code> of the prosoma changes from <code class="docutils literal notranslate"><span class="pre">dark</span></code> to <code class="docutils literal notranslate"><span class="pre">light</span></code>, <strong>while keeping the carapace <code class="docutils literal notranslate"><span class="pre">width</span></code> constant</strong>.</p>
</div></blockquote>
</section>
<section id="predictions">
<h3>5.7. Predictions<a class="headerlink" href="#predictions" title="Permalink to this heading">#</a></h3>
<p>Suppose we want to predict <strong>the mean count of male crabs (<code class="docutils literal notranslate"><span class="pre">n_males</span></code>) around a female breeding nest</strong> with a carapace <code class="docutils literal notranslate"><span class="pre">width</span></code> of <span class="math notranslate nohighlight">\(27.5\)</span> mm and <code class="docutils literal notranslate"><span class="pre">light</span></code> <code class="docutils literal notranslate"><span class="pre">color</span></code> of the prosoma. We could use the model <code class="docutils literal notranslate"><span class="pre">poisson_model_2</span></code> for making such prediction as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">round</span><span class="p">(</span><span class="nf">predict</span><span class="p">(</span><span class="n">poisson_model_2</span><span class="p">,</span><span class="w"> </span><span class="n">newdata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">tibble</span><span class="p">(</span><span class="n">width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">27.5</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;light&quot;</span><span class="p">),</span>
<span class="w">  </span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;response&quot;</span>
<span class="p">),</span><span class="w"> </span><span class="m">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><strong>1:</strong> 4.29</div></div>
</div>
<p>Note we have to use <code class="docutils literal notranslate"><span class="pre">type</span> <span class="pre">=</span> <span class="pre">&quot;response&quot;</span></code> in the function <code class="docutils literal notranslate"><span class="pre">predict()</span></code> to obtain the prediction <strong>on its original scale</strong>.</p>
</section>
</section>
<section id="overdispersion">
<h2>6. Overdispersion<a class="headerlink" href="#overdispersion" title="Permalink to this heading">#</a></h2>
<p>From <a class="reference external" href="https://ubc-mds.github.io/DSCI_551_stat-prob-dsci/lectures/index.html"><strong>DSCI 551</strong></a>, we know that population variances of some random variables are <strong>in function</strong> of their respective means. For instance:</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(X \sim \text{Exponential}(\lambda)\)</span>, then <span class="math notranslate nohighlight">\(\mathbb{E}(X) = \lambda\)</span> and <span class="math notranslate nohighlight">\(\text{Var}(X) = \lambda^2\)</span>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(X \sim \text{Binomial}(n , p)\)</span>, then <span class="math notranslate nohighlight">\(\mathbb{E}(X) = n p\)</span> and <span class="math notranslate nohighlight">\(\text{Var}(X) = n p (1 - p)\)</span>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(X \sim \text{Poisson}(\lambda)\)</span>, then <span class="math notranslate nohighlight">\(\mathbb{E}(X) = \lambda\)</span> and <span class="math notranslate nohighlight">\(\text{Var}(X) = \lambda\)</span>.</p></li>
</ul>
<p>Now, you might wonder:</p>
<blockquote>
<div><p>How does equidispersion affect our Poisson regression model?</p>
</div></blockquote>
<p>First, we must clarify that GLMs naturally deal with some types of <strong>heteroscedasticity</strong> (inequality of variances across the responses). For example, note that each observation <span class="math notranslate nohighlight">\(i = 1, \dots, n\)</span> in our training set (used to estimate a Poisson regression model) is assumed as:</p>
<div class="math notranslate nohighlight">
\[Y_i \sim \text{Poisson}(\lambda_i),\]</div>
<p>where the indexed parameter <span class="math notranslate nohighlight">\(\lambda_i\)</span> makes the model flexible enough to deal with heteroscedasticity. Moreover, the larger <span class="math notranslate nohighlight">\(\lambda_i\)</span> is, the larger the variance per observation will be.</p>
<p>Let us make a quick simulation on this matter. The code below creates ten samples of <span class="math notranslate nohighlight">\(n = 1000\)</span> each from different Poisson populations with an increasing variance <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">set.seed</span><span class="p">(</span><span class="m">562</span><span class="p">)</span>

<span class="n">poisson_samples</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">tibble</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">-1</span><span class="p">,</span><span class="w"> </span><span class="n">lambda</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">)</span>

<span class="nf">for </span><span class="p">(</span><span class="n">lambda</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="nf">seq</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">91</span><span class="p">,</span><span class="w"> </span><span class="m">10</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rpois</span><span class="p">(</span><span class="m">1000</span><span class="p">,</span><span class="w"> </span><span class="n">lambda</span><span class="p">)</span>
<span class="w">  </span><span class="n">poisson_samples</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">poisson_samples</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">bind_rows</span><span class="p">(</span><span class="nf">tibble</span><span class="p">(</span>
<span class="w">    </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sample</span><span class="p">,</span>
<span class="w">    </span><span class="n">lambda</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">lambda</span>
<span class="w">  </span><span class="p">))</span>
<span class="p">}</span>

<span class="n">poisson_samples</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">poisson_samples</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">filter</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="m">-1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The side-by-side jitter plots below illustrate the impact of an increasing variance in each of the ten Poisson populations, where each set of <span class="math notranslate nohighlight">\(n = 1000\)</span> data points gets more and more spread out. We see the same trend with the side-by-side boxplots.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">poisson_jitter_plots</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">poisson_samples</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">lambda</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_jitter</span><span class="p">(</span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">.</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2.5</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">theme</span><span class="p">(</span>
<span class="w">    </span><span class="n">plot.title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">31</span><span class="p">,</span><span class="w"> </span><span class="n">face</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;bold&quot;</span><span class="p">),</span>
<span class="w">    </span><span class="n">axis.text</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">21</span><span class="p">),</span>
<span class="w">    </span><span class="n">axis.title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">27</span><span class="p">)</span>
<span class="w">  </span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">coord_cartesian</span><span class="p">(</span><span class="n">ylim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">150</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">labs</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Observed Value&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">ggtitle</span><span class="p">(</span><span class="s">&quot;Side-by-Side Jitter Plots&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">scale_x_continuous</span><span class="p">(</span><span class="n">breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">seq</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">91</span><span class="p">,</span><span class="w"> </span><span class="m">10</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w"> </span>
<span class="w">  </span><span class="nf">labs</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">expression</span><span class="p">(</span><span class="n">lambda</span><span class="p">))</span>
<span class="n">poisson_jitter_plots</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7dee0211fb0e8db00b0d1ffb20ecaf63461d89daaa32fde20e0d5ef945df0e06.png" src="../_images/7dee0211fb0e8db00b0d1ffb20ecaf63461d89daaa32fde20e0d5ef945df0e06.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">poisson_boxplots</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">poisson_samples</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">as_factor</span><span class="p">(</span><span class="n">lambda</span><span class="p">),</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w"> </span>
<span class="w">  </span><span class="nf">geom_boxplot</span><span class="p">()</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">theme</span><span class="p">(</span>
<span class="w">    </span><span class="n">plot.title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">31</span><span class="p">,</span><span class="w"> </span><span class="n">face</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;bold&quot;</span><span class="p">),</span>
<span class="w">    </span><span class="n">axis.text</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">21</span><span class="p">),</span>
<span class="w">    </span><span class="n">axis.title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">27</span><span class="p">)</span>
<span class="w">  </span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">coord_cartesian</span><span class="p">(</span><span class="n">ylim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">150</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">labs</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Observed Value&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;lambda&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">ggtitle</span><span class="p">(</span><span class="s">&quot;Side-by-Side Boxplots&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span>
<span class="w">  </span><span class="nf">labs</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">expression</span><span class="p">(</span><span class="n">lambda</span><span class="p">))</span>
<span class="n">poisson_boxplots</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1a74fb6bf155cd2c4bb9ec19a3b5092c3f484aad6b1eec0bf0224ecf1d02d3cb.png" src="../_images/1a74fb6bf155cd2c4bb9ec19a3b5092c3f484aad6b1eec0bf0224ecf1d02d3cb.png" />
</div>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>We already defined <strong>equidispersion</strong> as an important characteristic of a Poisson random variable. Furthermore, we already explained that this random variable has the same mean and variance. The mean is the average of values in our dataset. Variance measures how spread the data are. It is computed as the average of the squared differences from the mean. A variance will be equal to zero if all values in our dataset are identical. The greater the difference between the values, the greater the variance.</p>
</div>
<p>From the below plot, note that the relationship of the <strong>sample variance</strong> in these samples is practically linear to the population <span class="math notranslate nohighlight">\(\lambda\)</span>. So this is what equidispersion graphically looks like.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">suppressWarnings</span><span class="p">(</span><span class="nf">suppressMessages</span><span class="p">(</span><span class="nf">print</span><span class="p">(</span><span class="n">poisson_samples</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">group_by</span><span class="p">(</span><span class="n">lambda</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">summarise</span><span class="p">(</span><span class="n">sample_variance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">var</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ggplot</span><span class="p">()</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_line</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">lambda</span><span class="p">,</span><span class="w"> </span><span class="n">sample_variance</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_point</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">lambda</span><span class="p">,</span><span class="w"> </span><span class="n">sample_variance</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">4</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">theme</span><span class="p">(</span>
<span class="w">    </span><span class="n">plot.title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">31</span><span class="p">,</span><span class="w"> </span><span class="n">face</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;bold&quot;</span><span class="p">),</span>
<span class="w">    </span><span class="n">axis.text</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">21</span><span class="p">),</span>
<span class="w">    </span><span class="n">axis.title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">27</span><span class="p">)</span>
<span class="w">  </span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">labs</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Sample Variance&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">expression</span><span class="p">(</span><span class="n">lambda</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">ggtitle</span><span class="p">(</span><span class="s">&quot;Sample Variance versus Lambda&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">scale_x_continuous</span><span class="p">(</span><span class="n">breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">seq</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">91</span><span class="p">,</span><span class="w"> </span><span class="m">10</span><span class="p">)))))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7715d2b934d262c6a680e4a87bf5789618d5f64b2f83c72b94ffe9b08171c6d8.png" src="../_images/7715d2b934d262c6a680e4a87bf5789618d5f64b2f83c72b94ffe9b08171c6d8.png" />
</div>
</div>
<p>Having said all this, in many cases, the variance of our data is sometimes larger than the variance considered by our model <strong>as in the basic Poisson regression</strong>.</p>
<div class="tip admonition">
<p class="admonition-title">Definition of Overdispersion</p>
<p>When the variance is larger than the mean in a random variable, we have <strong>overdispersion</strong>. This matter will impact the standard error of our parameter estimates in a basic Poisson regression, as we will see.</p>
</div>
<p>Let us go back to our <code class="docutils literal notranslate"><span class="pre">poisson_model_2</span></code> with the log link function from the <code class="docutils literal notranslate"><span class="pre">crabs</span></code> dataset with two regressors: <code class="docutils literal notranslate"><span class="pre">width</span></code> (<span class="math notranslate nohighlight">\(X_{\texttt{width}_i}\)</span>) and <code class="docutils literal notranslate"><span class="pre">color</span></code> (<span class="math notranslate nohighlight">\(X_{\texttt{color_darker}_i}\)</span>, <span class="math notranslate nohighlight">\(X_{\texttt{color_light}_i}\)</span>, and <span class="math notranslate nohighlight">\(X_{\texttt{color_medium}_i}\)</span>) for the <span class="math notranslate nohighlight">\(i\)</span>th observation:</p>
<div class="math notranslate nohighlight">
\[
h(\lambda_i) = \log (\lambda_i) = \beta_0 + \beta_1 X_{\texttt{width}_i} + \beta_2 X_{\texttt{color_darker}_i} + \beta_3 X_{\texttt{color_light}_i} + \beta_4 X_{\texttt{color_medium}_i}.
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">summary</span><span class="p">(</span><span class="n">poisson_model_2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
glm(formula = n_males ~ width + color, family = poisson, data = crabs)

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -3.08640    0.55750  -5.536 3.09e-08 ***
width        0.14934    0.02084   7.166 7.73e-13 ***
colordarker -0.01100    0.18041  -0.061   0.9514    
colorlight   0.43636    0.17636   2.474   0.0133 *  
colormedium  0.23668    0.11815   2.003   0.0452 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 632.79  on 172  degrees of freedom
Residual deviance: 559.34  on 168  degrees of freedom
AIC: 924.64

Number of Fisher Scoring iterations: 6
</pre></div>
</div>
</div>
</div>
<p>We will test whether there is overdispersion in this Poisson regression model via function <code class="docutils literal notranslate"><span class="pre">dispersiontest()</span></code> (from package <code class="docutils literal notranslate"><span class="pre">AER</span></code>).</p>
<p>Let <span class="math notranslate nohighlight">\(Y_i\)</span> be the <span class="math notranslate nohighlight">\(i\)</span>th Poisson response in the count regression model. <strong>Ideally in the presence of equidispersion</strong>, <span class="math notranslate nohighlight">\(Y_i\)</span> has the following parameters:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{gather*}
\mathbb{E}(Y_i) = \lambda_i \\
\text{Var}(Y_i) = \lambda_i.
\end{gather*}\end{split}\]</div>
<p>The test uses the following mathematical expression:</p>
<div class="math notranslate nohighlight">
\[
\text{Var}(Y_i) = \overbrace{(1 + \gamma)}^\text{Dispersion Factor} \lambda_i,
\]</div>
<p>with the hypotheses</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{gather*}
H_0: 1 + \gamma = 1 \\
H_a: 1 + \gamma &gt; 1.
\end{gather*}\end{split}\]</div>
<p>When there is evidence of overdispersion in our data, <strong>we will reject <span class="math notranslate nohighlight">\(H_0\)</span></strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">dispersiontest</span><span class="p">(</span><span class="n">poisson_model_2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>	Overdispersion test

data:  poisson_model_2
z = 5.3255, p-value = 5.033e-08
alternative hypothesis: true dispersion is greater than 1
sample estimates:
dispersion 
  3.154127 
</pre></div>
</div>
</div>
</div>
<p>With <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span>, we reject <span class="math notranslate nohighlight">\(H_0\)</span> since the <span class="math notranslate nohighlight">\(p\text{-value} &lt; .001\)</span>. Hence, the <code class="docutils literal notranslate"><span class="pre">poisson_model_2</span></code> has overdispersion.</p>
</section>
<section id="negative-binomial-regression">
<h2>7. Negative Binomial Regression<a class="headerlink" href="#negative-binomial-regression" title="Permalink to this heading">#</a></h2>
<p>Let</p>
<div class="math notranslate nohighlight">
\[Y_i \sim \text{Negative Binomial} (m, p_i) \quad \text{for} \quad i = 1, \dots, n.\]</div>
<p>From <strong>DSCI 551</strong>, recall that a Negative Binomial distribution has the following probability mass function (PMF):</p>
<div class="math notranslate nohighlight" id="equation-nb-pdf">
<span class="eqno">(8)<a class="headerlink" href="#equation-nb-pdf" title="Permalink to this equation">#</a></span>\[\begin{equation}
P(Y_i = y_i \mid m, p_i) = {m - 1 + y_i \choose y_i} p_i^{m} (1 - p_i)^{y_i} \quad \text{for} \quad y_i = 0, 1, \dots
\end{equation}\]</div>
<p>A Negative Binomial random variable depicts <strong>the number of <span class="math notranslate nohighlight">\(y_i\)</span> failed independent Bernoulli trials before experiencing <span class="math notranslate nohighlight">\(m\)</span> successes</strong> with a probability of success <span class="math notranslate nohighlight">\(p_i\)</span>.</p>
<p>This distribution has the following mean and variance:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{gather*}
\mathbb{E}(Y_i) = \frac{m(1 - p_i)}{p_i} \\
\text{Var}(Y_i) = \frac{m(1 - p_i)}{p_i^2}.
\end{gather*}\end{split}\]</div>
<section id="reparametrization">
<h3>7.1. Reparametrization<a class="headerlink" href="#reparametrization" title="Permalink to this heading">#</a></h3>
<p>Under the following parametrization:</p>
<div class="math notranslate nohighlight" id="equation-nb-param">
<span class="eqno">(9)<a class="headerlink" href="#equation-nb-param" title="Permalink to this equation">#</a></span>\[\begin{equation}
\lambda_i = \frac{m (1 - p_i)}{p_i} \qquad \Rightarrow \qquad p_i = \frac{m}{m + \lambda_i},
\end{equation}\]</div>
<p>the mean and variance of a Negative Binomial random variable can be reexpressed as</p>
<div class="math notranslate nohighlight" id="equation-nb-mean-variance">
<span class="eqno">(10)<a class="headerlink" href="#equation-nb-mean-variance" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{gather}
\mathbb{E}(Y_i) = \lambda_i \\
\text{Var}(Y_i) = \lambda_i \left( 1 + \frac{\lambda_i}{m} \right).
\end{gather}\end{split}\]</div>
<p>This reparametrized variance indicates that a Negative Binomial random variable allows for overdispersion through factor <span class="math notranslate nohighlight">\(\left( 1 + \frac{\lambda_i}{m} \right)\)</span>.</p>
<p>Finally, by applying parametrization <a class="reference internal" href="#equation-nb-param">(9)</a> in PMF <a class="reference internal" href="#equation-nb-pdf">(8)</a>, we have the following:</p>
<div class="math notranslate nohighlight" id="equation-nb-alt-pdf">
<span class="eqno">(11)<a class="headerlink" href="#equation-nb-alt-pdf" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{align}
P(Y_i = y_i \mid m, p_i) &amp;= {m - 1 + y_i \choose y_i} p_i^{m} (1 - p_i)^{y_i} \\
&amp;= \frac{(m + y_i - 1)!}{y_i! (m - 1 )!} \left( \frac{m}{m + \lambda} \right)^{m} \left( 1 - \frac{m}{m + \lambda} \right)^{y_i} \\
&amp;= \frac{\Gamma(y_i + m)}{\Gamma(y_i + 1) \Gamma(m)} \left( \frac{m}{m + \lambda} \right)^{m} \left( 1 - \frac{m}{m + \lambda} \right)^{y_i},
\end{align}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\Gamma(\cdot)\)</span> is the <a class="reference external" href="https://www.statlect.com/mathematical-tools/gamma-function"><strong>Gamma function</strong></a>. We actually use the property</p>
<div class="math notranslate nohighlight">
\[\Gamma(a) = (a - 1)!,\]</div>
<p>where <span class="math notranslate nohighlight">\(a \geq 1\)</span> is an integer.</p>
<div class="warning admonition">
<p class="admonition-title">Note</p>
<p>In <strong>DSCI 551</strong>, we highlighted the fact that some distributions converge to anothers where certain parameters tend to infinity. It can be shown that</p>
<div class="math notranslate nohighlight">
\[X \sim \text{Poisson}(\lambda) = \lim_{m \to \infty} \text{Negative Binomial}(m, p).\]</div>
<p><strong>The proof is out of the scope of this course.</strong></p>
</div>
</section>
<section id="id7">
<h3>7.2. Data Modelling Framework<a class="headerlink" href="#id7" title="Permalink to this heading">#</a></h3>
<p>As in the case of Poisson regression with <span class="math notranslate nohighlight">\(k\)</span> regressors, the Negative Binomial case is a GLM with the following link function:</p>
<div class="math notranslate nohighlight" id="equation-nb-model">
<span class="eqno">(12)<a class="headerlink" href="#equation-nb-model" title="Permalink to this equation">#</a></span>\[\begin{equation}
h(\lambda_i) = \log(\lambda_i) = \beta_0 + \beta_1 X_{i,1} + \dots + \beta_k X_{i,k}.
\end{equation}\]</div>
<p>Lastly, note the following:</p>
<ul class="simple">
<li><p>From <a class="reference internal" href="#equation-nb-mean-variance">(10)</a>, let <span class="math notranslate nohighlight">\(\theta = \frac{1}{m}\)</span>. Then, Negative Binomial regression will assume the following variance:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
\text{Var}(Y_i) &amp;= \lambda_i \left( 1 + \frac{\lambda_i}{m} \right) \\
&amp;= \lambda_i + \frac{\lambda_i^2}{m} \\
&amp;= \lambda_i + \theta \lambda_i^2.
\end{align*}\end{split}\]</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Therefore, the model has even more flexibility to deal with overdispersion compared to Poisson regression.</p>
</div>
</section>
<section id="id8">
<h3>7.3. Estimation<a class="headerlink" href="#id8" title="Permalink to this heading">#</a></h3>
<p>Via a training set of size <span class="math notranslate nohighlight">\(n\)</span> whose responses are <strong>independent counts</strong> <span class="math notranslate nohighlight">\(Y_i\)</span> (<span class="math notranslate nohighlight">\(i = 1, \dots, n\)</span>), we use the reparametrized PMF <a class="reference internal" href="#equation-nb-alt-pdf">(11)</a> along with the link function <a class="reference internal" href="#equation-nb-model">(12)</a> via <strong>maximum likelihood estimation</strong> to obtain <span class="math notranslate nohighlight">\(\hat{\beta}_0, \hat{\beta}_1, \dots, \hat{\beta}_k, \hat{\theta}\)</span>.</p>
<p>To fit a Negative Binomial regression via <code class="docutils literal notranslate"><span class="pre">R</span></code>, we can use the function <code class="docutils literal notranslate"><span class="pre">glm.nb()</span></code> from package <code class="docutils literal notranslate"><span class="pre">MASS</span></code>.</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">MASS</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">negative_binomial_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">glm.nb</span><span class="p">(</span><span class="n">n_males</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">width</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">color</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">crabs</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">negative_binomial_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
glm.nb(formula = n_males ~ width + color, data = crabs, init.theta = 0.9320986132, 
    link = log)

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -3.87271    1.19047  -3.253  0.00114 ** 
width        0.17839    0.04529   3.939 8.18e-05 ***
colordarker -0.01570    0.33125  -0.047  0.96220    
colorlight   0.52033    0.38396   1.355  0.17536    
colormedium  0.24440    0.22864   1.069  0.28510    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for Negative Binomial(0.9321) family taken to be 1)

    Null deviance: 216.56  on 172  degrees of freedom
Residual deviance: 196.23  on 168  degrees of freedom
AIC: 760.6

Number of Fisher Scoring iterations: 1


              Theta:  0.932 
          Std. Err.:  0.168 

 2 x log-likelihood:  -748.596 
</pre></div>
</div>
</div>
</div>
<p>Note that the output provides <span class="math notranslate nohighlight">\(\hat{\theta} = 0.932\)</span> along with its standard error <span class="math notranslate nohighlight">\(\text{se} \left( \hat{\theta} \right) = 0.168\)</span>. Moreover, the standard errors of the other estimates are larger than in <code class="docutils literal notranslate"><span class="pre">poisson_model_2</span></code> (check the below summary).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">summary</span><span class="p">(</span><span class="n">poisson_model_2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
glm(formula = n_males ~ width + color, family = poisson, data = crabs)

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -3.08640    0.55750  -5.536 3.09e-08 ***
width        0.14934    0.02084   7.166 7.73e-13 ***
colordarker -0.01100    0.18041  -0.061   0.9514    
colorlight   0.43636    0.17636   2.474   0.0133 *  
colormedium  0.23668    0.11815   2.003   0.0452 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 632.79  on 172  degrees of freedom
Residual deviance: 559.34  on 168  degrees of freedom
AIC: 924.64

Number of Fisher Scoring iterations: 6
</pre></div>
</div>
</div>
</div>
<div class="exercise admonition" id="lecture1-q3">

<p class="admonition-title"><span class="caption-number">Exercise 3 </span></p>
<section id="exercise-content">
<p>Under the context of this crab problem with overdispersion as assessed by the <strong>overdispersion test</strong>, suppose we use <code class="docutils literal notranslate"><span class="pre">poisson_model_2</span></code> to perform inference on the response and regressors.</p>
<p>What is the consequence of using these <strong>underestimated</strong> standard errors compared to the ones from <code class="docutils literal notranslate"><span class="pre">negative_binomial_model</span></code>?</p>
<p><strong>A.</strong> We are more prone to commit Type I errors (i.e., rejecting <span class="math notranslate nohighlight">\(H_a\)</span> when in fact it is true).</p>
<p><strong>B.</strong> We are more prone to commit Type I errors (i.e., rejecting <span class="math notranslate nohighlight">\(H_0\)</span> when in fact it is true).</p>
<p><strong>C.</strong> We are more prone to commit Type II errors (i.e., rejecting <span class="math notranslate nohighlight">\(H_a\)</span> when in fact it is true).</p>
<p><strong>D.</strong> We are more prone to commit Type II errors (i.e., rejecting <span class="math notranslate nohighlight">\(H_0\)</span> when in fact it is true).</p>
</section>
</div>
</section>
<section id="inference-coefficient-interpretation-prediction-goodness-of-fit-and-model-selection">
<h3>7.4. Inference, Coefficient Interpretation, Prediction, Goodness of Fit, and Model Selection<a class="headerlink" href="#inference-coefficient-interpretation-prediction-goodness-of-fit-and-model-selection" title="Permalink to this heading">#</a></h3>
<p>Since the link function in Negative Binomial regression is the same as in Poisson regression; <strong>inference, coefficient interpretation, and prediction are performed similarly</strong> (even with the same <code class="docutils literal notranslate"><span class="pre">broom</span></code> functions!). Regarding <strong>model selection</strong>, since we use a regular maximum likelihood approach to estimate the regression parameters, we can use analysis of deviance, AIC, and BIC to perform model selection and/or goodness of fit testing.</p>
</section>
</section>
<section id="wrapping-up-on-count-regression-models">
<h2>8. Wrapping Up on Count Regression Models<a class="headerlink" href="#wrapping-up-on-count-regression-models" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Poisson regression is the most basic regression approach for a count-type response. We have to be cautious about this model since it assumes equidispersion.</p></li>
<li><p>Negative Binomial regression is more flexible than the Poisson case since the variance’s representation allows it to be a quadratic function of the mean with an additional parameter.</p></li>
<li><p>As we can see, the Negative Binomial regression model has extra parameters in the variance expression that allows us to construct a more accurate model for specific count data (since the mean and the variance do not need to be equal!). However, the variable association tests and conclusions are conducted similarly under these two models, as in the Poisson regression.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            name: "ir",
            path: "./notes"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="../lecture-learning-objectives.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Lecture Learning Objectives</p>
      </div>
    </a>
    <a class="right-next"
       href="lecture2_glm_model_selection_multinomial.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Lecture 2 - Generalized Linear Models: Model Selection and Multinomial Logistic Regression</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-brief-digression-on-textboxes">A Brief Digression on Textboxes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#today-s-learning-goals">Today’s Learning Goals</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-libraries">Loading Libraries</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-preamble-on-our-data-science-workflow">1. A Preamble on our Data Science Workflow</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#study-design">1.1. Study Design</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-collection-and-wrangling">1.2. Data Collection and Wrangling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exploratory-data-analysis">1.3. Exploratory Data Analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-modelling">1.4. Data Modelling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation">1.5. Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#results">1.6. Results</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#storytelling">1.7. Storytelling</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#review-of-ordinary-least-squares-regression">2. Review of Ordinary Least-squares Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-modelling-framework">2.1. Data Modelling Framework</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#modelling-assumptions">2.1.1. Modelling Assumptions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-regressors">2.1.2. Categorical Regressors</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#can-we-apply-linear-regression-here">2.2. Can We Apply Linear Regression Here?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-makes-a-regression-model-linear">2.3. What makes a regression model “<em>linear</em>”?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">2.4. Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference">2.5. Inference</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#violations-of-assumptions">2.6. Violations of Assumptions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#distributional-misspecification">2.6.1. Distributional Misspecification</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#random-components-with-non-zero-mean">2.6.2. Random Components with Non-Zero Mean</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#heterocedasticity">2.6.3. Heterocedasticity</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#correlated-random-components">2.6.4. Correlated Random Components</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-ordinary-least-squares-regression-does-not-suffice">3. When Ordinary Least-squares Regression Does Not Suffice</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#paving-the-way-to-generalized-linear-models">4. Paving the Way to Generalized Linear Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nature-of-the-model-function">4.1. Nature of the Model Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-regression-problem">4.2. The Regression Problem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#black-box-models">4.3. Black-box Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretability-in-linear-models">4.4. Interpretability in Linear Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-types-of-parametric-assumptions">4.5. The Types of Parametric Assumptions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#an-example-where-ols-regression-totally-goes-wrong">4.6. An example where OLS regression totally goes wrong</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#let-us-make-it-right">4.7. Let us make it right!</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#restricted-response-ranges-in-linear-regression">4.8. Restricted Response Ranges in Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#link-function">4.9. Link Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#poisson-regression">5. Poisson Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-crabs-dataset">5.1. The Crabs Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">5.2. Exploratory Data Analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">5.3. Data Modelling Framework</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">5.4. Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">5.5. Inference</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#coefficient-interpretation">5.6. Coefficient Interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictions">5.7. Predictions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overdispersion">6. Overdispersion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#negative-binomial-regression">7. Negative Binomial Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reparametrization">7.1. Reparametrization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">7.2. Data Modelling Framework</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">7.3. Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-coefficient-interpretation-prediction-goodness-of-fit-and-model-selection">7.4. Inference, Coefficient Interpretation, Prediction, Goodness of Fit, and Model Selection</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wrapping-up-on-count-regression-models">8. Wrapping Up on Count Regression Models</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By G. Alexi Rodríguez-Arelis, Rodolfo Lourenzutti, and Vincenzo Coia
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>