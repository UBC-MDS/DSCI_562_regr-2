

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Regression Cheatsheet &#8212; DSCI 562 - Regression II</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notes/appendix-reg-cheatsheet';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Regression Mind Map" href="appendix-reg-mindmap.html" />
    <link rel="prev" title="Binary Logistic Regression" href="appendix-binary-log-regression.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../README.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/UBC_MDS_logo.png" class="logo__image only-light" alt="DSCI 562 - Regression II - Home"/>
    <script>document.write(`<img src="../_static/UBC_MDS_logo.png" class="logo__image only-dark" alt="DSCI 562 - Regression II - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../README.html">
                    Welcome to DSCI 562: Regression II
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lecture-learning-objectives.html">Lecture Learning Objectives</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture1-glm-link-functions-and-count-regression.html">Lecture 1 - Generalized Linear Models: Link Functions and Count Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture2_glm_model_selection_multinomial.html">Lecture 2 - Generalized Linear Models: Model Selection and Multinomial Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture3_glm_ordinal_regression.html">Lecture 3 - Generalized Linear Models: Ordinal Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture4_linear_mixed_effects_models.html">Lecture 4 - Linear Mixed-Effects Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture5_survival_analysis.html">Lecture 5 - Survival Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture6_local_regression.html">Lecture 6 - Local Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture7_quantile_regression.html">Lecture 7 - Quantile Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture8_missing_data.html">Lecture 8 - Missing Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendices</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="appendix-binary-log-regression.html">Binary Logistic Regression</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Regression Cheatsheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix-reg-mindmap.html">Regression Mind Map</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix-binary-multivariate-normal.html">Fundamentals of the Multivariate Normal Distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix-greek-alphabet.html">Greek Alphabet</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/UBC-MDS/DSCI_562_regr-2" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/UBC-MDS/DSCI_562_regr-2/issues/new?title=Issue%20on%20page%20%2Fnotes/appendix-reg-cheatsheet.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/notes/appendix-reg-cheatsheet.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Regression Cheatsheet</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-logistic-regression">Binary Logistic Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-modelling-framework">Data Modelling Framework</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation">Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference">Inference</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-selection">Model Selection</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#analysis-of-deviance">Analysis of Deviance</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#akaike-information-criterion">Akaike Information Criterion</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-information-criterion">Bayesian Information Criterion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cox-proportional-hazards-model">Cox Proportional Hazards Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Data Modelling Framework</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Inference</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction">Prediction</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cumulative-hazard-function">Cumulative Hazard Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hazard-function">Hazard Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#link-functions-in-generalized-linear-models-glms">Link Functions in Generalized Linear Models (GLMs)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multinomial-logistic-regression">Multinomial Logistic Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Data Modelling Framework</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Inference</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Model Selection</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#negative-binomial-regression">Negative Binomial Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reparametrization">Reparametrization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Data Modelling Framework</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-coefficient-interpretation-prediction-goodness-of-fit-and-model-selection">Inference, Coefficient Interpretation, Prediction, Goodness of Fit, and Model Selection</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ordinal-logistic-regression">Ordinal Logistic Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Data Modelling Framework</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">Inference</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">Model Selection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#non-proportional-odds-model">Non-proportional Odds Model</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-brant-wald-test">The Brant-Wald Test</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ordinary-least-squares-regression">Ordinary Least-squares Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">Data Modelling Framework</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modelling-assumptions">Modelling Assumptions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition-of-linearity">Definition of Linearity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">Inference</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#poisson-regression">Poisson Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">Data Modelling Framework</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">Inference</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">Model Selection</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#goodness-of-fit-test">Goodness of Fit Test</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#analysis-of-deviance-for-nested-models">Analysis of Deviance for Nested Models</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">Akaike Information Criterion</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">Bayesian Information Criterion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#survival-function">Survival Function</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="regression-cheatsheet">
<h1>Regression Cheatsheet<a class="headerlink" href="#regression-cheatsheet" title="Permalink to this heading">#</a></h1>
<section id="binary-logistic-regression">
<h2>Binary Logistic Regression<a class="headerlink" href="#binary-logistic-regression" title="Permalink to this heading">#</a></h2>
<section id="data-modelling-framework">
<h3>Data Modelling Framework<a class="headerlink" href="#data-modelling-framework" title="Permalink to this heading">#</a></h3>
<p>The Binary Logistic regression model has a response variable in the form:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Y_i =
\begin{cases}
1 \; \; \; \; \mbox{if the $i$th observation is a success},\\
0 \; \; \; \; \mbox{otherwise.}
\end{cases}
\end{split}\]</div>
<p>As the response variable can only take the values <span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span>, the key parameter becomes the probability that <span class="math notranslate nohighlight">\(Y_i\)</span> takes on the value of <span class="math notranslate nohighlight">\(1\)</span>, i.e. the probability of success, denoted as <span class="math notranslate nohighlight">\(p_i\)</span>. Hence:</p>
<div class="math notranslate nohighlight">
\[
Y_i \sim \text{Bernoulli}(p_i).
\]</div>
<p>The Binary Logistic regression approach models the probability of success, <span class="math notranslate nohighlight">\(p_i\)</span>, of the binary response <span class="math notranslate nohighlight">\(Y_i\)</span>. To re-express <span class="math notranslate nohighlight">\(p_i\)</span> <strong>on an unrestricted scale</strong>, the modelling is done in terms of the logit function (the link function in this model).</p>
<p>Specifically, for a training set of size <span class="math notranslate nohighlight">\(n\)</span>, <span class="math notranslate nohighlight">\(p_i\)</span> (<span class="math notranslate nohighlight">\(i = 1, 2, \dots, n\)</span>) will depend on the values of the <span class="math notranslate nohighlight">\(k\)</span> regressors <span class="math notranslate nohighlight">\(X_{i, 1}, X_{i, 2}, \dots, X_{i, k}\)</span> in the form:</p>
<div class="math notranslate nohighlight">
\[
h(p_i) = \mbox{logit}(p_i) = \log \left( \frac{p_i}{1 - p_i} \right) = \beta_0 + \beta_1 X_{i, 1} + \beta_1 X_{i, 2} + \ldots + \beta_k X_{i, k},
\]</div>
<p>or equivalently</p>
<div class="math notranslate nohighlight">
\[
p_i = \frac{\exp \left[ \mbox{logit} (p_i) \right]}{1 + \exp \left[ \mbox{logit}(p_i) \right]}.
\]</div>
<p>Note that the <span class="math notranslate nohighlight">\(\log(\cdot)\)</span> notation in the model above refers to the <strong>natural logarithm</strong>, i.e., <strong>logarithm base <span class="math notranslate nohighlight">\(e\)</span></strong>. The equation above for <span class="math notranslate nohighlight">\(p_i\)</span> shows that this Binary Logistic regression model will result in values of the probability of success <span class="math notranslate nohighlight">\(p_i\)</span> that are always between 0 and 1. Furthermore, the response in this GLM is called the log-odds, the logarithm of the odds</p>
<div class="math notranslate nohighlight">
\[\frac{p_i}{1 - p_i},\]</div>
<p>the ratio of the probability of the event to the probability of the non-event.</p>
</section>
<section id="estimation">
<h3>Estimation<a class="headerlink" href="#estimation" title="Permalink to this heading">#</a></h3>
<p>Under a general framework with <span class="math notranslate nohighlight">\(k\)</span> regressors, the <strong>regression parameters</strong> <span class="math notranslate nohighlight">\(\beta_0, \beta_1, \dots, \beta_k\)</span> in this model are also unknown. In order to fit the model, we can use the function <code class="docutils literal notranslate"><span class="pre">glm()</span></code> and its argument <code class="docutils literal notranslate"><span class="pre">family</span> <span class="pre">=</span> <span class="pre">binomial</span></code> (required to specify the binary nature of the response), which obtains the estimates <span class="math notranslate nohighlight">\(\hat{\beta}_0, \hat{\beta}_1, \dots \hat{\beta}_k\)</span> (note the hat notation).</p>
<p>The estimates are obtained through <strong>maximum likelihood</strong> where we assume a <strong>joint probability mass function of the <span class="math notranslate nohighlight">\(n\)</span> responses <span class="math notranslate nohighlight">\(Y_i\)</span></strong>.</p>
</section>
<section id="inference">
<h3>Inference<a class="headerlink" href="#inference" title="Permalink to this heading">#</a></h3>
<p>We can determine <strong>whether a regressor is statistically associated with the logarithm of the response’s odds</strong> through <strong>hypothesis testing</strong> for the parameters <span class="math notranslate nohighlight">\(\beta_j\)</span>. We will need information about the estimated regression coefficient <span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span> and its corresponding variability which is reflected in the <strong>standard error</strong> of the estimate, <span class="math notranslate nohighlight">\(\mbox{se} \left( \hat{\beta}_j \right)\)</span>.</p>
<p>To determine the <strong>statistical significance</strong> of <span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span>, you can use the <strong>Wald statistic</strong> <span class="math notranslate nohighlight">\(z_j\)</span></p>
<div class="math notranslate nohighlight">
\[
z_j = \frac{\hat{\beta}_j}{\mbox{se}(\hat{\beta}_j)}
\]</div>
<p>to test the hypotheses</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{gather*}
H_0: \beta_j = 0 \\
H_a: \beta_j \neq 0.
\end{gather*}\end{split}\]</div>
<p><strong>A statistic like <span class="math notranslate nohighlight">\(z_j\)</span> is analogous to the <span class="math notranslate nohighlight">\(t\)</span>-value in Ordinary Least-squares (OLS) regression.</strong> However, in Binary Logistic regression, provided the sample size <span class="math notranslate nohighlight">\(n\)</span> is large enough, <span class="math notranslate nohighlight">\(z_j\)</span> has an <strong>approximately Standard Normal distribution</strong> under <span class="math notranslate nohighlight">\(H_0\)</span> rather than a <span class="math notranslate nohighlight">\(t\)</span>-distribution.</p>
<p><code class="docutils literal notranslate"><span class="pre">R</span></code> provides the corresponding <strong><span class="math notranslate nohighlight">\(p\)</span>-value</strong> for each <span class="math notranslate nohighlight">\(\beta_j\)</span>. The smaller the <span class="math notranslate nohighlight">\(p\)</span>-value, the stronger the evidence against the null hypothesis <span class="math notranslate nohighlight">\(H_0\)</span>. Hence, a small enough <span class="math notranslate nohighlight">\(p\)</span>-value (less than the significance level <span class="math notranslate nohighlight">\(\alpha\)</span>) indicates that the data provides evidence in favour of <strong>association</strong> (<strong>or causation in the case of an experimental study!</strong>) between the log-dds and the <span class="math notranslate nohighlight">\(j\)</span>th regressor. Furthermore, given a specified level of confidence, we can construct approximate <span class="math notranslate nohighlight">\((1 - \alpha) \times 100\%\)</span> <strong>confidence intervals</strong> (CIs) for the corresponding true value of <span class="math notranslate nohighlight">\(\beta_j\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_j \pm z_{\alpha/2}\mbox{se}(\hat{\beta}_j),
\]</div>
<p>where <span class="math notranslate nohighlight">\(z_{\alpha/2}\)</span> is the upper <span class="math notranslate nohighlight">\(\alpha/2\)</span> quantile of the <strong>Standard Normal distribution</strong>.</p>
</section>
<section id="model-selection">
<span id="bin-log-model-selection-app"></span><h3>Model Selection<a class="headerlink" href="#model-selection" title="Permalink to this heading">#</a></h3>
<section id="analysis-of-deviance">
<h4>Analysis of Deviance<a class="headerlink" href="#analysis-of-deviance" title="Permalink to this heading">#</a></h4>
<p>The <strong>deviance</strong> (<span class="math notranslate nohighlight">\(D_k\)</span>) criterion can be used to compare a given model with <span class="math notranslate nohighlight">\(k\)</span> regressors with that of a <strong>baseline model</strong>. The usual baseline model is the <strong>saturated</strong> or <strong>full model</strong>, which perfectly fits the data because it allows a distinct probability of success <span class="math notranslate nohighlight">\(p_i\)</span> for the <span class="math notranslate nohighlight">\(i\)</span>th observation in the training dataset (<span class="math notranslate nohighlight">\(i = 1, \dots, n\)</span>), <strong>unrelated to the <span class="math notranslate nohighlight">\(k\)</span> regressors</strong>.</p>
<p>Given the definition of the saturated or full model under this context, we can view it as an <strong>overfitted model</strong>. Thus, we aim to avoid this type of model.</p>
<p>The <strong>maximized likelihood</strong> of this full model is denoted as <span class="math notranslate nohighlight">\(\hat{\mathscr{l}}_f\)</span>. Now, let <span class="math notranslate nohighlight">\(\hat{\mathscr{l}}_k\)</span> be the value of the maximized likelihood computed from our dataset of <span class="math notranslate nohighlight">\(n\)</span> observation with <span class="math notranslate nohighlight">\(k\)</span> regressors.</p>
<p>We can compare the fits provided by these two models by the deviance <span class="math notranslate nohighlight">\(D_k\)</span> given by</p>
<div class="math notranslate nohighlight">
\[
D_k = -2 \log \Bigg(\frac{\hat{\mathscr{l}}_k}{\hat{\mathscr{l}}_f} \Bigg) =  -2 \left[ \log \left( \hat{\mathscr{l}}_k \right) - \log \left( \hat{\mathscr{l}}_f \right) \right].
\]</div>
<p>Note that <span class="math notranslate nohighlight">\(D_k\)</span> expresses <strong>how much our given model deviates from the full model on log-likelihood scale</strong>. This metric is interpreted as follows:</p>
<ul class="simple">
<li><p><strong>Large values</strong> of <span class="math notranslate nohighlight">\(D_k\)</span> arise when <span class="math notranslate nohighlight">\(\hat{\mathscr{l}}_k\)</span> is small relative to <span class="math notranslate nohighlight">\(\hat{\mathscr{l}}_f\)</span>, indicating that <strong>our given model fits the data poorly compared to the baseline model</strong>.</p></li>
<li><p><strong>Small values</strong> of <span class="math notranslate nohighlight">\(D_k\)</span> arise when <span class="math notranslate nohighlight">\(\hat{\mathscr{l}}_k\)</span> is similar to <span class="math notranslate nohighlight">\(\hat{\mathscr{l}}_f\)</span>, indicating that <strong>our given model provides a good fit to the data compared to the baseline model</strong>.</p></li>
</ul>
<p><strong>For the specific case of the Binary Logistic regression</strong>, it can be shown that <span class="math notranslate nohighlight">\(D_k\)</span> is represented by the following equation:</p>
<div class="math notranslate nohighlight" id="equation-deviance-bin-log-app">
<span class="eqno">(29)<a class="headerlink" href="#equation-deviance-bin-log-app" title="Permalink to this equation">#</a></span>\[\begin{equation}
D_k = -2 \sum_{i = 1}^n \left[\hat{p}_i \text{logit}(\hat{p}_i) + \log (1 - \hat{p}_i) \right],
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{p}_i\)</span> is the estimated probability of success for the <span class="math notranslate nohighlight">\(i\)</span>th observation for <span class="math notranslate nohighlight">\(i = 1, \dots, n\)</span> in our training set <strong>with our fitted model of <span class="math notranslate nohighlight">\(k\)</span> regressors</strong>. Equation <a class="reference internal" href="#equation-deviance-bin-log-app">(29)</a> above comes from <strong>maximum likelihood estimation (MLE)</strong>.</p>
<p><strong>For the specific case of Binary Logistic regression</strong>, deviance <span class="math notranslate nohighlight">\(D_k\)</span> <a class="reference internal" href="#equation-deviance-bin-log-app">(29)</a> cannot be used as a standalone metric of <strong>goodness of fit</strong> because of <strong>data sparsity</strong>; i.e., each <span class="math notranslate nohighlight">\(i\)</span>th observation has a different set of observed values for the <span class="math notranslate nohighlight">\(k\)</span> regressors if at least one of them is of <strong>continuous-type</strong>.</p>
<p>This data sparsity puts <span class="math notranslate nohighlight">\(D_k\)</span> just in function of the fitted probabilities <span class="math notranslate nohighlight">\(\hat{p}_i\)</span> and not on the observed values <span class="math notranslate nohighlight">\(y_i\)</span> (which tells us nothing about the agreement of our model with <span class="math notranslate nohighlight">\(k\)</span> regressors to the observed data!).</p>
<p>Still, for the case of Binary Logistic regression, we can use the analysis of deviance to perform model selection <strong>between two models where one is nested in the other</strong>.</p>
<p>Suppose <strong>Model 1</strong> is nested in <strong>Model 2</strong>. Hence, this specific model selection will involve a hypothesis testing. The hypotheses are:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{gather*}
H_0: \textbf{Model 1} \text{ fits the data better than } \textbf{Model 2} \\
H_a: \textbf{Model 2} \text{ fits the data better than } \textbf{Model 1}.
\end{gather*}\end{split}\]</div>
<p>Let <span class="math notranslate nohighlight">\(D_2\)</span> be the deviance for <strong>Model 2</strong> and <span class="math notranslate nohighlight">\(D_1\)</span> the deviance for <strong>Model 1</strong>. The test statistic <span class="math notranslate nohighlight">\(\Delta_D\)</span> for the analysis of deviance is given by:</p>
<div class="math notranslate nohighlight">
\[
\Delta_D = D_1 - D_2 \sim \chi^2_{d},
\]</div>
<p>which <strong>assymptotically</strong> (i.e., <span class="math notranslate nohighlight">\(n \rightarrow \infty\)</span>) is <a class="reference external" href="https://www.math.wm.edu/~leemis/chart/UDR/PDFs/Chisquare.pdf"><strong>Chi-squared distributed</strong></a> with <span class="math notranslate nohighlight">\(d\)</span> degrees of freedom under <span class="math notranslate nohighlight">\(H_0\)</span>. The <span class="math notranslate nohighlight">\(d\)</span> degrees of freedom are the <strong>regression parameters of difference between both models</strong>.</p>
<p>Formally, this nested hypothesis testing is called the <strong>likelihood-ratio test</strong>.</p>
</section>
<section id="akaike-information-criterion">
<h4>Akaike Information Criterion<a class="headerlink" href="#akaike-information-criterion" title="Permalink to this heading">#</a></h4>
<p><strong>One of the drawbacks of the analysis of deviance</strong> is that it only allows to test <strong>nested</strong> regression models when we have sparse data (i.e., each response is associated with a different set of values in the regressors).</p>
<p>Fortunately, we have alternatives for model selection. <strong>The Akaike Information Criterion (AIC) makes it possible to compare models that are either nested or not.</strong> For a model with <span class="math notranslate nohighlight">\(k\)</span> regressors and a deviance <span class="math notranslate nohighlight">\(D_k\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
\mbox{AIC}_k = D_k + 2k.
\end{equation}\]</div>
<p>Models with <strong>smaller</strong> values of <span class="math notranslate nohighlight">\(\mbox{AIC}_k\)</span> are preferred. That said, <span class="math notranslate nohighlight">\(\mbox{AIC}_k\)</span> favours models with small values of <span class="math notranslate nohighlight">\(D_k\)</span>.</p>
<p>However, <span class="math notranslate nohighlight">\(\mbox{AIC}_k\)</span> penalizes for including more regressors in the model. Hence, it discourages overfitting, which is key in model selection. This is why we select that model with the smallest <span class="math notranslate nohighlight">\(\mbox{AIC}_k\)</span>.</p>
</section>
<section id="bayesian-information-criterion">
<h4>Bayesian Information Criterion<a class="headerlink" href="#bayesian-information-criterion" title="Permalink to this heading">#</a></h4>
<p>An alternative to AIC is the Bayesian Information Criterion (BIC). <strong>The BIC also makes it possible to compare models that are either nested or not.</strong> For a model with <span class="math notranslate nohighlight">\(k\)</span> regressors, <span class="math notranslate nohighlight">\(n\)</span> observations used for training, and a deviance <span class="math notranslate nohighlight">\(D_k\)</span>; it is defined as:</p>
<div class="math notranslate nohighlight">
\[\mbox{BIC}_k = D_k + k \log (n).\]</div>
<p>Models with <strong>smaller</strong> values of <span class="math notranslate nohighlight">\(\mbox{BIC}_k\)</span> are preferred. That said, <span class="math notranslate nohighlight">\(\mbox{BIC}_k\)</span> also favours models with small values of <span class="math notranslate nohighlight">\(D_k\)</span>.</p>
<p>The differences between AIC and BIC will be more pronounced in datasets with large sample sizes <span class="math notranslate nohighlight">\(n\)</span>. As the BIC penalty of <span class="math notranslate nohighlight">\(k \log (n)\)</span> will always be larger than the AIC penalty of <span class="math notranslate nohighlight">\(2k\)</span> when <span class="math notranslate nohighlight">\(n &gt; 7\)</span>, <strong>BIC tends to select models with fewer regressors than AIC</strong>.</p>
</section>
</section>
</section>
<section id="cox-proportional-hazards-model">
<h2>Cox Proportional Hazards Model<a class="headerlink" href="#cox-proportional-hazards-model" title="Permalink to this heading">#</a></h2>
<p>It is a widely popular <strong>semiparametric</strong> regression model (namely, <strong>Cox regression</strong>). <strong>In Statistics</strong>, we call a regression model semiparametric when we only define the systematic component (i.e., regressors and regression estimates). Yet, we <strong>do not assume any specific distribution</strong> for our response of interest.</p>
<section id="id1">
<h3>Data Modelling Framework<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<p>The Cox Proportional Hazards model is a commonly used survival model that allows us to interpret how regressors influence a censored response.</p>
<p>Using a training set of size <span class="math notranslate nohighlight">\(n\)</span>, the idea is to model the <strong>hazard function</strong> <span class="math notranslate nohighlight">\(\lambda_i(t)\)</span> directly for the <span class="math notranslate nohighlight">\(i\)</span>th observation (<span class="math notranslate nohighlight">\(i = 1, \dots, n\)</span>) subject to <span class="math notranslate nohighlight">\(k\)</span> regressors <span class="math notranslate nohighlight">\(X_{i,j}\)</span> (<span class="math notranslate nohighlight">\(j = 1, \dots, k\)</span>):</p>
<div class="math notranslate nohighlight">
\[\lambda_i \left( t | X_{i,1}, \dots, X_{i,k} \right) = \lambda_0(t) \exp\left(\sum_{j = 1}^k \beta_j X_{i,j}\right).\]</div>
<p>This model has certain particularities, which will be highlighted below:</p>
<ul class="simple">
<li><p>We model the <span class="math notranslate nohighlight">\(i\)</span>th individual hazard function <span class="math notranslate nohighlight">\(\lambda_i \left( t | X_{i,1}, \dots, X_{i,k} \right)\)</span> along with a baseline hazard <span class="math notranslate nohighlight">\(\lambda_0(t)\)</span>, which is equal for all the <span class="math notranslate nohighlight">\(n\)</span> observations, multiplied by <span class="math notranslate nohighlight">\(\exp\left(\sum_{j = 1}^k \beta_j X_{i,j}\right)\)</span>.</p></li>
<li><p>The model does not have an intercept <span class="math notranslate nohighlight">\(\beta_0\)</span>. Nevertheless, the baseline hazard <span class="math notranslate nohighlight">\(\lambda_0(t)\)</span> acts as the intercept in this survival framework.</p></li>
<li><p>The baseline hazard <span class="math notranslate nohighlight">\(\lambda_0(t)\)</span> is the reference <strong>for all subjects</strong> when all covariates are equal to zero. Note this baseline hazard depends on time <span class="math notranslate nohighlight">\(t\)</span> <strong>unlike the regressors <span class="math notranslate nohighlight">\(X_{i,j}\)</span></strong>. Cox regression does not assume any distribution on this baseline hazard, which is the non-parametric part of the model.</p></li>
<li><p>The parametric part of the model is reflected in <span class="math notranslate nohighlight">\(\exp\left(\sum_{j = 1}^k \beta_j X_{i,j}\right)\)</span>.</p></li>
<li><p>Therefore, given the combinations of both situations, Cox regression is overall defined as semiparametric.</p></li>
</ul>
<p>To exemplify the <strong>proportional hazards</strong> assumption, let us consider a simple Cox regression model with a continuous regressor <span class="math notranslate nohighlight">\(X_{i, 1}\)</span>. Moreover, assume these two subjects <span class="math notranslate nohighlight">\(i = 1, 2\)</span> have the following regressor values:</p>
<div class="math notranslate nohighlight">
\[X_{2, 1} = X_{1, 1} + 1 \qquad \text{Subject 2's regressor value is one unit larger}.\]</div>
<p>Their responses can be modelled as:</p>
<div class="math notranslate nohighlight">
\[\begin{align*}
\lambda_1 \left( t | X_{1,1} \right) = \lambda_0(t)\exp\left( \beta_1 X_{1,1} \right) \qquad \text{Subject 1}
\end{align*}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
\lambda_2 \left( t | X_{2,1} \right) &amp;= \lambda_0(t)\exp\left( \beta_1 X_{2,1} \right) \\
&amp;= \lambda_0(t)\exp\left[ \beta_1 (X_{1,1} + 1) \right] \\
&amp;= \lambda_0(t)\exp\left( \beta_1 X_{1,1} \right) \exp \left( \beta_1 \right) \qquad \text{Subject 2}
\end{align*}\end{split}\]</div>
<p>Then, the hazard ratio comes into play as:</p>
<div class="math notranslate nohighlight">
\[
\frac{\lambda_2 \left( t | X_{2,1} \right)}{\lambda_1 \left( t | X_{1,1} \right)} = \frac{\lambda_0(t)\exp\left( \beta_1 X_{1,1} \right) \exp \left( \beta_1 \right)}{\lambda_0(t)\exp\left( \beta_1 X_{1,1} \right)} = \exp\left( \beta_1 \right) \quad \Rightarrow \quad \lambda_2(t) = \lambda_1(t)\exp\left( \beta_1 \right)
\]</div>
<p><strong>In plain words and generally</strong>, the proportional hazards assumption assumes that the hazard for any subject is proportional to the hazard of any other subject <strong>via the exponentiated regression coefficients</strong>.</p>
</section>
<section id="id2">
<h3>Estimation<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<p>Parameter estimation in Cox regression is done through another special maximum likelihood technique using a <strong>partial likelihood</strong>. A partial likelihood is a specific class of <strong>quasi-likelihood</strong>, which does not require assuming any specific PDF for the continuous survival times <span class="math notranslate nohighlight">\(Y_i\)</span> (thus, the baseline hazard is not required to take any distributional form).</p>
<p>However, we use the parametric part</p>
<div class="math notranslate nohighlight">
\[\exp\left(\sum_{j = 1}^k \beta_j X_{i,j}\right)\]</div>
<p>to perform our corresponding estimation.</p>
</section>
<section id="id3">
<h3>Inference<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h3>
<p>We can determine <strong>whether a regressor is statistically associated with the response’s hazard function</strong> through <strong>hypothesis testing</strong> for <span class="math notranslate nohighlight">\(\beta_j\)</span>. We will need the estimated regression coefficient <span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span> and its <strong>standard error</strong>, <span class="math notranslate nohighlight">\(\mbox{se}(\hat{\beta}_j)\)</span>.</p>
<p>You can test the below hypotheses via the <strong>Wald statistic</strong> <span class="math notranslate nohighlight">\(z _j= \frac{\hat{\beta}_j}{\mbox{se}(\hat{\beta}_j)}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
H_0: \beta_j &amp;= 0 \\
H_a: \beta_j &amp;\neq 0.        
\end{align*}\end{split}\]</div>
<p>Given a large enough training size <span class="math notranslate nohighlight">\(n\)</span>, <span class="math notranslate nohighlight">\(z_j\)</span> has an <strong>approximately Standard Normal distribution</strong> under <span class="math notranslate nohighlight">\(H_0\)</span>.</p>
<p><code class="docutils literal notranslate"><span class="pre">R</span></code> provides the corresponding <strong><span class="math notranslate nohighlight">\(p\)</span>-value</strong> for each <span class="math notranslate nohighlight">\(\beta_j\)</span>. The smaller the <span class="math notranslate nohighlight">\(p\)</span>-value, the stronger the evidence against the null hypothesis <span class="math notranslate nohighlight">\(H_0\)</span>. Hence, a small enough <span class="math notranslate nohighlight">\(p\)</span>-value (less than the significance level <span class="math notranslate nohighlight">\(\alpha\)</span>) indicates that the data provides evidence in favour of <strong>association</strong> (<strong>or causation in the case of an experimental study!</strong>) between the hazard function and the <span class="math notranslate nohighlight">\(j\)</span>th regressor. Furthermore, given a specified level of confidence, we can construct approximate <span class="math notranslate nohighlight">\((1 - \alpha) \times 100\%\)</span> CIs for the corresponding true value of <span class="math notranslate nohighlight">\(\beta_j\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_j \pm z_{\alpha/2}\mbox{se}(\hat{\beta}_j),
\]</div>
<p>where <span class="math notranslate nohighlight">\(z_{\alpha/2}\)</span> is the upper <span class="math notranslate nohighlight">\(\alpha/2\)</span> quantile of the <strong>Standard Normal distribution</strong>.</p>
</section>
<section id="prediction">
<h3>Prediction<a class="headerlink" href="#prediction" title="Permalink to this heading">#</a></h3>
<p>Even though Cox regression models the hazard function <span class="math notranslate nohighlight">\(\lambda_i(t)\)</span>, it is possible to obtain <strong>a given estimated survival function</strong> via the following equation:</p>
<div class="math notranslate nohighlight">
\[S\left( t | X_1, \dots X_k \right) = S_0(t)^{\exp\left(\sum_{j = 1}^k \beta_j X_{i,j}\right)},\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[S_0(t) = \exp \left[ - \Lambda_0(t) \right]\]</div>
<p>is the <strong>baseline survival function</strong>.</p>
<p>Note that <span class="math notranslate nohighlight">\(\Lambda_0(t)\)</span> is the cumulative baseline hazard function:</p>
<div class="math notranslate nohighlight">
\[\Lambda_0(t) = \int_0^{t} \lambda_0(u)du.\]</div>
<p>We use the baseline cumulative hazard <span class="math notranslate nohighlight">\(\Lambda_0(t)\)</span> to obtain <span class="math notranslate nohighlight">\(S_0(t)\)</span>.</p>
</section>
</section>
<section id="cumulative-hazard-function">
<h2>Cumulative Hazard Function<a class="headerlink" href="#cumulative-hazard-function" title="Permalink to this heading">#</a></h2>
<p>Let us consider a <strong>continuous</strong> random variable</p>
<div class="math notranslate nohighlight">
\[Y = \text{Time until an event occurs.}\]</div>
<p>The cumulative hazard function <span class="math notranslate nohighlight">\(\Lambda(t)\)</span> depict the accumulated amount of hazard up to time <span class="math notranslate nohighlight">\(t\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[
\Lambda(t) = \int_0^{t} \lambda(u)du.
\]</div>
<p>Furthermore, the cumulative hazard function <span class="math notranslate nohighlight">\(\Lambda(t)\)</span> and survival function <span class="math notranslate nohighlight">\(S_Y(t)\)</span> are mathematically related as follows:</p>
<div class="math notranslate nohighlight">
\[\Lambda(t) = -\log S_Y(t).\]</div>
</section>
<section id="hazard-function">
<h2>Hazard Function<a class="headerlink" href="#hazard-function" title="Permalink to this heading">#</a></h2>
<p>Let us consider a <strong>continuous</strong> random variable</p>
<div class="math notranslate nohighlight">
\[Y = \text{Time until an event occurs.}\]</div>
<p>Their survival and PDF functions are <span class="math notranslate nohighlight">\(S_Y(t)\)</span> and <span class="math notranslate nohighlight">\(f_Y(t)\)</span>. That said, the hazard function <span class="math notranslate nohighlight">\(\lambda(t)\)</span>, the <strong>instantaneous rate of event occurrence per unit of time</strong>, is given by</p>
<div class="math notranslate nohighlight">
\[
\lambda(t) = \lim_{\Delta t\rightarrow 0} \frac{P(t\leq Y &lt; t+\Delta t | Y\geq t)}{\Delta t} = \frac{f_Y(t)}{S_Y(t)}.
\]</div>
<p>One can interpret <span class="math notranslate nohighlight">\(\lambda(t)\Delta t\)</span> as the approximate probability of the event occurring immediately, given that the event has not occurred up until time <span class="math notranslate nohighlight">\(t\)</span>.</p>
</section>
<section id="link-functions-in-generalized-linear-models-glms">
<h2>Link Functions in Generalized Linear Models (GLMs)<a class="headerlink" href="#link-functions-in-generalized-linear-models-glms" title="Permalink to this heading">#</a></h2>
<p>A GLM has the components of the conceptual regression model in a training set of <span class="math notranslate nohighlight">\(n\)</span> elements as:</p>
<ul class="simple">
<li><p><strong>Random component.</strong> Each <em>response</em> <span class="math notranslate nohighlight">\(Y_1,\ldots,Y_n\)</span> is a random variable with its respective mean <span class="math notranslate nohighlight">\(\mu_i\)</span>.</p></li>
<li><p><strong>Systematic component.</strong> How the <span class="math notranslate nohighlight">\(k\)</span> regressors come into the model denoted as a <strong>linear combination</strong>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\eta_i = \beta_0 + \beta_1 X_{i,1} + \beta_2 X_{i,2} + \ldots + \beta_k X_{i,k} \; \; \; \; \text{for} \; i = 1, \ldots, n.
\]</div>
<ul class="simple">
<li><p><strong>Link function.</strong> The element that connects the <strong>random component</strong> with the <strong>systematic component</strong> <span class="math notranslate nohighlight">\(\eta_i\)</span>. The connection is made through <span class="math notranslate nohighlight">\(h(\mu_i)\)</span>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
h(\mu_i) = \eta_i.
\]</div>
<p>Note the following:</p>
<ul class="simple">
<li><p>The link function needs to be monotonic so we can allow putting the systematic component <span class="math notranslate nohighlight">\(\eta_i\)</span> in terms of the corresponding mean <span class="math notranslate nohighlight">\(\mu_i\)</span>, i.e.:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\mu_i = h^{-1}(\eta_i).\]</div>
<ul class="simple">
<li><p>Furthermore, it needs to be differentiable <strong>since we rely on maximum likelihood estimation</strong> to obtain <span class="math notranslate nohighlight">\(\hat{\beta}_0, \hat{\beta}_1, \dots, \hat{\beta}_k\)</span>.</p></li>
</ul>
</section>
<section id="multinomial-logistic-regression">
<h2>Multinomial Logistic Regression<a class="headerlink" href="#multinomial-logistic-regression" title="Permalink to this heading">#</a></h2>
<section id="id4">
<h3>Data Modelling Framework<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h3>
<p>Let us suppose that a given <strong>discrete nominal response <span class="math notranslate nohighlight">\(Y_i\)</span> has categories <span class="math notranslate nohighlight">\(1, 2, \dots, m\)</span></strong>. Categories <span class="math notranslate nohighlight">\(1, 2, \dots, m\)</span> <strong>are merely labels here</strong>. Thus, they <strong>do not</strong> implicate an ordinal scale.</p>
<p>This regression approach assumes a <a class="reference external" href="https://www.sciencedirect.com/topics/mathematics/multinomial-distribution"><strong>Multinomial distribution</strong></a> where <span class="math notranslate nohighlight">\(p_{i,1}, p_{i,2}, \dots, p_{i,m}\)</span> are the probabilities that <span class="math notranslate nohighlight">\(Y_i\)</span> will belong to categories <span class="math notranslate nohighlight">\(1, 2, \dots, m\)</span> respectively; i.e.,</p>
<div class="math notranslate nohighlight">
\[
P(Y_i = 1) = p_{i,1} \;\;\;\; P(Y_i = 2) = p_{i,2} \;\; \dots \;\; P(Y_i = m) = p_{i,m},
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\sum_{j = 1}^m p_{i,j} = p_{i,1} + p_{i,2} + \dots + p_{i,m} = 1.
\]</div>
<p>The Multinomial Logistic regression <strong>also models the logarithm of the odds</strong>. However, only one logarithm of the odds (or <strong>logit</strong>) will not be enough anymore. Recall we can capture the odds between two categories with a single logit function. <strong>What about adding some other ones?</strong></p>
<p>Here is what we can do:</p>
<ol class="arabic simple">
<li><p>Pick one of the categories to be the <strong>baseline</strong>. For example, the category “<span class="math notranslate nohighlight">\(1\)</span>”.</p></li>
<li><p>For each of the <strong>other</strong> categories, we model the logarithm of the odds to the baseline category.</p></li>
</ol>
<p><strong>What is the math for the general case with <span class="math notranslate nohighlight">\(m\)</span> response categories and <span class="math notranslate nohighlight">\(k\)</span> regressors?</strong> For the <span class="math notranslate nohighlight">\(i\)</span>th observation, we end up with a system of <span class="math notranslate nohighlight">\(m - 1\)</span> link functions in the Multinomial Logistic regression model as follows:</p>
<div class="math notranslate nohighlight" id="equation-multinomial-model-app">
<span class="eqno">(30)<a class="headerlink" href="#equation-multinomial-model-app" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{gather*}
\eta_i^{(2,1)} = \log\left[\frac{P(Y_i = 2\mid X_{i,1}, \ldots, X_{i,k})}{P(Y_i = 1 \mid X_{i,1}, \ldots, X_{i,k})}\right] = \beta_0^{(2,1)} + \beta_1^{(2,1)} X_{i, 1} + \beta_2^{(2,1)} X_{i, 2} + \ldots + \beta_k^{(2,1)} X_{i, k} \\
\eta_i^{(3,1)} = \log\left[\frac{P(Y_i = 3\mid X_{i,1}, \ldots, X_{i,k})}{P(Y_i = 1 \mid X_{i,1}, \ldots, X_{i,k})}\right] = \beta_0^{(3,1)} + \beta_1^{(3,1)} X_{i, 1} + \beta_2^{(3,1)} X_{i, 2} + \ldots + \beta_k^{(3,1)} X_{i, k} \\
\vdots \\
\eta_i^{(m,1)} = \log\left[\frac{P(Y_i = m\mid X_{i,1}, \ldots, X_{i,k})}{P(Y_i = 1 \mid X_{i,1}, \ldots, X_{i,k})}\right] = \beta_0^{(m,1)} + \beta_1^{(m,1)} X_{i, 1} + \beta_2^{(m,1)} X_{i, 2} + \ldots + \beta_k^{(m,1)} X_{i, k}.
\end{gather*}\end{split}\]</div>
<p>Note that the superscript <span class="math notranslate nohighlight">\((j, 1)\)</span> in <a class="reference internal" href="#equation-multinomial-model-app">(30)</a> indicates that the equation is on level <span class="math notranslate nohighlight">\(j\)</span> (for <span class="math notranslate nohighlight">\(j = 2, \dots, m\)</span>) with respect to level <span class="math notranslate nohighlight">\(1\)</span>. Furthermore, <strong>the regression intercept and coefficients are different for each link function</strong>.</p>
<p>With some algebraic manipulation, we can show that the probabilities <span class="math notranslate nohighlight">\(p_{i,1}, p_{i,2}, \dots, p_{i,m}\)</span> of <span class="math notranslate nohighlight">\(Y_i\)</span> belonging to categories <span class="math notranslate nohighlight">\(1, 2, \dots, m\)</span> are:</p>
<div class="math notranslate nohighlight" id="equation-prob-multinomial-app">
<span class="eqno">(31)<a class="headerlink" href="#equation-prob-multinomial-app" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{gather*}
p_{i,1} = P(Y_i = 1 \mid X_{i,1}, \ldots, X_{i,k}) = \frac{1}{1 + \sum_{j = 2}^m \exp \big( \eta_i^{(j,1)} \big)} \\
p_{i,2} = P(Y_i = 2 \mid X_{i,1}, \ldots, X_{i,k}) = \frac{\exp \big( \eta_i^{(2,1)} \big)}{1 + \sum_{j = 2}^m \exp \big( \eta_i^{(j,1)} \big)} \\
\vdots \\
p_{i,m} = P(Y_i = m \mid X_{i,1}, \ldots, X_{i,k}) = \frac{\exp \big( \eta_i^{(m,1)} \big)}{1 + \sum_{j = 2}^m \exp \big( \eta_i^{(j,1)} \big)}.
\end{gather*}\end{split}\]</div>
<p>If we sum all <span class="math notranslate nohighlight">\(m\)</span> probabilities in <a class="reference internal" href="#equation-prob-multinomial-app">(31)</a>, the sum will be equal to <span class="math notranslate nohighlight">\(1\)</span> for the <span class="math notranslate nohighlight">\(i\)</span>th observation. <strong>This is particularly important when we want to use this model for making predictions in classification matters</strong>. In a Multinomial Logistic regression model, each link function has its own intercept and regression coefficients.</p>
</section>
<section id="id5">
<h3>Estimation<a class="headerlink" href="#id5" title="Permalink to this heading">#</a></h3>
<p><strong>All parameters</strong> in the Multinomial Logistic regression model are also unknown. To fit the model with the package <code class="docutils literal notranslate"><span class="pre">nnet</span></code>, we use the function <code class="docutils literal notranslate"><span class="pre">multinom()</span></code>, which obtains the corresponding estimates. The estimates are obtained through <strong>maximum likelihood</strong>, where we assume a <strong>Multinomial joint probability mass function</strong> of the <span class="math notranslate nohighlight">\(n\)</span> responses <span class="math notranslate nohighlight">\(Y_i\)</span>. You can find more information on this matter <a class="reference external" href="https://czep.net/stat/mlelr.pdf"><strong>here</strong></a>.</p>
</section>
<section id="id6">
<h3>Inference<a class="headerlink" href="#id6" title="Permalink to this heading">#</a></h3>
<p><strong>We can determine whether a regressor is statistically associated with the logarithm of the odds</strong> through hypothesis testing for the parameters <span class="math notranslate nohighlight">\(\beta_j^{(u, v)}\)</span> <strong>by link function</strong>. We also use the <strong>Wald statistic</strong> <span class="math notranslate nohighlight">\(z_j^{(u, v)}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{equation*}
z_j^{(u, v)} = \frac{\hat{\beta}_j^{(u, v)}}{\mbox{se}\left(\hat{\beta}_j^{(u, v)}\right)}
\end{equation*}\]</div>
<p>to test the hypotheses</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{gather*}
H_0: \beta_j^{(u, v)} = 0\\
H_a: \beta_j^{(u, v)} \neq 0.
\end{gather*}\end{split}\]</div>
<p>Provided the sample size <span class="math notranslate nohighlight">\(n\)</span> is large enough, <span class="math notranslate nohighlight">\(z_j\)</span> has an <strong>approximately Standard Normal distribution</strong> under <span class="math notranslate nohighlight">\(H_0\)</span>.</p>
<p><code class="docutils literal notranslate"><span class="pre">R</span></code> provides the corresponding <strong><span class="math notranslate nohighlight">\(p\)</span>-values</strong> for each <span class="math notranslate nohighlight">\(\beta_j^{(u, v)}\)</span>. The smaller the <span class="math notranslate nohighlight">\(p\)</span>-value, the stronger the evidence against the null hypothesis <span class="math notranslate nohighlight">\(H_0\)</span>. As in the previous regression models, we would set a predetermined significance level <span class="math notranslate nohighlight">\(\alpha\)</span> (usually taken to be 0.05) to infer if the <span class="math notranslate nohighlight">\(p\)</span>-value is small enough. If the <span class="math notranslate nohighlight">\(p\)</span>-value is smaller than the predetermined level <span class="math notranslate nohighlight">\(\alpha\)</span>, then you could claim that there is evidence to reject the null hypothesis. Hence, <span class="math notranslate nohighlight">\(p\)</span>-values that are small enough indicate that the data provides evidence in favour of <strong>association</strong> (<strong>or causation in the case of an experimental study!</strong>) between the response variable and the <span class="math notranslate nohighlight">\(j\)</span>th regressor.</p>
<p>Furthermore, given a specified level of confidence where <span class="math notranslate nohighlight">\(\alpha\)</span> is the significance level, we can construct approximate <span class="math notranslate nohighlight">\((1 - \alpha) \times 100\%\)</span> <strong>confidence intervals</strong> for the corresponding true value of <span class="math notranslate nohighlight">\(\beta_j^{(u, v)}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{equation*}
\hat{\beta}_j^{(u, v)} \pm z_{\alpha/2}\mbox{se} \left( \hat{\beta}_j^{(u, v)} \right),
\end{equation*}\]</div>
<p>where <span class="math notranslate nohighlight">\(z_{\alpha/2}\)</span> is the upper <span class="math notranslate nohighlight">\(\alpha/2\)</span> quantile of the <strong>Standard Normal distribution</strong>.</p>
</section>
<section id="id7">
<h3>Model Selection<a class="headerlink" href="#id7" title="Permalink to this heading">#</a></h3>
<p><strong>To perform model selection</strong>, we can use the same techniques from the Binary Logistic regression model (check <a class="reference internal" href="#bin-log-model-selection-app"><span class="std std-ref">Model Selection</span></a>).</p>
</section>
</section>
<section id="negative-binomial-regression">
<h2>Negative Binomial Regression<a class="headerlink" href="#negative-binomial-regression" title="Permalink to this heading">#</a></h2>
<p>Let</p>
<div class="math notranslate nohighlight">
\[Y_i \sim \text{Negative Binomial} (m, p_i) \quad \text{for} \quad i = 1, \dots, n.\]</div>
<p>From <strong>DSCI 551</strong>, recall that a Negative Binomial distribution has the following probability mass function (PMF):</p>
<div class="math notranslate nohighlight" id="equation-nb-pdf-app">
<span class="eqno">(32)<a class="headerlink" href="#equation-nb-pdf-app" title="Permalink to this equation">#</a></span>\[\begin{equation}
P(Y_i = y_i \mid m, p_i) = {m - 1 + y_i \choose y_i} p_i^{m} (1 - p_i)^{y_i} \quad \text{for} \quad y_i = 0, 1, \dots
\end{equation}\]</div>
<p>A Negative Binomial random variable depicts <strong>the number of <span class="math notranslate nohighlight">\(y_i\)</span> failed independent Bernoulli trials before experiencing <span class="math notranslate nohighlight">\(m\)</span> successes</strong> with a probability of success <span class="math notranslate nohighlight">\(p_i\)</span>.</p>
<p>This distribution has the following mean and variance:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{gather*}
\mathbb{E}(Y_i) = \frac{m(1 - p_i)}{p_i} \\
\text{Var}(Y_i) = \frac{m(1 - p_i)}{p_i^2}.
\end{gather*}\end{split}\]</div>
<section id="reparametrization">
<h3>Reparametrization<a class="headerlink" href="#reparametrization" title="Permalink to this heading">#</a></h3>
<p>Under the following parametrization:</p>
<div class="math notranslate nohighlight" id="equation-nb-param-app">
<span class="eqno">(33)<a class="headerlink" href="#equation-nb-param-app" title="Permalink to this equation">#</a></span>\[\begin{equation}
\lambda_i = \frac{m (1 - p_i)}{p_i} \qquad \Rightarrow \qquad p_i = \frac{m}{m + \lambda_i},
\end{equation}\]</div>
<p>the mean and variance of a Negative Binomial random variable can be reexpressed as</p>
<div class="math notranslate nohighlight" id="equation-nb-mean-variance-app">
<span class="eqno">(34)<a class="headerlink" href="#equation-nb-mean-variance-app" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{gather}
\mathbb{E}(Y_i) = \lambda_i \\
\text{Var}(Y_i) = \lambda_i \left( 1 + \frac{\lambda_i}{m} \right).
\end{gather}\end{split}\]</div>
<p>This reparametrized variance indicates that a Negative Binomial random variable allows for overdispersion through factor <span class="math notranslate nohighlight">\(\left( 1 + \frac{\lambda_i}{m} \right)\)</span>.</p>
<p>Finally, by applying parametrization <a class="reference internal" href="#equation-nb-param-app">(33)</a> in PMF <a class="reference internal" href="#equation-nb-pdf-app">(32)</a>, we have the following:</p>
<div class="math notranslate nohighlight" id="equation-nb-alt-pdf-app">
<span class="eqno">(35)<a class="headerlink" href="#equation-nb-alt-pdf-app" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{align}
P(Y_i = y_i \mid m, p_i) &amp;= {m - 1 + y_i \choose y_i} p_i^{m} (1 - p_i)^{y_i} \\
&amp;= \frac{(m + y_i - 1)!}{y_i! (m - 1 )!} \left( \frac{m}{m + \lambda} \right)^{m} \left( 1 - \frac{m}{m + \lambda} \right)^{y_i} \\
&amp;= \frac{\Gamma(y_i + m)}{\Gamma(y_i + 1) \Gamma(m)} \left( \frac{m}{m + \lambda} \right)^{m} \left( 1 - \frac{m}{m + \lambda} \right)^{y_i},
\end{align}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\Gamma(\cdot)\)</span> is the <a class="reference external" href="https://www.statlect.com/mathematical-tools/gamma-function"><strong>Gamma function</strong></a>. We actually use the property</p>
<div class="math notranslate nohighlight">
\[\Gamma(a) = (a - 1)!,\]</div>
<p>where <span class="math notranslate nohighlight">\(a \geq 1\)</span> is an integer.</p>
</section>
<section id="id8">
<h3>Data Modelling Framework<a class="headerlink" href="#id8" title="Permalink to this heading">#</a></h3>
<p>As in the case of Poisson regression with <span class="math notranslate nohighlight">\(k\)</span> regressors, the Negative Binomial case is a GLM with the following link function:</p>
<div class="math notranslate nohighlight" id="equation-nb-model-app">
<span class="eqno">(36)<a class="headerlink" href="#equation-nb-model-app" title="Permalink to this equation">#</a></span>\[\begin{equation}
h(\lambda_i) = \log(\lambda_i) = \beta_0 + \beta_1 X_{i,1} + \dots + \beta_k X_{i,k}.
\end{equation}\]</div>
<p>Lastly, note the following:</p>
<ul class="simple">
<li><p>From <a class="reference internal" href="#equation-nb-mean-variance-app">(34)</a>, let <span class="math notranslate nohighlight">\(\theta = \frac{1}{m}\)</span>. Then, Negative Binomial regression will assume the following variance:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
\text{Var}(Y_i) &amp;= \lambda_i \left( 1 + \frac{\lambda_i}{m} \right) \\
&amp;= \lambda_i + \frac{\lambda_i^2}{m} \\
&amp;= \lambda_i + \theta \lambda_i^2.
\end{align*}\end{split}\]</div>
<p>Therefore, the model has even more flexibility to deal with overdispersion compared to Poisson regression.</p>
</section>
<section id="id9">
<h3>Estimation<a class="headerlink" href="#id9" title="Permalink to this heading">#</a></h3>
<p>Via a training set of size <span class="math notranslate nohighlight">\(n\)</span> whose responses are <strong>independent counts</strong> <span class="math notranslate nohighlight">\(Y_i\)</span> (<span class="math notranslate nohighlight">\(i = 1, \dots, n\)</span>), we use the reparametrized PMF <a class="reference internal" href="#equation-nb-alt-pdf-app">(35)</a> along with the link function <a class="reference internal" href="#equation-nb-model-app">(36)</a> via <strong>maximum likelihood estimation</strong> to obtain <span class="math notranslate nohighlight">\(\hat{\beta}_0, \hat{\beta}_1, \dots, \hat{\beta}_k, \hat{\theta}\)</span>.</p>
<p>To fit a Negative Binomial regression via <code class="docutils literal notranslate"><span class="pre">R</span></code>, we can use the function <code class="docutils literal notranslate"><span class="pre">glm.nb()</span></code> from package <code class="docutils literal notranslate"><span class="pre">MASS</span></code>.</p>
</section>
<section id="inference-coefficient-interpretation-prediction-goodness-of-fit-and-model-selection">
<h3>Inference, Coefficient Interpretation, Prediction, Goodness of Fit, and Model Selection<a class="headerlink" href="#inference-coefficient-interpretation-prediction-goodness-of-fit-and-model-selection" title="Permalink to this heading">#</a></h3>
<p>Since the link function in Negative Binomial regression is the same as in Poisson regression; <strong>inference, coefficient interpretation, and prediction are performed similarly</strong> (even with the same <code class="docutils literal notranslate"><span class="pre">broom</span></code> functions!). Regarding <strong>model selection</strong>, since we use a regular maximum likelihood approach to estimate the regression parameters, we can use analysis of deviance, AIC, and BIC to perform model selection and/or goodness of fit testing.</p>
</section>
</section>
<section id="ordinal-logistic-regression">
<h2>Ordinal Logistic Regression<a class="headerlink" href="#ordinal-logistic-regression" title="Permalink to this heading">#</a></h2>
<section id="id10">
<h3>Data Modelling Framework<a class="headerlink" href="#id10" title="Permalink to this heading">#</a></h3>
<p>Let us suppose that a given <strong>discrete ordinal response</strong> <span class="math notranslate nohighlight">\(Y_i\)</span> (for <span class="math notranslate nohighlight">\(i = 1, \dots, n\)</span>) has categories <span class="math notranslate nohighlight">\(1, 2, \dots, m\)</span> in a training set of size <span class="math notranslate nohighlight">\(n\)</span>.</p>
<p>Categories <span class="math notranslate nohighlight">\(1, 2, \dots, m\)</span> <strong>implicate an ordinal scale here</strong>, i.e., <span class="math notranslate nohighlight">\(1 &lt; 2 &lt; \dots &lt; m\)</span>.</p>
<p>Also, note there is more than one class of Ordinal Logistic regression. We will review the <strong>proportional odds</strong> model (a <strong>cumulative logit model</strong>).</p>
<p>We have to point out that Ordinal Logistic regression will indicate how each one of the <span class="math notranslate nohighlight">\(k\)</span> regressors <span class="math notranslate nohighlight">\(X_{i,1}, \dots, X_{i,k}\)</span> affects the <strong>cumulative logarithm of the odds</strong> in the ordinal response  for the following <span class="math notranslate nohighlight">\(m - 1\)</span> situations:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{gather*}
\text{Level } m - 1 \text{ or any lesser degree versus level } m\\
\text{Level } m - 2 \text{ or any lesser degree versus level } m - 1 \text{ or any higher degree}\\
\vdots \\
\text{Level } 2 \text{ or any lesser degree versus level } 3 \text{ or any higher degree}\\
\text{Level } 1 \text{ versus level } 2 \text{ or any higher degree}\\
\end{gather*}\end{split}\]</div>
<p>These <span class="math notranslate nohighlight">\(m - 1\)</span> situations are translated into cumulative probabilities using the logarithms of the odds on the left-hand side (<span class="math notranslate nohighlight">\(m - 1\)</span> link functions) subject to the linear combination of the <span class="math notranslate nohighlight">\(k\)</span> regressors <span class="math notranslate nohighlight">\(X_{i,j}\)</span> (for <span class="math notranslate nohighlight">\(j = 1, \dots, k\)</span>):</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{gather*}
\eta_i^{(m - 1)} = \log\left[\frac{P(Y_i \leq m - 1 \mid X_{i,1}, \ldots, X_{i,k})}{P(Y_i = m \mid X_{i,1}, \ldots, X_{i,k})}\right] = \beta_0^{(m - 1)} - \beta_1 X_{i, 1} - \beta_2 X_{i, 2} - \ldots - \beta_k X_{i, k} \\
\eta_i^{(m - 2)} = \log\left[\frac{P(Y_i \leq m - 2 \mid X_{i,1}, \ldots, X_{i,k})}{P(Y_i &gt; m - 2 \mid X_{i,1}, \ldots, X_{i,k})}\right] = \beta_0^{(m - 2)} - \beta_1 X_{i, 1} - \beta_2 X_{i, 2} - \ldots - \beta_k X_{i, k} \\
\vdots \\
\eta_i^{(2)} = \log\left[\frac{P(Y_i \leq 2 \mid X_{i,1}, \ldots, X_{i,k})}{P(Y_i &gt; 2 \mid X_{i,1}, \ldots, X_{i,k})}\right] = \beta_0^{(2)} - \beta_1 X_{i, 1} - \beta_2 X_{i, 2} - \ldots - \beta_k X_{i, k} \\
\eta_i^{(1)} = \log\left[\frac{P(Y_i = 1 \mid X_{i,1}, \ldots, X_{i,k})}{P(Y_i &gt; 1 \mid X_{i,1}, \ldots, X_{i,k})}\right] = \beta_0^{(1)} - \beta_1 X_{i, 1} - \beta_2 X_{i, 2} - \ldots - \beta_k X_{i, k}.
\end{gather*}\end{split}\]</div>
<p>Note that the system above has <span class="math notranslate nohighlight">\(m - 1\)</span> intercepts but <strong>only <span class="math notranslate nohighlight">\(k\)</span> regression coefficients</strong>. In general, the previous <span class="math notranslate nohighlight">\(m - 1\)</span> equations can be generalized for levels <span class="math notranslate nohighlight">\(j = m - 1, \dots, 1\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{gather*}
\eta_i^{(j)} = \log\left[\frac{P(Y_i \leq j \mid X_{i,1}, \ldots, X_{i,k})}{P(Y_i &gt; j \mid X_{i,1}, \ldots, X_{i,k})}\right] = \beta_0^{(j)} - \beta_1 X_{i, 1} - \beta_2 X_{i, 2} - \ldots - \beta_k X_{i, k} \\
\; \; \; \; \; \; \; \; \Rightarrow \; P(Y_i \leq j \mid X_{i,1}, \ldots, X_{i,k}) = \frac{\exp\left(\beta_0^{(j)} - \beta_1 X_{i, 1} - \beta_2 X_{i, 2} - \ldots - \beta_k X_{i, k}\right)}{1 + \exp\left(\beta_0^{(j)} - \beta_1 X_{i, 1} - \beta_2 X_{i, 2} - \ldots - \beta_k X_{i, k}\right)}.
\end{gather*}\end{split}\]</div>
<p>The probability that <span class="math notranslate nohighlight">\(Y_i\)</span> will fall in the category <span class="math notranslate nohighlight">\(j\)</span> can be computed as follows:</p>
<div class="math notranslate nohighlight">
\[
p_{i,j} = P(Y_i = j \mid X_{i,1}, \ldots, X_{i,k}) = P(Y_i \leq j \mid X_{i,1}, \ldots, X_{i,k}) - P(Y_i \leq j - 1 \mid X_{i,1}, \ldots, X_{i,k}),
\]</div>
<p>which leads to</p>
<div class="math notranslate nohighlight">
\[
P(Y_i = 1) = p_{i,1} \;\;\;\; P(Y_i = 2) = p_{i,2} \;\; \dots \;\; P(Y_i = m) = p_{i,m}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\sum_{j = 1}^m p_{i,j} = p_{i,1} + p_{i,2} + \dots + p_{i,m} = 1.
\]</div>
</section>
<section id="id11">
<h3>Estimation<a class="headerlink" href="#id11" title="Permalink to this heading">#</a></h3>
<p><strong>All parameters</strong> in the Ordinal Logistic regression model are also unknown. Therefore, model estimates are obtained through <strong>maximum likelihood</strong>, where we also assume a <strong>Multinomial joint probability mass function</strong> of the <span class="math notranslate nohighlight">\(n\)</span> responses <span class="math notranslate nohighlight">\(Y_i\)</span>. Moreover, this Multinomial assumption plays around with cumulative probabilities in the joint likelihood function <strong>as discussed in <a class="reference external" href="https://ebookcentral.proquest.com/lib/ubc/detail.action?docID=1168529">Agresti (2013)</a> in Chapter 8 (Section 8.2.2)</strong>.</p>
<p>To fit the model with the package <code class="docutils literal notranslate"><span class="pre">MASS</span></code>, we use the function <code class="docutils literal notranslate"><span class="pre">polr()</span></code>, which obtains the corresponding estimates. The argument <code class="docutils literal notranslate"><span class="pre">Hess</span> <span class="pre">=</span> <span class="pre">TRUE</span></code> is required to compute the <a class="reference external" href="https://mathworld.wolfram.com/Hessian.html">Hessian matrix</a> of the <strong>log-likelihood function</strong>, which is used to obtain the standard errors of the estimates.</p>
</section>
<section id="id12">
<h3>Inference<a class="headerlink" href="#id12" title="Permalink to this heading">#</a></h3>
<p><strong>We can determine whether a regressor is statistically associated with the logarithm of the cumulative odds</strong> through hypothesis testing for the parameters <span class="math notranslate nohighlight">\(\beta_j\)</span>. We also use the <strong>Wald statistic</strong> <span class="math notranslate nohighlight">\(z_j\)</span>:</p>
<div class="math notranslate nohighlight">
\[
z_j = \frac{\hat{\beta}_j}{\mbox{se}(\hat{\beta}_j)}
\]</div>
<p>to test the hypotheses</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{gather*}
H_0: \beta_j = 0\\
H_a: \beta_j \neq 0.
\end{gather*}\end{split}\]</div>
<p>The <strong>null hypothesis</strong> <span class="math notranslate nohighlight">\(H_0\)</span> indicates that the <span class="math notranslate nohighlight">\(j\)</span>th regressor associated to <span class="math notranslate nohighlight">\(\beta_j\)</span> does not affect the response variable in the model, and the <strong>alternative hypothesis</strong> <span class="math notranslate nohighlight">\(H_a\)</span> otherwise. Moreover, provided the sample size <span class="math notranslate nohighlight">\(n\)</span> is large enough, <span class="math notranslate nohighlight">\(z_j\)</span> has an <strong>approximately Standard Normal distribution</strong> under <span class="math notranslate nohighlight">\(H_0\)</span>.</p>
<p><code class="docutils literal notranslate"><span class="pre">R</span></code> provides the corresponding <strong><span class="math notranslate nohighlight">\(p\)</span>-values</strong> for each <span class="math notranslate nohighlight">\(\beta_j\)</span>. The smaller the <span class="math notranslate nohighlight">\(p\)</span>-value, the stronger the evidence against the null hypothesis <span class="math notranslate nohighlight">\(H_0\)</span>. As in the previous regression models, we would set a predetermined significance level <span class="math notranslate nohighlight">\(\alpha\)</span> (usually taken to be 0.05) to infer if the <span class="math notranslate nohighlight">\(p\)</span>-value is small enough. If the <span class="math notranslate nohighlight">\(p\)</span>-value is smaller than the predetermined level <span class="math notranslate nohighlight">\(\alpha\)</span>, then you could claim that there is evidence to reject the null hypothesis. Hence, <span class="math notranslate nohighlight">\(p\)</span>-values that are small enough indicate that the data provides evidence in favour of <strong>association</strong> (<strong>or causation in the case of an experimental study!</strong>) between the response variable and the <span class="math notranslate nohighlight">\(j\)</span>th regressor.</p>
<p>Furthermore, given a specified level of confidence where <span class="math notranslate nohighlight">\(\alpha\)</span> is the significance level, we can construct approximate <span class="math notranslate nohighlight">\((1 - \alpha) \times 100\%\)</span> <strong>confidence intervals</strong> for the corresponding true value of <span class="math notranslate nohighlight">\(\beta_j\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_j \pm z_{\alpha/2}\mbox{se}(\hat{\beta}_j),
\]</div>
<p>where <span class="math notranslate nohighlight">\(z_{\alpha/2}\)</span> is the upper <span class="math notranslate nohighlight">\(\alpha/2\)</span> quantile of the <strong>Standard Normal distribution</strong>.</p>
</section>
<section id="id13">
<h3>Model Selection<a class="headerlink" href="#id13" title="Permalink to this heading">#</a></h3>
<p><strong>To perform model selection</strong>, we can use the same techniques from the Binary Logistic regression model (check <a class="reference internal" href="#bin-log-model-selection-app"><span class="std std-ref">Model Selection</span></a>).</p>
</section>
<section id="non-proportional-odds-model">
<h3>Non-proportional Odds Model<a class="headerlink" href="#non-proportional-odds-model" title="Permalink to this heading">#</a></h3>
<p>We might wonder</p>
<blockquote>
<div><p>What happens if we do not fulfil the proportional odds assumption in our Ordinal Logistic regression model?</p>
</div></blockquote>
<section id="the-brant-wald-test">
<h4>The Brant-Wald Test<a class="headerlink" href="#the-brant-wald-test" title="Permalink to this heading">#</a></h4>
<p>It is essential to remember that the Ordinal Logistic model under the proportional odds assumption is <strong>the first step</strong> when performing Regression Analysis on an ordinal response.</p>
<p>Once this model has been fitted, <strong>it is possible to assess whether it fulfils this strong assumption statistically</strong>.</p>
<p>We can do it via the Brant-Wald test:</p>
<ul class="simple">
<li><p>This tool statistically assesses whether our model globally fulfils this assumption. It has the following hypotheses:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{gather*}
H_0: \text{Our Ordinal Logistic regression model globally fulfils the proportional odds assumption.} \\
H_a: \text{Otherwise}.
\end{gather*}\end{split}\]</div>
<ul class="simple">
<li><p>Moreover, it also performs further hypothesis testing on each regressor. With <span class="math notranslate nohighlight">\(k\)</span> regressors for <span class="math notranslate nohighlight">\(j = 1, \dots, k\)</span>; we have the following hypotheses:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{gather*}
H_0: \text{The } j \text{th regressor in our Ordinal Logistic regression model fulfils the proportional odds assumption.} \\
H_a: \text{Otherwise}.
\end{gather*}\end{split}\]</div>
<p>Function <code class="docutils literal notranslate"><span class="pre">brant()</span></code> from package <code class="docutils literal notranslate"><span class="pre">brant</span></code> implements this tool, which can be used in our <code class="docutils literal notranslate"><span class="pre">polr()</span></code> object.</p>
<p>The Brant Wald test essentially compares this basic Ordinal Logistic regression model of <span class="math notranslate nohighlight">\(k-1\)</span> cumulative logit functions versus a combination of <span class="math notranslate nohighlight">\(k − 1\)</span> correlated Binary Logistic regressions.</p>
</section>
</section>
</section>
<section id="ordinary-least-squares-regression">
<h2>Ordinary Least-squares Regression<a class="headerlink" href="#ordinary-least-squares-regression" title="Permalink to this heading">#</a></h2>
<section id="id14">
<h3>Data Modelling Framework<a class="headerlink" href="#id14" title="Permalink to this heading">#</a></h3>
<p>Conceptually, the OLS regression model can be expressed as:</p>
<div class="math notranslate nohighlight" id="equation-eq-conceptual-model-app">
<span class="eqno">(37)<a class="headerlink" href="#equation-eq-conceptual-model-app" title="Permalink to this equation">#</a></span>\[
\mbox{Response} = \mbox{Systematic Component} + \mbox{Random Component}.
\]</div>
<p>For the <span class="math notranslate nohighlight">\(i\)</span>th observation in our <strong>random sample</strong> or <strong>training data</strong> (<span class="math notranslate nohighlight">\(i = 1, \dots, n\)</span>), the conceptual model <a class="reference internal" href="#equation-eq-conceptual-model-app">(37)</a> is mathematically represented as:</p>
<div class="math notranslate nohighlight" id="equation-eq-ols-model-app">
<span class="eqno">(38)<a class="headerlink" href="#equation-eq-ols-model-app" title="Permalink to this equation">#</a></span>\[
\underbrace{Y_i}_\text{Response}  = \underbrace{\beta_0 + \beta_1 g_1(X_{i, 1}) + \ldots + \beta_k g_k(X_{i,k})}_\text{Systematic Component} + \underbrace{\varepsilon_i.}_\text{Random Component}
\]</div>
<p>Note the following:</p>
<ul class="simple">
<li><p>The response <span class="math notranslate nohighlight">\(Y_i\)</span> is equal to the sum of <span class="math notranslate nohighlight">\(k + 2\)</span> terms on the right-hand side.</p></li>
<li><p>The systematic component is the sum of:</p>
<ul>
<li><p>An <strong>unknown intercept</strong> <span class="math notranslate nohighlight">\(\beta_0\)</span> and</p></li>
<li><p><span class="math notranslate nohighlight">\(k\)</span> <strong>regressor functions</strong> <span class="math notranslate nohighlight">\(g_j(X_{i,j})\)</span> multiplied by their respective <strong>unknown regression coefficient</strong> <span class="math notranslate nohighlight">\(\beta_j\)</span> (<span class="math notranslate nohighlight">\(j = 1, \dots, k\)</span>).</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\varepsilon_i\)</span> is the <strong>random noise</strong>.</p></li>
</ul>
</section>
<section id="modelling-assumptions">
<h3>Modelling Assumptions<a class="headerlink" href="#modelling-assumptions" title="Permalink to this heading">#</a></h3>
<p>For the <strong>random component</strong> in <a class="reference internal" href="#equation-eq-ols-model-app">(38)</a>, we assume the following:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{gather*}
\mathbb{E}(\varepsilon_i) = 0 \\
\text{Var}(\varepsilon_i) = \sigma^2 \\
\varepsilon_i \sim \mathcal{N}(0, \sigma^2) \\
\varepsilon_i \perp \!\!\! \perp \varepsilon_k \; \; \; \; \text{for} \; i \neq k  \; \; \; \; \text{(independence)}.
\end{gather*}\end{split}\]</div>
<p>Hence, <strong>each <span class="math notranslate nohighlight">\(Y_i\)</span> is also assumed to be independent and normally distributed</strong>:</p>
<div class="math notranslate nohighlight">
\[
Y_i \mid X_{i, j} \sim \mathcal{N} \big( \beta_0 + \beta_1 g_1(X_{i, 1}) + \ldots + \beta_k g_k(X_{i,k}), \sigma^2 \big).
\]</div>
</section>
<section id="definition-of-linearity">
<h3>Definition of Linearity<a class="headerlink" href="#definition-of-linearity" title="Permalink to this heading">#</a></h3>
<p>The classical OLS model, from <strong>DSCI 561</strong>, implicates the identity function <span class="math notranslate nohighlight">\(g_j(X_{i, j}) = X_{i, j}\)</span> in equation
<a class="reference internal" href="#equation-eq-ols-model-app">(38)</a>. This leads to:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
Y_i &amp;= \beta_0 + \beta_1 g_1(X_{i, 1}) + \ldots + \beta_k g_k(X_{i,k}) + \varepsilon_i \\
&amp;= \beta_0 + \beta_1 X_{i,1} + \ldots + \beta_k X_{i,k} + \varepsilon_i.
\end{align*}\end{split}\]</div>
<p>Note the model is “linear” on the parameters (i.e., regression terms), not the regressors.</p>
</section>
<section id="id15">
<h3>Inference<a class="headerlink" href="#id15" title="Permalink to this heading">#</a></h3>
<p>In terms of inference, we use the fitted model to identify the relationship between the response and regressors. We will need the <span class="math notranslate nohighlight">\(j\)</span>th estimated regression coefficient <span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span> and its corresponding variability which is reflected in the <strong>standard error</strong> of the estimate, <span class="math notranslate nohighlight">\(\mbox{se} \left( \hat{\beta}_j \right)\)</span>. To determine the statistical significance of <span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span>, we use the <strong>test statistic</strong></p>
<div class="math notranslate nohighlight">
\[t_j = \frac{\hat{\beta}_j}{\mbox{se} \left( \hat{\beta}_j \right)}\]</div>
<p>to test the hypotheses</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{gather*}
H_0: \beta_j = 0 \\
H_a: \beta_j \neq 0.
\end{gather*}\end{split}\]</div>
<p>A statistic like <span class="math notranslate nohighlight">\(t_j\)</span> is referred to as a <span class="math notranslate nohighlight">\(t\)</span>-value. It has a <span class="math notranslate nohighlight">\(t\)</span>-distribution <strong>under the null hypothesis</strong> <span class="math notranslate nohighlight">\(H_0\)</span> with <span class="math notranslate nohighlight">\(n - k - 1\)</span> degrees of freedom.</p>
<p>We can obtain the corresponding <span class="math notranslate nohighlight">\(p\)</span>-values for each <span class="math notranslate nohighlight">\(\beta_j\)</span> associated to the <span class="math notranslate nohighlight">\(t\)</span>-values under the null hypothesis <span class="math notranslate nohighlight">\(H_0\)</span>. <strong>The smaller the <span class="math notranslate nohighlight">\(p\)</span>-value, the stronger the evidence against the null hypothesis <span class="math notranslate nohighlight">\(H_0\)</span> in our sample</strong>. Hence, small <span class="math notranslate nohighlight">\(p\)</span>-values (less than the significance level <span class="math notranslate nohighlight">\(\alpha\)</span>) indicate that the data provides evidence in favour of association (or <strong>causation</strong> if that is the case) between the response variable and the <span class="math notranslate nohighlight">\(j\)</span>th regressor.</p>
<p>Similarly, given a specified <span class="math notranslate nohighlight">\((1-\alpha) \times 100\%\)</span> level of confidence, we can construct <strong>confidence intervals</strong> for the corresponding true value of <span class="math notranslate nohighlight">\(\beta_j\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_j \pm t_{\alpha/2, n - k - 1}\mbox{se} \left( \hat{\beta}_j \right),
\]</div>
<p>where <span class="math notranslate nohighlight">\(t_{\alpha/2, n - k - 1}\)</span> is the upper <span class="math notranslate nohighlight">\(\alpha/2\)</span> quantile of the <span class="math notranslate nohighlight">\(t\)</span>-distribution with <span class="math notranslate nohighlight">\(n - k - 1\)</span> degrees of freedom.</p>
</section>
</section>
<section id="poisson-regression">
<h2>Poisson Regression<a class="headerlink" href="#poisson-regression" title="Permalink to this heading">#</a></h2>
<section id="id16">
<h3>Data Modelling Framework<a class="headerlink" href="#id16" title="Permalink to this heading">#</a></h3>
<p>Besides OLS and Binary Logistic regressions, another alternative is count data modelling, as in <strong>Poisson regression</strong>. Unlike Binary Logistic regression, <strong>we use counts as a response variable</strong>. Hence, we have to modify the modelling framework to consider this fact. Poisson regression would be the primary resource when it comes to modelling counts. Note this model also fits into the GLM class.</p>
<blockquote>
<div><p>What is the distributional key difference between the Poisson and the OLS regression models in terms of the response?</p>
</div></blockquote>
<p>First of all, we have to specify what a Poisson random variable is. Recall <a class="reference external" href="https://ubc-mds.github.io/DSCI_551_stat-prob-dsci/notes/appendix-dist-cheatsheet.html#poisson"><strong>DSCI 551</strong></a>, a Poisson random variable refers to discrete data with non-negative integer values that count something. <strong>These counts could happen during a given timeframe or even a space such as a geographic unit!</strong></p>
<p>The Poisson regression model assumes a random sample of <span class="math notranslate nohighlight">\(n\)</span> count observations <span class="math notranslate nohighlight">\(Y_i\)</span>s, hence <strong>independent</strong> (<strong>but not identically distributed!</strong>), which have the following distribution:</p>
<div class="math notranslate nohighlight">
\[Y_i \sim \text{Poisson}(\lambda_i).\]</div>
<p>Each <span class="math notranslate nohighlight">\(i\)</span>th observation has its own <span class="math notranslate nohighlight">\(\mathbb{E}(Y_i) = \lambda_i &gt; 0\)</span>, which also implicates <span class="math notranslate nohighlight">\(\text{Var}(Y_i) = \lambda_i &gt; 0\)</span>.</p>
<p>The equality of the expected value and variance in a random variable is called <strong>equidispersion</strong>. Parameter <span class="math notranslate nohighlight">\(\lambda_i\)</span> is the risk of an event occurrence, coming from the definition of the Poisson random variable, <strong>in a given timeframe or even a space</strong>. Furthermore, we have to highlight another particularity in the Poisson distribution: <strong><span class="math notranslate nohighlight">\(\lambda_i\)</span> is a continuous distributional parameter!</strong></p>
<p>Let us generalize the Poisson regression model with <span class="math notranslate nohighlight">\(k\)</span> regressors as:</p>
<div class="math notranslate nohighlight" id="equation-poisson-model-app">
<span class="eqno">(39)<a class="headerlink" href="#equation-poisson-model-app" title="Permalink to this equation">#</a></span>\[\begin{equation*}
h(\lambda_i) = \log(\lambda_i) = \beta_0 + \beta_1 X_{i,1} + \dots + \beta_k X_{i,k}.
\end{equation*}\]</div>
<p>In model <a class="reference internal" href="#equation-poisson-model-app">(39)</a>, each one of the <span class="math notranslate nohighlight">\(k\)</span> regression coefficients <span class="math notranslate nohighlight">\(\beta_{1}, \dots, \beta_{k}\)</span> represents <strong>the expected change in the natural logarithm of the mean <span class="math notranslate nohighlight">\(\lambda_i\)</span> per unit change in their respective regressors <span class="math notranslate nohighlight">\(X_{i,1}, \dots, X_{i,k}\)</span></strong>. Nonetheless, we could make more sense in the interpretation by exponentiating <a class="reference internal" href="#equation-poisson-model-app">(39)</a>:</p>
<div class="math notranslate nohighlight">
\[
\lambda_i = \exp{(\beta_0 + \beta_1 X_{i,1} + \dots + \beta_k X_{i,k})},
\]</div>
<p>where an increase in one unit in any of the <span class="math notranslate nohighlight">\(k\)</span> regressors (<strong>while keeping the rest of them constant</strong>) <strong>multiplies the mean <span class="math notranslate nohighlight">\(\lambda_i\)</span> by a factor <span class="math notranslate nohighlight">\(\exp{(\beta_j)}\)</span>, for all <span class="math notranslate nohighlight">\(j = 1, \dots, k\)</span></strong>.</p>
<p>As a side note, we have to clarify that the <strong>systematic component</strong> in the Poisson regression model is explicitly depicted by the regressors and their coefficients as in multiple linear regression. The <strong>random component</strong> is implicitly contained in each random variable</p>
<div class="math notranslate nohighlight">
\[Y_i \sim \text{Poisson}(\lambda_i).\]</div>
</section>
<section id="id17">
<h3>Estimation<a class="headerlink" href="#id17" title="Permalink to this heading">#</a></h3>
<p>Under a general framework with <span class="math notranslate nohighlight">\(k\)</span> regressors, the <strong>regression parameters</strong> <span class="math notranslate nohighlight">\(\beta_0, \beta_1, \dots, \beta_k\)</span> in the model are also unknown. In order to estimate them, we will use function <code class="docutils literal notranslate"><span class="pre">glm()</span></code> and its argument <code class="docutils literal notranslate"><span class="pre">family</span> <span class="pre">=</span> <span class="pre">poisson</span></code> (required to specify the Poisson nature of the response), which obtains the estimates <span class="math notranslate nohighlight">\(\hat{\beta}_0, \hat{\beta}_1, \dots \hat{\beta}_k\)</span>.</p>
<p>The estimates are obtained through <strong>maximum likelihood</strong> where we assume a <strong>Poisson joint probability mass function</strong> of the <span class="math notranslate nohighlight">\(n\)</span> responses <span class="math notranslate nohighlight">\(Y_i\)</span>.</p>
<p>For the sake of coding clarity, you could also use <code class="docutils literal notranslate"><span class="pre">family</span> <span class="pre">=</span> <span class="pre">poisson(link</span> <span class="pre">=</span> <span class="pre">&quot;log&quot;)</span></code>. Nevertheless, <code class="docutils literal notranslate"><span class="pre">link</span> <span class="pre">=</span> <span class="pre">&quot;log&quot;</span></code> is a default in <code class="docutils literal notranslate"><span class="pre">glm()</span></code> for Poisson regression. Thus, <code class="docutils literal notranslate"><span class="pre">family</span> <span class="pre">=</span> <span class="pre">poisson</span></code> suffices when using the logarithmic link function.</p>
</section>
<section id="id18">
<h3>Inference<a class="headerlink" href="#id18" title="Permalink to this heading">#</a></h3>
<p><strong>The fitted regression model will be used to identify the relationship between the logarithm of the response’s mean and regressors.</strong> To determine the <strong>statistical significance</strong> of <span class="math notranslate nohighlight">\(\beta_j\)</span> in this model, we also use the <strong>Wald statistic</strong>:</p>
<div class="math notranslate nohighlight">
\[
z_j = \frac{\hat{\beta}_j}{\mbox{se} \left( \hat{\beta}_j \right)}
\]</div>
<p>to test the hypotheses</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{gather*}
H_0: \beta_j = 0 \\
H_a: \beta_j \neq 0;
\end{gather*}\end{split}\]</div>
<p>where the <strong>null hypothesis</strong> <span class="math notranslate nohighlight">\(H_0\)</span> indicates that the <span class="math notranslate nohighlight">\(j\)</span>th regressor corresponding to <span class="math notranslate nohighlight">\(\beta_j\)</span> does not have any association on the response variable in the model, and the <strong>alternative hypothesis</strong> <span class="math notranslate nohighlight">\(H_a\)</span> otherwise. Provided the sample size <span class="math notranslate nohighlight">\(n\)</span> is large enough, <span class="math notranslate nohighlight">\(z_j\)</span> has an <strong>approximately Standard Normal distribution</strong> under <span class="math notranslate nohighlight">\(H_0\)</span>.</p>
<p><code class="docutils literal notranslate"><span class="pre">R</span></code> provides the corresponding <strong><span class="math notranslate nohighlight">\(p\)</span>-values</strong> for each <span class="math notranslate nohighlight">\(\beta_j\)</span>. The smaller the <span class="math notranslate nohighlight">\(p\)</span>-value, the stronger the evidence against the null hypothesis <span class="math notranslate nohighlight">\(H_0\)</span>. As in the previous regression models, we would set a predetermined significance level <span class="math notranslate nohighlight">\(\alpha\)</span> to infer if the <span class="math notranslate nohighlight">\(p\)</span>-value is small enough. If the <span class="math notranslate nohighlight">\(p\)</span>-value is smaller than the predetermined level <span class="math notranslate nohighlight">\(\alpha\)</span>, then we could claim that there is evidence to reject the null hypothesis. Hence, <span class="math notranslate nohighlight">\(p\)</span>-values that are small enough indicate that the data provides evidence in favour of <strong>association</strong> (<strong>or causation in the case of an experimental study!</strong>) between the response variable and the <span class="math notranslate nohighlight">\(j\)</span>th regressor.</p>
<p>Furthermore, given a specified level of confidence where <span class="math notranslate nohighlight">\(\alpha\)</span> is the significance level, we can construct approximate <span class="math notranslate nohighlight">\((1 - \alpha) \times 100\%\)</span> <strong>confidence intervals</strong> for the corresponding true value of <span class="math notranslate nohighlight">\(\beta_j\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_j \pm z_{\alpha/2}\mbox{se} \left( \hat{\beta}_j \right),
\]</div>
<p>where <span class="math notranslate nohighlight">\(z_{\alpha/2}\)</span> is the upper <span class="math notranslate nohighlight">\(\alpha/2\)</span> quantile of the <strong>Standard Normal distribution</strong>.</p>
</section>
<section id="id19">
<h3>Model Selection<a class="headerlink" href="#id19" title="Permalink to this heading">#</a></h3>
<section id="goodness-of-fit-test">
<h4>Goodness of Fit Test<a class="headerlink" href="#goodness-of-fit-test" title="Permalink to this heading">#</a></h4>
<p>The <strong>deviance</strong> (<span class="math notranslate nohighlight">\(D_k\)</span>) criterion can be used to compare a given model with <span class="math notranslate nohighlight">\(k\)</span> regressors with that of a <strong>baseline model</strong>. The usual baseline model is the <strong>saturated</strong> or <strong>full model</strong>, which perfectly fits the data because it allows a distinct Poisson mean <span class="math notranslate nohighlight">\(\lambda_i\)</span> for the <span class="math notranslate nohighlight">\(i\)</span>th observation in the training dataset (<span class="math notranslate nohighlight">\(i = 1, \dots, n\)</span>), <strong>unrelated to the <span class="math notranslate nohighlight">\(k\)</span> regressors</strong>.</p>
<p>The <strong>maximized likelihood</strong> of this full model is denoted as <span class="math notranslate nohighlight">\(\hat{\mathscr{l}}_f\)</span>. Now, let <span class="math notranslate nohighlight">\(\hat{\mathscr{l}}_k\)</span> be the value of the maximized likelihood computed from our dataset of <span class="math notranslate nohighlight">\(n\)</span> observation with <span class="math notranslate nohighlight">\(k\)</span> regressors.</p>
<p>We can compare the fits provided by these two models by the deviance <span class="math notranslate nohighlight">\(D_k\)</span> given by</p>
<div class="math notranslate nohighlight" id="equation-deviance-general-app-2">
<span class="eqno">(40)<a class="headerlink" href="#equation-deviance-general-app-2" title="Permalink to this equation">#</a></span>\[D_k = -2 \log \left( \frac{\hat{\mathscr{l}}_k}{\hat{\mathscr{l}}_f} \right) =  -2 \left[ \log \left( \hat{\mathscr{l}}_k \right) - \log \left( \hat{\mathscr{l}}_f \right) \right].\]</div>
<p>Note that <span class="math notranslate nohighlight">\(D_k\)</span> expresses <strong>how much our given model deviates from the full model on log-likelihood scale</strong>. This metric is interpreted as follows:</p>
<ul class="simple">
<li><p><strong>Large values</strong> of <span class="math notranslate nohighlight">\(D_k\)</span> arise when <span class="math notranslate nohighlight">\(\hat{\mathscr{l}}_k\)</span> is small relative to <span class="math notranslate nohighlight">\(\hat{\mathscr{l}}_f\)</span>, indicating that <strong>our given model fits the data poorly compared to the baseline model</strong>.</p></li>
<li><p><strong>Small values</strong> of <span class="math notranslate nohighlight">\(D_k\)</span> arise when <span class="math notranslate nohighlight">\(\hat{\mathscr{l}}_k\)</span> is similar to <span class="math notranslate nohighlight">\(\hat{\mathscr{l}}_f\)</span>, indicating that <strong>our given model provides a good fit to the data compared to the baseline model</strong>.</p></li>
</ul>
<p><strong>Specifically for Poisson regression with <span class="math notranslate nohighlight">\(k\)</span> regressors</strong>, it can be shown that <span class="math notranslate nohighlight">\(D_k\)</span> <a class="reference internal" href="#equation-deviance-general-app-2">(40)</a> is defined as follows:</p>
<div class="math notranslate nohighlight" id="equation-deviance-poisson-app">
<span class="eqno">(41)<a class="headerlink" href="#equation-deviance-poisson-app" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{gather}
\hat{\lambda}_i = \exp{\left( \hat{\beta_0} + \hat{\beta}_1 x_{i,1} + \dots + \hat{\beta}_k x_{i,k} \right)} \\
D_k = 2 \sum_{i = 1}^n \left[ y_i \log \left( \frac{y_i}{\hat{\lambda}_i} \right) - \left( y_i - \hat{\lambda}_i \right) \right]
\end{gather}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(y_i\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>th observed response in the training set of size <span class="math notranslate nohighlight">\(n\)</span>. Note that when <span class="math notranslate nohighlight">\(y_i = 0\)</span> counts, then <span class="math notranslate nohighlight">\(\log \left( \frac{y_i}{\hat{\lambda}_i} \right)\)</span> is assumed as <span class="math notranslate nohighlight">\(0\)</span>.</p>
<p>Equation <a class="reference internal" href="#equation-deviance-poisson-app">(41)</a> depicts the agreement of our model with <span class="math notranslate nohighlight">\(k\)</span> regressors to the observed data. Hence, we can use <a class="reference internal" href="#equation-deviance-poisson-app">(41)</a> to test the goodness of fit; i.e., <strong>whether our fitted model fits the data better than the saturated model, which makes it correctly specified (with a level of significance <span class="math notranslate nohighlight">\(\alpha\)</span>!)</strong>.</p>
<p>The hypothesis are the following:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{gather*}
H_0: \text{Our}\textbf{ Model with $k$ regressors} \text{ fits the data better than the } \textbf{Saturated Model} \\
H_a: \text{otherwise.}
\end{gather*}\end{split}\]</div>
<p>We use the <strong>residual deviance</strong> <span class="math notranslate nohighlight">\(\Delta_k\)</span> as a <strong>test statistic</strong>. <strong>Asymptotically</strong>, we have the following <strong>null distribution</strong>:</p>
<div class="math notranslate nohighlight">
\[
\Delta_k \sim \chi^2_{n - (k + 1)}.
\]</div>
</section>
<section id="analysis-of-deviance-for-nested-models">
<h4>Analysis of Deviance for Nested Models<a class="headerlink" href="#analysis-of-deviance-for-nested-models" title="Permalink to this heading">#</a></h4>
<p>Suppose <strong>Model 1</strong> is nested in <strong>Model 2</strong>. Hence, this specific model selection will involve a hypothesis testing. The hypotheses are:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{gather*}
H_0: \textbf{Model 1} \text{ fits the data better than } \textbf{Model 2} \\
H_a: \textbf{Model 2} \text{ fits the data better than } \textbf{Model 1}.
\end{gather*}\end{split}\]</div>
<p>Let <span class="math notranslate nohighlight">\(D_2\)</span> be the deviance for <strong>Model 2</strong> and <span class="math notranslate nohighlight">\(D_1\)</span> the deviance for <strong>Model 1</strong>. The test statistic <span class="math notranslate nohighlight">\(\Delta_D\)</span> for the analysis of deviance is given by:</p>
<div class="math notranslate nohighlight">
\[
\Delta_D = D_1 - D_2 \sim \chi^2_{d},
\]</div>
<p>which <strong>assymptotically</strong> (i.e., <span class="math notranslate nohighlight">\(n \rightarrow \infty\)</span>) is <a class="reference external" href="https://www.math.wm.edu/~leemis/chart/UDR/PDFs/Chisquare.pdf"><strong>Chi-squared distributed</strong></a> with <span class="math notranslate nohighlight">\(d\)</span> degrees of freedom under <span class="math notranslate nohighlight">\(H_0\)</span>. The <span class="math notranslate nohighlight">\(d\)</span> degrees of freedom are the <strong>regression parameters of difference between both models</strong>.</p>
<p>Formally, this nested hypothesis testing is called the <strong>likelihood-ratio test</strong>.</p>
</section>
<section id="id20">
<h4>Akaike Information Criterion<a class="headerlink" href="#id20" title="Permalink to this heading">#</a></h4>
<p><strong>One of the drawbacks of the analysis of deviance</strong> is that it only allows to test <strong>nested</strong> regression models when we have sparse data (i.e., each response is associated with a different set of values in the regressors).</p>
<p>Fortunately, we have alternatives for model selection. <strong>The Akaike Information Criterion (AIC) makes it possible to compare models that are either nested or not.</strong> For a model with <span class="math notranslate nohighlight">\(k\)</span> regressors and a deviance <span class="math notranslate nohighlight">\(D_k\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
\mbox{AIC}_k = D_k + 2k.
\end{equation}\]</div>
<p>Models with <strong>smaller</strong> values of <span class="math notranslate nohighlight">\(\mbox{AIC}_k\)</span> are preferred. That said, <span class="math notranslate nohighlight">\(\mbox{AIC}_k\)</span> favours models with small values of <span class="math notranslate nohighlight">\(D_k\)</span>.</p>
<p>However, <span class="math notranslate nohighlight">\(\mbox{AIC}_k\)</span> penalizes for including more regressors in the model. Hence, it discourages overfitting, which is key in model selection. This is why we select that model with the smallest <span class="math notranslate nohighlight">\(\mbox{AIC}_k\)</span>.</p>
</section>
<section id="id21">
<h4>Bayesian Information Criterion<a class="headerlink" href="#id21" title="Permalink to this heading">#</a></h4>
<p>An alternative to AIC is the Bayesian Information Criterion (BIC). <strong>The BIC also makes it possible to compare models that are either nested or not.</strong> For a model with <span class="math notranslate nohighlight">\(k\)</span> regressors, <span class="math notranslate nohighlight">\(n\)</span> observations used for training, and a deviance <span class="math notranslate nohighlight">\(D_k\)</span>; it is defined as:</p>
<div class="math notranslate nohighlight">
\[\mbox{BIC}_k = D_k + k \log (n).\]</div>
<p>Models with <strong>smaller</strong> values of <span class="math notranslate nohighlight">\(\mbox{BIC}_k\)</span> are preferred. That said, <span class="math notranslate nohighlight">\(\mbox{BIC}_k\)</span> also favours models with small values of <span class="math notranslate nohighlight">\(D_k\)</span>.</p>
<p>The differences between AIC and BIC will be more pronounced in datasets with large sample sizes <span class="math notranslate nohighlight">\(n\)</span>. As the BIC penalty of <span class="math notranslate nohighlight">\(k \log (n)\)</span> will always be larger than the AIC penalty of <span class="math notranslate nohighlight">\(2k\)</span> when <span class="math notranslate nohighlight">\(n &gt; 7\)</span>, <strong>BIC tends to select models with fewer regressors than AIC</strong>.</p>
</section>
</section>
</section>
<section id="survival-function">
<h2>Survival Function<a class="headerlink" href="#survival-function" title="Permalink to this heading">#</a></h2>
<p>For this function, let us consider a <strong>continuous</strong> random variable</p>
<div class="math notranslate nohighlight">
\[Y = \text{Time until an event occurs.}\]</div>
<p>We know that the cumulative distribution function (CDF) of <span class="math notranslate nohighlight">\(Y\)</span> tells us the probability that the event of interest occurs <strong>before a certain point in time <span class="math notranslate nohighlight">\(t\)</span></strong>:</p>
<div class="math notranslate nohighlight">
\[
F_Y(t) = P(Y \leq t).
\]</div>
<p>However, as the name suggests, in Survival Analysis, we are more interested in the probability that the event <strong>WILL NOT</strong> occur before a certain point in time <span class="math notranslate nohighlight">\(t\)</span> (i.e., <strong>the subject survives at least at point <span class="math notranslate nohighlight">\(t\)</span></strong>):</p>
<div class="math notranslate nohighlight">
\[
S_Y(t) = P(Y &gt; t) = 1 - F_Y(t).
\]</div>
<p><span class="math notranslate nohighlight">\(S_Y(t)\)</span> is called the <strong>survival function</strong>.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            name: "ir",
            path: "./notes"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="appendix-binary-log-regression.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Binary Logistic Regression</p>
      </div>
    </a>
    <a class="right-next"
       href="appendix-reg-mindmap.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Regression Mind Map</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-logistic-regression">Binary Logistic Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-modelling-framework">Data Modelling Framework</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation">Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference">Inference</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-selection">Model Selection</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#analysis-of-deviance">Analysis of Deviance</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#akaike-information-criterion">Akaike Information Criterion</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-information-criterion">Bayesian Information Criterion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cox-proportional-hazards-model">Cox Proportional Hazards Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Data Modelling Framework</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Inference</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction">Prediction</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cumulative-hazard-function">Cumulative Hazard Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hazard-function">Hazard Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#link-functions-in-generalized-linear-models-glms">Link Functions in Generalized Linear Models (GLMs)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multinomial-logistic-regression">Multinomial Logistic Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Data Modelling Framework</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Inference</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Model Selection</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#negative-binomial-regression">Negative Binomial Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reparametrization">Reparametrization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Data Modelling Framework</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-coefficient-interpretation-prediction-goodness-of-fit-and-model-selection">Inference, Coefficient Interpretation, Prediction, Goodness of Fit, and Model Selection</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ordinal-logistic-regression">Ordinal Logistic Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Data Modelling Framework</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">Inference</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">Model Selection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#non-proportional-odds-model">Non-proportional Odds Model</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-brant-wald-test">The Brant-Wald Test</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ordinary-least-squares-regression">Ordinary Least-squares Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">Data Modelling Framework</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modelling-assumptions">Modelling Assumptions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition-of-linearity">Definition of Linearity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">Inference</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#poisson-regression">Poisson Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">Data Modelling Framework</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">Inference</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">Model Selection</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#goodness-of-fit-test">Goodness of Fit Test</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#analysis-of-deviance-for-nested-models">Analysis of Deviance for Nested Models</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">Akaike Information Criterion</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">Bayesian Information Criterion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#survival-function">Survival Function</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By G. Alexi Rodríguez-Arelis, Payman Nickchi, Rodolfo Lourenzutti, and Vincenzo Coia
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>